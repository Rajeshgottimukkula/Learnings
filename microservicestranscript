Topic Name - Introduction - Microservices with Spring Boot


Transcript - 

Hey, what's up, everyone?

Welcome to the course of microservices with Springboard.

This course is going to be in depth course from beginner to advanced level, from springboard to deploying

the microservices.

In this course we are going to cover how to start working with the springboard, how to create the springboard

projects and how to create the different APIs, how to create different rest endpoints in this course.

And then we are going to learn more about the Springboard and spring data JPA, how we can create the

different REST APIs and connect all those rest APIs with our backend, that is our database.

We are also going to learn about the basics of Web services and then we will move towards the microservices.

We are going to see what is microservices and what are the advantages and disadvantages and we are going

to create the microservices architecture, We are going to create multiple microservices and we are

going to connect all those microservices together.

We are going to use the spring cloud as well in this microservices.

So we are going to touch all the different components of Spring Cloud and we are going to implement

all those in our microservices.

So we'll see how all those things work together and create the entire microservices architecture.

Then we are going to learn Docker and convert our application into Docker format to convert our application

into containerization so we can deploy our applications into Docker.

And we are also going to learn Kubernetes, how we can orchestrate our containers and deploy all those

into our Kubernetes cluster.

And we are going to deploy our entire microservices architecture, what we have built into our Kubernetes

cluster.

And we are also going to implement each and everything with the automated pipeline.

So this is the agenda of the course.

And if you want, you can go through any of the topics that are mentioned here.

If you already know about the springboard, that you can skip these topics and you can directly jump

to the microservices.

If you know the basics of microservices and you want to go ahead and jump towards the deploying the

microservices, you can go ahead that way as well.

And you can also go directly to the last section about deploying all those using this pipeline.

But I highly suggest you to go through all those topics if you are interested, and if you want to revise

all those chapters, then we are adding all those things in detail.

So any beginner from advanced level, anyone would be able to go through this course easily without

any hesitation now, without wasting any time.

Let's go towards learning Springboard.




Transcript - 

Now what is Springwood?

Before understanding Springwood, we need to understand what is spring framework and why there was a

need of a spring framework when the JAO was introduced.

Developer used to develop the application using the Java kit.

That is the enterprise edition.

They were using all the servers and JSP to create our application.

Then on top of that there were different frameworks created.

There are frameworks like Struts, Live Free Spring and many, and with all those spring is the most

famous framework.

Now, why was spring the most famous framework?

Because spring provides a lot of functionality and it provides a lot of different modules to work and

they can easily work with the different modules and we can create the applications easily.

So suppose if I want to build the web application, I can use the spring MVC module.

If I want to work with the batch processing, I can use the spring batch processing module and there

are many modules available which I can easily use and integrate those modules with the spring framework.

That being said, there were a lot of configurations also required to work with spring, so in the earlier

days spring was configured using the XML configuration, so you had to add a lot of XML configuration

to work with spring.

So if you added the base spring core module in your project, so for that you had to do some configuration.

On top of that, if you are adding spring web or spinning MVC dependencies, then for that using those

modules you have to do some configurations in the XML file.

Now if you're using any ORM framework like Hibernate libraries or anything.

So for that you had to do some configurations.

So that means for each and everything, you had to do some configurations to work seamlessly.

Now those spring framework provides a lot of functionality.

There was a drawback about a lot of configuration because those spring framework was also known as frameworks

of frameworks, because it allows you to integrate all the different frameworks within the spring.

That comes with the drawback of the multiple configuration.

There are a lot of configurations now to solve that problem.

Spring developers decide to build a framework around it and to minimize the configuration from the developer

side.

So for that reason, Spring Boot framework was introduced.

Now, if I define you in the simple word, springboard is the abstraction layer on top of the spring

framework.

So Springboard is built on top of the spring framework, plus it has all the different configuration

added to it.

That means all the auto configuration has been added to the spring framework.

So whenever you have to use any frameworks or any modules of spring, all those basic configurations

are added in your project.

By default, you don't have to add anything only if you are doing some changes in the configuration

at that time.

You can add those configurations very easily, so you can see that a lot of burden from the developer

shoulders was put off and all those configurations and all those functionalities and everything was

by default provided to developer.

So developer can only focus on their actual business logics.

So this way Springboard is widely used in the industry and it is really easy to develop application

in the springboard.









Transcript - 
Now, whenever we talk about any frameworks or particularly with spring, we talk about dependency injection.

Now, what is dependency injection?

Let's see that.

Now consider you have a class student, So let me just define here.

I have a class student and in this class student I have few fields.

Suppose first name.

Last name.

An email ID.

Look, there are a few fields with this student now.

Whenever we want to use this particular class, what we do is we create the objects.

So what we do by default is we create.

Student as equals to.

New student.

So this is how generally we create the object in Java.

Right now what we are doing is we as a developer are creating the objects here.

So that means I have a complete control on creating the objects.

Now, within this student, I need the object of subjects.

So I am defining the subjects here.

Now this object is a class, so let me just define the class object here.

And within this subject class, there are different things like name of the subject and the class teacher

of that particular subject.

And there are other details available within this particular subject class.

Now, you can see that as the subject is a class that is the dependent right in the student.

So student is dependent on the subject class.

That means whenever I want to use this class, I need the object of a subject because within the student

I need to define that.

What are the different subjects of a particular student?

So what I had to do is generally I had to create the subject class and then that particular object I

need to pass to the student.

Now you can see that this everything is done by manually by me as a developer, but to create all those

particular objects by a developer, by a user is not ideal, right?

Because there are a lot of dependencies available and there might be chances as well.

That user is creating a lot of unnecessary objects as well, and the garbage collection is not proper

and there might be a lot of scenarios.

So what spring suggest is spring suggest to use the invasion of control, which means to give the control

to the framework itself, rather than taking the control to your own.

So what I also defines is invasion of control.

That means giving control back to the framework.

Now, to implement this we use the dependency injection pattern.

Now, dependency injection pattern allows us to use the IOC where all of the dependencies are being

handled by the spring framework itself.

Now, whenever we define all the classes in the spring framework, we define all those particular classes

as a bean.

So we define all those different beans.

And whenever the spring application starts, it creates the factory of all those beans.

It has a bean factory and all those beans are created and stored in a container.

So suppose I have ten beans available.

All those ten beans are created and stored in the container.

Now, if any of the beans is dependent on the another bean, spring frameworks takes the responsibility

to inject all those beans wherever it is necessary based on the different scopes defined in the spring

framework.

So you can see that rather than creating the objects by a user, by a developer itself, spring takes

care of all those particular things rather than developer, worrying about creating the objects, doing

the garbage collections and unnecessarily creating all those different objects.

Strings framework tells that Hold on, I will do everything for you.

You just define that what you need.

You need ten classes.

Don't worry, define those ten classes.

Define the different scopes.

For that, I will take care of each and every thing so that dependency injection is taken care by the

spring framework and we have to define like which particular bean is required, where we will see each

and everything in the code when we are doing the examples.

But this is a general idea what dependency injection is.



Transcript - 
Now, as we have gone through what is springboard and what is dependency injection, let's try to create

a simple springboard project and from there we'll go towards different topics.

So let's create a springboard project now to create a springboard project.

There are different ways available by default.

Spring Developer has provided us with this spring initialized tool with which we can create the Spring

boot projects very easily.

There is a springboard select tool also available, and within your IDs of your choice, you can also

create your springboard project.

I will show you here with the spring initialized.

So whatever ID you are using, you should be able to create the project and open that project in the

favorite ID.

So open your browser and go to spring initial user.

So go to start dot spring dot IO.

So you can see that this is the tool available to create your springboard project and you can see that

there are different options available.

So first of all, you can see that what type of project you want to build, you want to build a MAVEN

project or you want to build a gradle project.

Both of these are project management tool.

You can use any one we are going to use MAVEN in this entire course.

So we are going ahead with the MAVEN project selection here.

But if you are familiar with the gradle, you can also go ahead with the gradle.

As Maven is widely used in the industry for Springboard application.

We are going ahead with the MAVEN project, so the project selection is MAVEN year.

Then you have to select the language in which language you want to create the project.

There are three options available Java Groovy and Kotlin.

We will go ahead with the Java, but if you are familiar with Kotlin and Groovy, you can go ahead with

that as well.

Then you can see that you will get the option for choosing the springboard version.

The current stable version is 2.7.2, so we are going to use this version, but whichever version you

want, you can use this particular version.

From here you can select any versions from your.

And then you can see there is a project metadata.

This is the artifact information of your project.

So for each and every project, you need to define three things.

That is the group artifact and the version information.

So with these three details, your project would be unique in the entire repository.

So ideally the group name is given as the domain name.

So my domain name is daily code Buffer dotcom, so I'll give that.

Com dot.

Daily core buffer and generally it is given in the reverse order.

So I have given common daily code before and then you have to give the name of the artifact what name

your application would be.

So I am just giving that.

Springboard demo.

Simple.

And then the name of the.

Application and then the description, and then you have to give the package name.

So package name, I'm giving a springboard demo.

Simple.

And then you can see that you have to choose the packaging.

So either you want to package as a jar or you want the packages of war.

If you want to create as a standalone application where you can directly run your application, you

go ahead with the JAR file and it will also have the embedded server in it.

We will talk about embedded server later, but when you're choosing the jar packaging, the embedded

server will come together and if you want to deploy your application to any other server manually,

then you can go ahead with the more application war packaging type and then you can see that Java version.

So which particular Java version that you want to select in my machine, Java 11 is installed, so I

will go ahead and select Java 11.

Then if you come here you can see that there is a dependency sections available.

So if you click here you can see there are a lot of dependencies available.

Then you can add in your MAVEN project.

So you can see that spring native Spring Wood DevTools, Lombok Spring Configuration processor.

There are a lot of different modules available, different dependencies, Whatever dependencies you

add that dependency will be added in the XML file that is the MAVEN configuration file for your project.

So what I'll do as we have to create the web application, I will select the spring web.

You can also add multiple dependencies here.

Then what you can do.

There are three options available to generate your project, to explore your project, and to share

the URL.

So if you click on share, the URL will be created and whoever is using that particular URL, all those

fields and dependencies will be auto populated for them.

And if you click on Explorer, you can explore the projects and you can explore the project structure.

Over here you can see that there is a folder available that is the MAVEN wrapper that is MV and W and

MV and W command.

So if Maven is not installed in your system as well, you should you would be able to easily work with

this project.

Then you can see that XML file is there.

This is the main configuration file of your project and it is the MAVEN configuration that is also called

as Project Object model.

So here you will see all those configurations of your project.

As you can see, all those informations are added.

What was the group ID, Artifact ID version and the relative path?

And you can see that this was all about the springboard starter Parent Always Springboard Project will

be having a parent and the group ID artifact and version.

This is something that we give and the name of the project.

And here we have added the dependency that was this dependency and springboard starter test.

This is the by default dependency added and there is a source folder also available.

You can see all those information here and you can click on the download button to generate your project

and you can see that the project is generated.

Now we can go ahead and use this particular project and open this project in our favorite ID.

So let's do that.






Transcript - 

So you can see that this is a project that I downloaded.

Now, let's unzip this.

Now you can see that this is the unzip folder.

Now you can open this particular folder in your favorite ID.

You can either use Eclipse, you can use spring tool suite that is SSDs.

You can use in idea or you can use vsco as well.

Whatever you are familiar with, you can use that.

In this particular course we are using the IntelliJ idea.

If you want to install the intelligent ID on your machine, you can go towards the browser and search

for.

Intelligent idea and go towards the community version.

So install the community version here of this indie league idea and you should be good to learn anything.

You should be good to use this software.

I would highly recommend using IntelliJ idea because in this course we are going to use that and if

there is any issue, we should be able to help to solve any of the issues here.

But if you're not comfortable with, you can go ahead and use any of your IDs.

So once you have downloaded it, what we will do is we will open the IntelliJ idea.

And we will open the project.

We'll click on Open Project here.

And we will select the project.

We'll go to the documents, Springboard Microservices Project, Springboard Demo, and this is the project

that we need to open.

So we'll open the project here.

We'll trust it.

And you can see that the project is open for the first time.

It will take some time to download all the dependencies in your machine, so wait for a couple of minutes

to complete all the processes.

Once everything is done, you can see that your project structure will be this way.

You can see that your project structure is there, your XML file would be there and your SR directory

would be there.

If you open your XML file, this is your POM XML file and all the configurations are here for your MAVEN

project.

And in the CRC directory you can see there is two folders.

Mean and test test will include all your test cases.

That is J unit and your main is your main application code.

If you open main there are two things that is Java files and the resources files.

In the resources file you can see there is an application dot properties.

So all the configuration, all the settings that you need to do, all the properties you will be adding

in the application properties and your all Java classes will be adding in the Java folder by default,

one application file will be there created and there will be one main method so we can run our application.

This is the entire application and if I want to run this application, I can just run this from this

play button here.

And if you are in the clips, you can do right click and run as Java application.

So I'll just start my application here.

And you can see that your application is started.

Your application is deployed to the default Tomcat server.

That embedded server comes with the spring boot.

If you go here, you can see the logs as well.

That your spring boat application is using Java 11 OC.

The Tomcat is initialized, initialized on port number 88, so it's using 8880 as the default port and

your application is started.

So if I go to the browser.

And if I do.

Localhost 88.

You can see that your application is working fine.

There is an error shown here because there is no handler mapping configured yet.

So this is how you should be able to create a project using spring initialize and open in your ID and

you should be able to run your project.



Transcript - 
Now as you have configured your spring board application and started your spring application, let's

create a basic controller and solve that request.

So what we will do is we will go to our IntelliJ idea and what we will do is we are going to create

the helo controller.

So basically what we are doing is we are creating one rest endpoints so we can solve the request.

So it's very easy to do that.

So what we will do is we are going to create a package so we have a proper structure.

Ideally, whatever we will do in our Java application, we are going to create different packages for

the different types of classes that we are going to create as we are going to create a controller here

to handle the HTTP request, we are going to create a controller package and in that controller package

we are going to create a class.

So let me just create a Java class Java package here and that says.

Controller.

The package is created and within this package let me create a Java class that says Home controller.

Look, you can see that simple classes created now all the configurations in Springboard we will do

mostly using the annotations.

So we'll define the annotations.

And based on those annotations, Springwood will react to it and the functionalities will be added for

that class.

So you can see that this is a simple class created.

Now what I want this particular class to do is I want this particular class to behave as a controller.

That means to.

So the different requests for me.

So for that what I'll do, I will use an annotation that is at the rate controller.

You can see that this controller annotation is from the object spring framework, dot stereotype annotations,

that's studio type controller.

So you can see that I've just added the controller here.

That means now my this class home controller controller will be behaved as a controller.

Now one more thing.

Now this home controller, whatever the methods that I'm going to create here, I want to create all

those as the rest endpoints.

That means I want to send the data in the response body format either would be JSON or XML or anything.

I want to return the data.

I don't want to create the MVC pattern where I will be passing the JSP or the index files.

So what I will do, I will annotate this with at the rate.

Response body.

Now, what we have done till now is we told the spring that my home controller class or whatever I've

created that should behave as a controller.

Plus it should also behave in such a way that it should return the data in the response body itself.

It should not go to search for the any view's configured OC.

We are not configuring any views, we just want to send the data back in the response body format.

This is what I have defined here.

Let's define the endpoint.

So whenever I hit any endpoint, I want that endpoint to reach here and whatever the method is there,

it should reach that method.

And whatever the steps that I have written in that method, all those things should be executed and

the response should be sent back.

This is something that I want.

So what I'll do, I will define a method, public method.

What I want in return is a string.

So I'm defining string here and the name of the method that is home.

Again, this is a simple thing.

Now I want to return.

Hello, world.

Look, you can see that simple thing that I've defined here.

Now, I want this method to be attached to any of the endpoints, to any of the resources.

What I'll do, I will configure that way, so I'll define the annotation to do that.

The annotation is at the rate request mapping.

Okay, So all the requests that comes here in this home controller I am mapping.

That request to come here.

Now, which request should come here?

So I need to give the path of a request.

So I'm defining that all the request with a slash.

That means my home path should come here.

If I do a, B, c here, that means all the requests with slash ABC will come here.

That's the simple thing I'm defining.

So let me just put it the slash.

Only now what I will do is I will just restart my application.

Currently, if you see that, if I still run, nothing happens, right?

So what I'll do, I will restart my application.

You can see that my application is restarted.

I'll go to the browser again and I will refresh the page.

And you can see that I am getting Hello World here.

Right.

So you can see that I have mapped the slash request.

That is the home request to this controller method.

Cut his public string home and it is returning the string here.

Very simple thing that we have done here.

So we just created a controller.

We created a method.

That method will have a request mapping and whatever the request will come to the given path, This

method will solve that request.

That's the simple thing that we have defined, and this is the basics of each and everything that we

are going to do in the entire course.

We are going to create a lot of different rest APIs and we are going to call another rest APIs from

one of the APIs and vice versa.





Transcript - 

Now you can see that whatever we did was right.

With just 5 to 6 lines of code, we were able to create our entire application.

We were able to deploy each and everything in the embedded server, and we were also able to map a particular

request as well.

And we were able to send the response data back as well.

You can see that we did a lot of things, but we did only file line of code rest.

Everything was handled by the springboard itself.

And if you go towards the XML file as well here you can see that there is only two dependency, it is

added, but ideally we only added one.

That is the spring starter web dependency.

This one was by default added.

So you can see that everything here you can see it has something called starter.

You can see that springboard starter parent, you can see that springboard starter web, you can see

Springboard starter test.

So what is the starter?

So what Springboard does is springboard bundles all the required dependency into one bundle and it calls

as a starter.

So if you are manually creating anything, suppose if you are creating an application and in that application

you need ten dependencies, you will go ahead and do all those ten dependencies, but the same ten dependencies

you want to use in any other project as well.

Then you will go ahead there as well and you will add all those ten dependencies in the third project.

If you want to do, you will do the same things.

You will go ahead and add those ten dependencies.

That means all those ten dependencies are basic to create or to give you some sort of functionalities.

Right?

So what Springboard does is it adds all those ten dependencies into one of the starter.

So that means whenever you use that starter dependency, it will have all those dependencies configured

for you so you don't have to go ahead and add all those ten dependencies.

You just add that one dependency and you will have all those configurations, all those dependencies

added to it.

So here if I check for the springboard starter web, we just added one dependency and if you open this

dependency you will be able to see that this is the palm right XML file for the springboard starter

web.

And here you will see all those different dependencies.

You can see that it has the springboard starter dependency, it has the springboard strategy and dependency.

Springboard starter Tomcat dependency Spring web framework is added spring web and VCs added.

If it was not providing all those dependencies into a bundle format into a starter package, which we

would have to add a lot of dependencies.

So this way it combines each and everything and give us.

So if you go to the springboard starter tomcat, this is also a starter dependency bundled into another

starter dependency.

So if you go ahead and see this, you are also it will be having different dependencies, right?

So you can see that it has a Jakarta API Tomcat embed, core Tomcat embed L So you can see that there

are a lot of dependencies and there are a lot of dependencies nested as well.

So to minimize all these complexities, Springboard provides the different status for us.

So whatever the functionalities we want, we will just add the status for that and all the dependency

dependencies will be added in our project support.

If you want to work with the hibernate, if you want to work with the JPA, you just add the status

of the GPA and you're good to go.

If you want to work with this, you just add the status for the Redis and you are good to go.

So this way there are a lot of starters available and you can just add the status and you have all the

order configurations and everything added in your project.

You don't have to go ahead and search all the different dependencies and everything.

So this house, pretty good starters, help us in minimizing the complexity and minimizing our code

as well.






Transcript - 

Now let's understand how Springboard works.

Now, whenever you're creating the Springwood project and we saw many times as well, when we are creating

the Springwood project, everything is automatically done.

Everything works like a magic.

You don't have to do anything.

But actually it's not magic.

Everything is a code, everything is a programmatically written code and all those things are added

programmatically based on the different configurations that you add in a project.

Now you may say What configurations?

I didn't add anything, but yeah, you would be adding something and based on that, the configuration

will take place.

So let's see how this Springwood works internally.

So whenever you're creating a Springwood project, it has the spring factories added to the Medina folder

and all the configuration, all the jar files required are mentioned there.

So whenever you add any properties or any configuration and it matches with the spring factories, it

tries to add that configuration to it.

So based on the different configurations that you add or based on a different dependency, that dependencies

that you add, the different configuration takes place internally.

So if I show you if you go to the external libraries here.

And if you go to the auto configure and if you open your in the meeting folder, you can see that there

is a spring dot factories.

So if you open this.

You can see that all those configurations are added here.

Now, if I open one of the class, I'm just opening one of the class that is JPA repositories auto configuration.

Now you can see that here there are different annotations available now based on all these annotations,

all the configuration takes place.

So suppose if you are creating a springboard project and in that Springboard project you have just added

the dependency for web, nothing else.

Then only spring web will be added in your project.

That configuration only will be added in your project.

Nothing else will be added.

And if you want to work with the JPA, then you need to add the dependency of the JPA and you need to

give some properties to make sure that the configuration will work.

So all those things how Springwood will know, right?

So you can see, Springwood will know all those things based on the annotations provided here.

So this particular class JPA repositories order configuration is to configure the JPA whatever you have

added to configure your application to connect with your database.

Now this will only work when it will fulfill all these conditions.

You can see that it has a condition that conditional on being that is data source class.

So that means it should have the been data source class available in the bean factory.

You can see there is a condition on conditional on class.

So it should find the JPA repository class in the class path.

Then only it will add the configuration that is conditional on missing bin.

It should not be having the missing beans, condition and property.

It should have these properties available.

So you need to define the spring data repositories.

So when all these conditions are met, the auto configuration will happen and any of the things that

are missing, you will get the error.

So we'll see that as well.

When we are implementing the JPA, I will show you that how we will get the errors when you have not

configured everything properly.

So whatever the minimum things it needs, if you have, if you give those minimum things, then it will

auto configure everything for you.

Otherwise it won't configure, it will throw that these are the things missing.

So all these things it will get to know from all these conditional classes and conditional annotations

provided here for all the different configurations.

Everything is defined this way only, and based on this, the configuration will happen.

So there is no magic.

All these are conditional beans and conditional implementations.

Everything is programmatically added to your project.

Now this is something how the different order configurations and how the different dependencies are

added in your project.

But how about running your application, how it works?

So for that, let's see if you go to the let me close each and everything and if you go to the springboard

application, this is something that is very important.

Annotation that is spring mode application and there is a method that is spring application that run

these two things help you to create your springboard application.

Now what it will do is if you open this springboard application.

You can see that it ideally does mean three things over here.

Whatever the configurations are added in your project, load those configuration first, then do the

order configuration for all those configurations and what are the extra beans or components or anything

that you have added load all those things as well.

So this is the task of this annotation.

So you can see that first of all, it will do spring boot configuration.

So all these configurations are being added.

So if I open this, you can see that it will check for all the added configurations.

So any class annotated with added configuration is been used for configuration purposes in the spring

boot.

So once you have all those configurations, it will do the order configurations.

So enable order configuration annotation is used to do all the order configurations for all the dependencies

and all the different configurations being added in our project.

So if you open this, you can see that it will do auto configuration package and you can see that it

will import this class and all those configurations are there.

You can use all those things and if you want to exclude any particular configurations, you can exclude,

you can see that there is a property available, exclude and exclude name.

So that means if you want to exclude anything you can provide in the exclude annotations.

So that means you have added the dependency of JPA, but you don't want the auto configuration of the

JPA, you want to do everything manually.

You can go ahead and do that.

You just need to exclude the auto configuration and do everything manually by adding the different configurations,

adding the different class and annotating with added configuration.

So that's how generally it would be.

So you can see everything is done for you by default with this one annotation.

But if you want to exclude and do manually, you have the option to do that.

Now, once everything is loaded, your run method will run and this run method will help you to start

your application.

So if you go towards this run method, if you open this run method and let me go to this run method

where.

We pass each and everything.

So this is the run method that is going to run and it will return the configurable application context.

So at the end it will return the application context, which is configurable, and it will start your

application.

And you can see it is taking the different variable arguments here.

So you can see that this method, first thing, what it will do is it will bootstrap the application

context.

You can see there is a method create bootstrap context so it will first create the application context

for you.

So whatever the context it has to create, right, by adding all the different beans into the IOC container

and everything, everything will be added and your context will be created.

And you can see that after that there are different things.

You can see configure headless properties, all the different configurations will be added and if there

are any listeners added in your application, it will get all those listeners and attached to your application.

And you can see after that it will take the argument.

So if there are any arguments added right, whenever you are starting your application, you will be

adding the different arguments.

So if there are any arguments, all those arguments will be added, collected and will be added in your

application context, in view into your configuration.

And once everything is added, it will refresh your context to get everything into the updated context.

And also it will do.

One thing is whenever it's creating the application context for you, it will also check what type of

context it is.

So if you are adding the web dependency in your application, that means it will end up creating the

web application context, right?

Because you are creating the web application and web application context needs to be created and that

needs to be deployed in your embedded server as well.

So all those steps are added.

If you are not creating a web application, you are just creating the terminal application.

Those type of contacts will be added.

So there are different types of contacts available based on the dependencies that you have added and

the configuration that you have added.

The different context type will be created and will be attached in your application.

And after that, if it's a web application, it will also create the Tomcat embedded servlet container.

That container will be created and your context will be attached to that container.

So that means your context can run on the embedded server provided.

We will talk more about the embedded server later.

But whatever the embedded server has been configured for your application, that particular servlet

container will be used to create your application and the context will be attached to it.

So you can see that it is nothing magic.

It is based on all the configurations and dependencies we add.

Whatever the dependencies that we are adding here from the spring dot factories, it will do the configuration

and based on the classes added, based on the configurations added in the application properties, all

those configuration, all those order configuration will happen in a project.

And based on this added Springboard application annotation, it will first try to get all the configurations,

it will enable all those configurations and it will scan for all the packages to get all the different

beans available and everything will be ready for you to use in the IOC container.

As we talked about IOC container earlier, every beans will be there in the IOC container for you to

use.

And whenever the run method is called, all those beans are being used to create your context.

Your context is defined based on the different dependencies.

Either it's a web context or a normal application context or anything.

And based on that, whatever the embedded server is defined for your application, that embedded server

container will be used to attach the context to that container to start your application.

Everything is really defined.

Everything is defined within the run method itself.

You can go through that easily and you'll be able to understand each and everything.

So there is no magic at all.






Transcript - 
Now let's talk about the embedded servers in the Springboard application.

Now, ideally, whenever you were creating the Java application or the spring application, what you

do is you create your application.

OC and that application.

Will be packaged in the water file.

And this water file.

You will go ahead and deploy to the server.

That's the generally flow you would be doing and this server would be.

Tomcat server.

WebLogic Server.

JB was server anything?

OC There are different servers available.

You will go ahead and deploy this water file to any of the servers.

So that means there are a lot of configuration that you have to do for your application as well and

for your server as well.

But with Springboard.

How about application plus server?

All together contained in a single jar file.

Right.

And you just run this jar file.

You don't have to worry about deploying to any server.

It has the server inbuilt to it.

Now there are three types of server available as the embedded server in the springboard application

that is Tomcat Jetty.

And undertow.

And if you go towards the reactive springboard application, there are different servers for that as

well available like Nat and all.

But let's keep those things aside.

When you whenever you're creating the simple Springboard application, these are the servers that are

used and Tomcat is the by default choice and by default option available.

So if you go here in your application, you have just added the spring web dependency that is the springboard

starter web, only one dependency we have added till now.

And whenever I will open this dependency if you scroll down here.

You can see that there is a springboard starter Tomcat dependency available.

So whenever I'm adding the spring web dependency springboard start Tomcat is the default option I'm

getting.

So that means my application will run on the Tomcat server whenever I'm starting my application.

So if I start this application, let me just restart the application.

And if I scroll here, you can see that my Tomcat is the container here, right?

And you will see everywhere that my Tomcat has been started.

It's a Tomcat web server, so it's by default Tomcat option and everything works fine.

Everything is completely fine.

Everything is okay.

But what about using Jet?

Right?

Why can't I use Jetty?

Let's see how we can use Jetty.

So what I have to do is as my springboard application already has the tomcat in it, I have to remove

Tomcat first and then add the jetty to it right then only it will able to understand that I need to

run my application on the jetty.

If I keep both jetty and Tomcat boat together, then it will get confused what to use.

Right.

So that means I need to remove Tomcat and add the jetty.

So that's something that we have to do.

So what I'll do, as we already saw that Tomcat was inside the spring setter web, right?

So this is the dependency and inside this dependency Tomcat was there.

So what I can do, I can go ahead and exclude the dependency.

So what I'll do, I will add the exclusion tags here.

I read the exclusion tags and in the exclusion tags I will add exclude.

And here I will add.

Spring springboard.

Starter Tomcat.

Oak Spring boat starter Tomcat is the dependency is the artifact that I want to remove.

I'll use that and I will use the group or the dot spring framework dot.

But you can see that I have excluded the Tomcat dependency until now.

My application is started right?

If I go here to my browser and if I run this, it's working.

But at the moment.

I will restart my application.

You can see that my application still runs right, because I've not refreshed the Maven dependency.

So let me just refresh load them even changes.

Okay.

You can see that the application is loaded now.

One knows that we need to remove the Tomcat dependency from the spring web dependency.

Let me restart the application.

And now you can see that your application worked, but it didn't deploy.

You can see that no logs are available, that it deployed into Tomcat or the Tomcat is running on port.

Any of the port number, Right?

Nothing is there.

And if you go here.

Nothing is working right, because your application is fine, your application is working, but there

is no embedded server now.

We had excluded it.

Now we can go ahead and add Jetty to it.

So what I will do, I will add the dependency.

So after the dependency of the web, I can add the dependency.

I've added the dependency and I'll add.

Springboard starter.

Jedi, and it's from the Arab Spring Framework Boot.

And I will refresh my memory settings and even project.

You can see that thinking it's completed.

Now, if I restart my application, let me just restart it.

And you can see that the application is started and you can see that now it's a web server and the application

is deployed on a jetty on the same port added.

Now, if I go back.

And if I run my application, you can see that my application is running fine.

So you can see that in just simple two steps.

We just changed our server, right?

How easy it is.

Now you might also say like, how about changing the configurations of these servers that we are using

so we can do all those configurations in the application properties?

Suppose I want to change the port number, so I will just go here and I will just do server.

Dot pot spelling is wrong.

Server dot port.

You can see the server dot port.

I'll just change to 88 one simple and if I read on the application.

And if you see the logs, you can see that now your jetty has started on Port 8081.

If I go back now and if I run the application, it won't work on 88 because I should work on 8081.

You can see that now.

It's working on Port 8081.

Now you can see that we added this exclusion manually, but sometimes we might not know that.

What are the exclusion that we need to do or what particular dependencies we need to remove?

So two ways I can suggest to you is go through this particular artifacts and from here you will be able

to understand what is available.

You can see that from this particular springboard starter Tomcat.

We were able to understand that this is something that we don't need and we need to remove, but there

might be complex scenarios when we're working with the complex projects.

There are multiple dependencies available that might be conflicting dependencies as well available.

So to handle all those and to rectify all those scenarios, what I would suggest is, is a simple way.

There is a plugin available in the idea that is.

Let me go to the plugin section.

And the plugin name is Maven Helper.

Just install this plugin and restart your IntelliJ idea and you will be able to see all those dependencies

in your MAVEN file.

So once you have installed this plugin that is maven helper, you will get the extra tab over here.

That is dependency analyzer.

So currently you can see that in the spring starter web we had added the exclusion by default manually.

Let me just remove it for now and let me just refresh this and then what we will do is let's go to the

dependency analyzer here.

Let's refresh the UI and here if I search for Tomcat, if I select all dependencies as list here, you

can see that I'm getting Tomcat embedded core server here and you can see this is from the springboard

starter tomcat.

It's from here as well and it's from the your web socket as well.

So what I want is I want to remove this.

Right.

So what I can do, I can click here and jump to the left tree.

It will show me here that this is something that is part of the springboard starter web.

Right click on it and exclude once you add this and if you go to the text.

And if you scroll up, you can see that the exclusion is added automatically.

You don't have to add everything.

So this way you can use this dependency analyzer plugin to handle your dependencies.





Transcript - 

Now let's talk about the Springwood actuators.

Springwood actuators are nothing but a tool that allows us to monitor our Springwood application.

So whatever happens in our application that all things we can handle using the actuator so actuator

gives us different endpoints to monitor our application.

So to add the Springwood actuator, we need to add the Springwood actuator dependency in our project.

So what we will do, we will go to our browser.

And I will go to start spring io spring and I will go to the dependencies and I will add the actuator

dependency here.

Springboard actuator.

You can see that it supports built in or custom endpoint that lets you monitor and manage your application

such as application, health matrix sessions, etc. everything we can handle using the springboard actuator.

So if I just add this dependency and if I do control space Explorer.

I can get the dependency here.

Right.

So what I'll do, I will just take this dependency.

I'll copy this.

And I will add in my project.

So I will go to the XML file and in the dependency after dependency here, I will add the dependency.

This is a springboard, actually dependency.

You can see that it's simple.

We just added a dependency.

Now.

I just reached out to my application.

And if I scroll here, you will be able to see that exposing one end points beneath base path slash

actuator.

So you can see that there are few endpoints added in the slash actuator endpoint.

So if I go.

Two my browser.

And if I do slash actuator.

You can see that these are the endpoints.

Let me just zoom out.

You can see that you will get this information.

So that means your actuator has been added in your project and you can check the health.

So currently there are three endpoints.

This is the mean endpoint.

This is the actuator slash health and actuator slash health slash path.

If I go to this, you can see that I'm getting the status as application is up.

Now, this is just a basic information available.

This is the default thing available, but there are a lot of different configurations that you can add

to modify that different endpoints for you.

So suppose if you want to get all the beans registered, you can do the configuration, you can enable

all those endpoints and those endpoints will be visible for you.

So for the security reasons, only one endpoint has been exposed directly just to check the application

health.

The rest of the endpoints are secured and disabled by default.

If you want to enable, you can go ahead and enable those endpoints based on the configurations, based

on the security you need to add the security so that no one would be able to see those endpoints by

default.

The status is fine because that is nothing that has been exposing from your application for this.

It just your application is healthy or not.

So this way you can add the actuator.

There are different configurations available.

You can go through the documentation and add the configuration.

It's fairly simple.





Transcript - 
Now let's talk about spring totals.

We, as you have seen that we have gone through a lot of things available.

And every time we are doing some changes, we are restarting our application.

This is not feasible when you are doing a lot of changes.

So Springwood DevTools gives us the tools that will help us in developing our applications.

So for that we need to add the developer tools.

The first important thing is live reload functionality.

It will allow us to do all the changes.

Like whenever we will do the changes, it will automatically detect all those changes and it will build

the project and deploy the project.

That's the basic thing that we are going to use from the Springboard DevTools.

Again, what we'll do is we will go to the initialized start dot spring.

I'll.

I will go here.

I will add the dependency as the devtools.

You can see Springboard DevTools provides fast application, restarts, live, reload and configuration

for enhanced development experience.

This is something that I want.

I will select this.

I'll go to the Browse explore section and I will copy this dependency.

This is the dependency that I need.

I'll copy this.

Go to my spring boot application, go to the XML file and I will add this dependency.

You can see that dependency is added and I will do the load maven changes now after this.

If you're working with Eclipse status or VTS code, you are good to go.

All those live changes will be reflected directly, but in individual idea you have to do some configurations.

So if you are on Mac, go to IntelliJ idea and preferences.

But if you are on Windows, go to file settings, open this and go to build an execution.

Deployment, go to compiler and just select this build project automatically.

After that, what you have to do is once this is done, go to advanced settings and enable this allow

automate to start even if deployed application is currently running.

Just enable this to settings and click on apply and okay, so now whenever you will do any changes within

5 to 10 seconds, it will reload each and everything and everything will be deployed for you so you

don't have to restart the application now and then.

So this is what Springboard DevTools in Springboard and it will help to reduce the overhead burden to

restart your applications.






Transcript - 

Now, in this section, we are going to learn about the web services.

We are going to learn what is Web services, how Web services work and what is RESTful Web services,

because we are going to develop a lot of RESTful Web services in this entire course.

So we need to have the basic understanding about what is a web services and the different aspects of

it.

So let's check this out.






Transcript - 
Now, what is a web service?

If I give you an example, like any social media application, Right.

Are those applications directly a web service?

It's difficult to say.

Ideally, if I had to define Web services in a wide general form, it would be like any services that

I can access over the network.

It's on the web, it's on the network.

So it should be ideally be able to access from the network.

The other thing is it is a service, so any application would be able to access those servers.

There's a two broad thing that I'm able to think about when it comes to Web services, but if I show

you in a more detailed manner, if application one is able to use the application to.

Okay.

Over the Web.

OC.

This is why my drawing is bad.

So please don't mind if any one application is able to access another application or web.

It's a web service.

It's simple right now, as I took the example of a social media.

Right.

Suppose let's take the example of a Facebook Instagram.

Or Twitter, right?

Anything.

Are those application web services?

Ideally, I can say no, but internally it might be if those application I am as a user, I am directly

using those applications.

So if a user is interacting with those application, I cannot tell those as a web services.

But if one application is accessing another application, suppose there is another application called

the analytics, right?

Analytics of the social media, if any of the post or anything, how the interaction is, how the impression

is and everything has been calculated right based on the data provided.

Right.

So that means there are some web services enabled here.

Which and analytics application can use those endpoints.

You can use those services exposed by this application and it can generate some data.

That's what something I can call it as a web services.

If we take the example of a Twitter right, so directly we can access Twitter right from the Twitter

application.

But there are a lot of other third party applications also available that can use Twitter and we can

use those application to tweet retweets and everything.

So that means it's not it's not a different application.

It's a Twitter itself.

So what Twitter is doing is Twitter is giving the Web services.

Twitter is giving Web services over the HTTP protocol over the Web.

Right.

So that means over the HTTP protocol to access or to be available by the other application to use.

So there are multiple application A, B, C, D, all this application can access this web services

and can use the data from this Web services.

So I can say that the general idea about the Web services now, you don't have to get over complicated

about all these things.

You just need to understand like this is what a web service would be.

Now there are different types of web services available.

That is a rest web service and a soap web service.

We will be mainly concentrating on the rest Web services.





Transcript - 

Now let's talk about how Web services work.

So let's take the example.

Here's how Web service works.

Now, as we saw that Web services is used to have the interaction between the two application.

If there is application one right, and there is application to both, this can interact with each other

on web via web services.

That means both are on the web and they both can interact with each other.

Now, how will they interact with each other?

So ideally what will happen is application one and this is an application to now application one will

send a request to the application to that.

I need some data or I need some processing to be done.

And on return, what application two will do is application.

Two will process this request.

It will get all the information that application one is requesting and it will do the processing and

it will.

Sandy response back.

This is the ideal or general flow for the request and responses.

So all the application one or any of the application.

If application two has exposed the services application, one can use those services via the request.

It will do the request like I need this data over a resource, over a URI.

So a URI has been called for that particular request and application.

Two will process those requests and it will send the response back.

So that's how generally the web services will work with the help of requests and responses.

Now all this particular thing will happen over a web on a particular protocol.

So if everything so if I tell all this thing.

Is happening over any one protocol.

So in this case, I'm telling this is happening over the HTTP protocol.

OC that all the requests and response has been handled for each and every application.

Now, how application one will get to know that what request I need to send and what response I'll be

getting and how application two will get to know that on what request, what response I need to send.

Right.

So that is something that we need to take care of.

We need to set up a contract for the services that we create for handling all the scenarios.

So ideally, whenever there is a web services in place for any web services.

For any web services.

There are some things that needs to be defined.

So the one thing is.

What is the request type?

Och, that means what request it will have or the request data.

I can say.

Look like it may be string.

It may be XML, it may be JSON, anything.

And what other thing would be like the type of data?

Like what type of data it is expecting and what type of data it is expecting back.

Either XML.

Or Jason.

Anything.

These are the standard formats used to parse the data.

Now, the other thing is what would be the.

Response object or the response data.

What data needs to be sent back for a particular service when the request has been completed?

And the next thing is the resource.

And I can say you are right where we are going to call the service.

So ideally, like maybe the URL.

Okay, like local host or daily codebook for dating whatever.

I suppose user is one of the UI you are slash.

User slash one is one of the UI.

So these are the different users can be defined and based on all this data, based on this particular

contract, web service is defined and all the different application would be able to understand what

data needs to be sent on, what data needs to be requested.

Now, all this thing, everything we have defined here will happen based on the different HTTP methods.

Now, why I'm telling that everything will happen on the HTTP method because we are using the RESTful

Web services.

We are going to work with the RESTful Web services.

So for that different methods are defined like.

Get method.

Post method.

Put the method.

Delete method and post and options and passion.

There are different methods available, so based on this different methods, you are right.

A resource is defined and the operation will happen.

So if I want to get the data, I can define a particular URI with a get method to get the data.

If I want to create any data I can define post, if I want to update, I can use the put and if I want

to delete, I can use the delete methods.

So this is a general definition and the general idea what are the things that define and how the web

services will work?






Transcript - 
Now let's talk about the RESTful Web services.

RESTful Web services means representational state transfer.

That is rust.

That means a service can be representative based on the state and how the data transfer will happen.

We will get to know about each and everything in detail.

So what does it mean is whenever we are creating a restful web services.

As we talked about earlier, like what All the definition that we need to take care about, what is

the contract that we need to take care about.

So if I consider there are there is a web services created that is a web service that is created now,

website is created now irrespective of any language or any technology or any framework.

We should be able to access this web services based on rest.

So that means it has to define some standards so anyone would be able to access it.

It's not like only Java application can be able to access a Java web service only.

The dot net application can be accessed the dot and web service.

It's not like that.

It's a representation of a service.

It means everything is represented in a form and any one would be able to use that.

So if this service is defined like how the data is needed to be processed, either it is processed based

on the JSON or XML.

Okay.

These are the two standard data formats used in exposing the services to send the data to and forth

what type of data it is.

What is the resource?

That means you are a OC.

Which you are.

We need to call.

What is the HTTP method?

OC like get put post or what it is and what would be the request format?

And what would be the response format?

Look, you can see that all these particular things are defined as a part of a contract.

In this web services now, everything is defined here and is available.

Now either it is a Java application or a dot net application or a go application or a Python application.

Look, whatever it is, they can go and call this services.

It's that simple.

Anyone would be able to come here and whatever the contact is defined and whatever the definition is

defined based on that, any one, any application develop in any of the language or framework is able

to get the data from here, from this web services.

Other than that, what it will define whenever the resource is defined, like for each and everything,

what we will define is what is the request like we will do the get operation will do the post operation.

Port operation or whatever.

Anything is defined for the request.

Like we will do the request this way in return.

We have to send the response back.

So in return, what we will do is we will send the response data back.

That will be.

Body like what data we need to send alongside will send the status messages.

Like, what is the status?

We will send a different status cause like 200 404201503.

There are different HTTP status codes available and all the status code define different things.

Like 200 is OC request is OC 201 is request is created or the or the data is created for zero four is

not found.

The result that you have defined is not found and so on.

So all these particular status codes are defined and based on that the body is also defined.

So whoever is calling this data based on the request that they have provided, they will get the data

back in terms of the body status and the header information OC.

All this data, they will return back here.

So all this particular application will get to know what happens.

So if if they got some body right in the JSON format, something something, something and the status

is 200, they can get to know, okay, this request was successfully completed if they got 404 and still

they got somebody, that means there was some error.

And what was the header that will be defined in the body itself?

So you can see that most of the things are defined in a structured format and a contract is defined

for the web services.

So what we are going to do is whenever we are going to create the different APIs for our microservices,

we are going to declare or we are going to define a contract.

And based on that contract, based on that definition, we are or all the other services can call a

particular web services.

So whatever the definition that we have defined, we will share those definitions.

And based on that definition, other calling services can call those API.

So you might have seen many places like if whenever you are using the API, a contract information is

shared and based on that contact information you will get or you will fetch all those APIs, all those

services.

So this is the general idea about what the RESTful Web services would be and how you can call the services

and what are the things that you need to define for a web services.

We will see each and everything in detail when we are creating the web services.

But this is a very basic idea, basic theory portion.

If you didn't know then if going through this would help.







Transcript - 


Now let's go ahead and create a RESTful Web services.

In the earlier videos we had already created when RESTful Web Services.

So let's go there.

We had created the home controller and here we had defined a web services, right?

We are defined a service.

We are defined the endpoint, like whenever you will call your UI that is local, call an 88 and the

forward slash, this is the method that will be invoked and you will get the data in the string format,

which is helloworld.

So if I check again, if I go to local host 8080, I'll get the data.

We had changed the port to 8081, so let me just change to 8081 and you will get the data that is Hello

World.

Now you can see that it's very simple.

And here what we had done was we had defined the controller and the response body both together.

That means this controller is responsible for handling the request, mapping that whatever I defined

and it will send that data in the response body format.

That means either in the XML or JSON.

So that means rather than creating or using two annotation that is at the root controller and other

response body, I can directly define that this is a rest controller.

That means I am creating the RESTful APIs.

RESTful APIs means the same thing, that it will handle the resources and it will send the data in the

response body.

It will send the data in the body alongside the headers and the status codes and everything.

So we have just defined the rest controller rather than using response body and controller.

And if you go to the rest controller, you will see the same thing that a direct controller is defined

and at the response body is defined.

So one last line of code.

So now by seeing this, you can easily understand that this is a rest controller.

That means this is going to handle all our RESTful web services and this is my API.





Transcript - 
Now let's create a new end point where we can handle the objects.

So currently you can see that we had only worked with a string.

But now let's create the object and let's see how we can send the objects back.

So to create the object, what we can do is we have to create the object over here.

What I will do is I will create one class and create a user class which will have some fields and I

will return that to user.

So for that, what I will do, I will create.

I'm model year, so I will create a package.

So let me just create a package.

I'll create a packet that says model.

And in this package I will create one class that is user.

And here I will define some fields that is private.

String ID.

Private string name.

And private string email ID you can see a simple thing three fields I have defined.

Now I will add the boilerplate code that is the getters and setters to make it as a podio write.

So I'll just generate here getter setters for all OC.

All the setters are done now.

Now I will go back to my home controller and I will define a method to work with that user.

So what I will do, I will create method public and what I want to return.

I want to return an object.

So I want to return a user object which I have defined.

So I will use that user and I'll give the name of a method.

You can give any name here.

Make sure that you give a meaningful name.

So I'm defining a user here.

And here I can say I can create the object of a user.

Okay.

And I can define user a dot set ID.

To one user set a name to.

Shabir.

And.

User dot set email id to submit added gmail.com.

You can see that we have just set a few of the fields here and the user object is created and what my

return type is user.

So I will just do return.

User.

Simple, right?

Now, I need to make sure that this method is able to handle those API.

Handle that request.

So I need to make sure that I use the request mapping.

So I need to map a particular request.

Now what I will do is I will map a request with slash user.

So whenever you will do slash user, you will be able to get this data.

Okay, So the simple I would define if I go to run.

You can see that using our devtools it is automatically recompiled and deployed.

So if I go back.

If I do slash user, you can see that you are able to get the data back in the JSON format.

You can see this is a JSON format and you have fields, ID, name and email ID Now by default, whenever

you will do such operation, it's a get request, right?

So you can see that this is a get request I'm doing if I check in the postman as well if I open postman.

Postman is a tool to test your rest API so make sure that you install postman as well.

Okay.

This is the postman.

What I'll do.

I'll do the new request and.

I will copy this URL.

Okay.

And I will add this URL and you can see that this is the UI that we have defined.

And yes, we need to define the type of the HTTP method.

You can see all those types are there.

We are using the get method by default.

It's a gate method.

So we define this gate and we will do send.

And you can see we are getting the record here.

We are getting the same data here and you can see that we are getting a lot of other information as

well, like 200, which we defined, right?

This is the HTTP status code.

So if you go to the headers, you will get the header as well.

Like content type is the application JSON.

So we are getting the data in the JSON format.

Transfer encoding, is there data, is there key?

Is that all those values by default has been added by the springboard?

Now, as we saw, this is a great method, but we have not defined that this is the gate method, right?

We just define this as a request mapping.

So what it does is by default it is always a good mapping.

So if you want to define other things, you can define such a way that whatever the user you define,

you have to define in the value.

So value is defined here and then you need to define the type of a method.

So suppose you are defining what is the method.

Method is request method that get this the same thing as earlier, but by default it's always a good

method so you don't have to specify.

But if you are working with the another method you can define.

If you're working with a post, you can define post here and let it rerun your.

You can see that it is working now.

And if I go back and if I'm using get and if I run again, if I go to body, you can see that method

is not allowed, right?

So that means if I do post here instead of get because we have defined post, right.

And if I do sign you can see that I'm getting the data right.

So post is allowed.

So this way you can define.

But let me make sure it's a gate because it was a gate request.

Now you can see that just to define it's a gate request.

I have added a lot of things, right?

We have to define a value and then we have to define a method as well.

So rather than using this way, Springboard provides an efficient way to do that.

Rather than defining a direct request mapping and defining the value of that request mapping and defining

the method type, there are stereotype annotation that you can directly use.

If it's a gate mapping that I have to use, I can directly define it, get mapping simple, and I can

define the value that is slash user.

You can see that your line of code is much easier to read.

Now, if you want to define the post mapping rather than request method that you want to use request

metadata post, you can directly define it post mapping simple and you can pass on the URL.

And if you want to do put, you can define with put mapping.

If you want to do delete mapping, you can define with the delete mapping.

You can see how simple it is.

Simple annotation you can directly use to define your method types.

Let me just remove everything here.

Now, if you see here in the grid mapping, it's the same thing.

It is defined that request mapping calls to method equals to request method.

So it's defined the same thing.

But rather than doing everything by your own already in annotation is defined, so you can directly

define the annotation and you're good to go.

So I will just comment this out and we will read it in our application.

So let it reload.

The application application is reloaded and if I go again, you check with the post should not work

and if I change back to that, it should work.

You can see that it is working now.








Transcript - 
Now let's define path variables.

Now what is path variable?

Path variable is a path that you define in the UI.

Suppose here if you define slash user slash one, slash two, anything, any values that you define

right, that is called a path variable and path variables are always a mandatory mandatory thing.

So whenever you need something to be defined in the UI and based on that you are, you need some different

resources and, and some different configurations or different processing in your web services.

At that time.

You can use path variable.

Be sure that path variable you can only use when that particular information is a mandatory information.

So suppose within this right user slash one I have defined here and if I run this it will not work because

it will not found here.

So that means whenever it's a mandatory field and if you want to get that information, define the path

variable.

So let's see how we can define a path variable.

So what I'll do, I will go here and I will create a new method.

This is a new method, public string, and I defined path variable as a method name.

Look and what I'm doing.

I am defining a gait mapping here.

Simple.

Now what I want as a well, whenever I do slash and if I do any ID.

OC.

You can see that I have added in the parenthesis here that whatever I defined here in the idea that

is something, a dynamic field.

So whenever that particular value is passed, that is a dynamic field and I want to process that.

So whenever something is defined here, you need to take this as an input value in the method.

So I can define spring ID here.

So this is something that I'm taking as an input.

Now, how it will get to know that whatever is defined, it needs to take as a path variable in the

ID.

So for that that is an annotation that we need to define.

That is at the rate path variable.

So whenever a UI is mapped and any value has been passed, that value will be mapped to the string of

ID here using this edit path variable annotation simple.

And here I can define return.

I'm simply returning whatever is parsed return.

Depart variable is.

Plus ID simple thing application is reloaded.

So let me just go back and if I do slash user slash one and if I do send, it won't work because my

gate mapping is defined slash ID.

So if I just remove user here and if I do send.

You can see that I'm getting the path variable is one if I pass.

ABC.

Then it will sell.

The path variable is ABC, so you can see that whatever you will define here, it will work.

But suppose if I do ABC slash X, Y, Z.

And if I hit send, then you can see that it is not formed because it is not available until you are

only we have defined.

So if you want to define multiple path variables for a particular resource to be handled, you can do

this way.

Slash ID and after slash you want any other path variable as well.

So you can define ID to.

And here you can define string ID to.

And this is also a path variable right at the rate path variable.

So you can see that first ID, it will match with this ID and it will pass the value.

Second ID two will match your ID two and it will be in the second path variable.

And I can tell plus ID to let the application restart now.

Application is restarted.

Now, if I go back again and if I run this, you can see that it is working.

The path variable is ABC and X, y, Z.

Now one more thing.

Now you can see that the variable that we have defined I don't like the variable though it is whatever

it is here.

So what I want, I want this has to be a name, whatever it is, I need this to be a name.

But I have defined here as ID two.

So what I can do is here in the path variable, I can define which particular variable that I need to

take.

I need to take ID two from this.

That means the second path variable and attached to in the name variable.

That is something that we have defined.

So the application is restarted.

Let's check now.

It should work as it is.

You can see that it is working completely fine.

So you can see that we saw how the path variable is defined, how the different ways we can define the

path variables and how we can use the path variable.

So whenever there is a mandatory data that you need to take at that time, we will define the path variables.

Now, in the next video we will see request parameters.








Transcript - 

Now, in the previous video we saw about path variables and how to define path variables.

In this video, let's define the request mapping.

Now when to use request Mapping Request mapping you can use when you have some of the data that needs

to be taken.

But those are not mandatory.

Whenever the data is mandatory, you take it using the path variable.

But when the data is not mandatory at the time, you will take using the request parameters.

So you can see path variable was declared with slash values right as the path forward slash, whatever

the URL would be like whatever the ABC or XYZ dot com slash, whatever the parameters is there, whatever

the path variable that way you will define slash parameter one that is path variable one slash path

variable to that way.

But request parameter will be defined with question mark.

That is the query parameters.

So after the URL, it will be defined as a question mark.

And then whatever the request parameters you define, those request parameters will be added.

If those are multiple, multiple request parameters will be added separated by the ampersand symbol.

So let's define a method and let's see in working.

So I need a method that returns string to handle the request params.

So I'm just defining a method name request params and I am attaching a request handler.

So request mapping we need to attach.

So we will do get mapping here and.

What I will do is I will define request param as the URL so local host call an id slash request parameter

will call and then whatever the request params are there we can define here.

So suppose I need a name.

So if I need name I can define a variable string name and I need that name from the request param.

So I'll define request parameter.

And I will just return that return.

Your name is.

Her name OC.

Let the application reload and we will check this.

The application is reloaded.

Now let's go to the browser and what we will do rather than 8081, whatever was defined, it was request

params.

So let me just copy this.

And it was request param.

And if I use this directly, you can see that it's an error.

It's not working because there is a required request parameter here.

So we need to define the request parameter.

So we'll define request parameter with question mark.

And then the name of the request parameter that is name.

So I'll define name equals to Shabbir and enter.

You can see that it is working completely fine that I'm getting your name equals to Shabbir.

Now let's define another request parameter.

So let me define another request parameter.

I request param and this is email ID.

Okay.

And here I'll define.

And email ID is.

Email ID.

Simple code.

Whatever I'm passing, I'm just returning back.

Let the application restart and let's go here and let's run the application.

You can see that again.

We got the error because now we have defined to request parameters and I only passed one.

So let's pass another.

So to pass another we will do ampersand symbol after the first parameter and then we'll pass second.

The second name is Email ID, email ID, the same name we have to give email id equals to shall be read

read gmail.com.

And you can see that it is working completely fine.

But now I don't want email ID as the parameter here.

I want only email.

So what I can do is either I can change this particular variable name here or I can define here.

That.

My name should be email that the application restart application restarted.

Now if I go and run the application again, it will again fail because we changed the email ID here.

Right now it should be email.

And if I run, you can see that it is completely working fine.

Now what I need is I don't want this email ID as a mandatory field.

If I don't give email ID as well, it should work.

It should not fail because these are non mandatory fields.

If it fields are available, then fine.

If fields are not available then that's okay.

Right.

So what I can do is I can define comma required equals to false.

That means if value is available, then fine.

If value is not available, then fine.

Then also it's completely fine.

So if I go again here, if I run this, you can see that your name is Shabbir and email additional because

email ID is not provided if I provide email ID.

And.

Email ID equals two shall be at gmail.com.

It will work because we just pass email addy.

It should be email, right?

Email.

You can see that it is working completely fine.

Now what I want is I don't want to pass this email ID, but I don't want it to be null as well.

So if it's not anything right, if we are not passing the data at that time, I want that default value

to be set for that to be blank.

Now let the application restart and we will check.

Okay, We will check and let's run this again.

And you can see that NULL is gone and you are getting a blank value.

So this way you can see that we can handle the request parameters.

You can see that we have gone through all those different types of parameter handling.

We went through how we can handle with the different name with required false default values and everything.

So this way we can handle the request parameters.

So keep in mind, whenever you want the mandatory fields, mandatory values to be taken, use the path

variable.

Path variable will be always mandatory and the request parameter will not be mandatory.

And if you want to handle that, you can always handle using the properties that that is required false.

And you can pass the default values whenever you want.

So these are the different properties that you can use to create the request parameters and path variables.

Now these are the most important and main things that we are going to use in our entire course.

So if you are able to understand this and you are able to create the rest endpoints with the path variables,

request parameters and the default values, you will be able to easily grab all those particular topics.

So that's all in this video.

I will see you in the next chapter.






Transcript - 
Now, as we have understood the basics, let's create a credit application where we will use the employer

as the object, and on that employee will do some operation.

So we will create the entity object for an employee.

And with that employee, we will try to create a credit application where we will try to save the employee,

get all the employees list down, all the employees four and list employees based on the ID, and we're

going to delete that particular employee as well based on the ID.

So for all this, we will create the different APIs.

So let's see how all these things will work.

So as we are going to work with the employees particularly, so what we'll do is we'll create the employee

resources.

That means an employee controller, which means for all the requests to handle the employee will come

to a particular controller and that controller will be responsible to do all the operations for that.

So within the employee controller, there will be different methods and all those different methods

will handle all the different types of requests and to handle that employee to store all the data for

that employee, we will create a model as well, a class a project class in Java in which we will have

some properties and that properties we can fill to store some data.

So that's what we are going to do.

As in the previous example, we had used the user model here to store some of the data.

Now we will use the employee model, so let's go ahead and create a model.

So what I'll do, I'll just create a new class in the model package itself, and I will name this as

a employee.

OC.

This is a simple Java class within this.

Let's add some fields.

So what I'll do, I'll add some fields private.

String.

Employee ID.

Private string.

First name.

Private string.

Last name.

Private string email ID and.

Private string.

Department, which department this employee belongs to?

Okay.

These are the few fields that I have added in the employee portfolio.

For this now, let's create the getters and setters.

So I'll create the guides and setters.

If you want this generator options, you can just right click on it and.

Use generate options for Mac.

The shortcut is command end, but for windows I think it is command insert, so just check it and use

that shortcut to generate this.

I'll click on generate and I will generate the status.

I'll select all these fields and click on.

Okay.

And for this you can see that the status is created for each and everything.

Now what we will do, we will create a controller layer.

So within the controller package, let's go ahead and create one controller.

Create a class and I will mention this class as a employee controller.

All the spelling is correct.

Yes.

So the employee controller is created.

And for this employee controller, as is a controller, and with this controller, we are going to create

the rest APIs.

That means we are going to send the response body back based on some of the request.

We will annotate this class with a rest controller.

So this means my this class is going to be my rest controller.

And what one more thing I will do is in the previous example you had seen that in the home controller,

we had given the mapping for each and every method and for each and every mapping we had provided the

URI like on which you are.

When the application hits, which particular method has to be invoked and how the processing has to

be done.

But for employee, what I'm trying to do is anything comes with slash employee should come to this particular

controller.

So rather than defining all the things in the method itself, that particular base path I will define

in the controller itself.

So that means anything coming to that employee will come to this controller.

And then for each and every different method, we can define differently what things to be configured.

So I will define request mapping here itself at the class level and anything coming with slash employees

will come to this controller.

Now, controller is always used to handle your request, but once you get the request at your controller

level, all your processing should be done at your business layer or at your service layer.

So whatever the validations or whatever the processing has to be done, all those things has to be done

at your service level.

So that's the standard architecture or standard format used across the industry.

So we will use that as well.

So we will create a service layer or you can say a business layer where we will define all our application

logic.

So the controller layer is defined.

So now we have to create a service layer.

So for service layer, what we will do is we will follow the interface pattern, where we will define

the interface and we will use the implementation because in the business layer there might be a chance

where there are multiple implementations for a particular logic.

So we'll go ahead with the interface design where we'll create the interface.

And for that interface there might be an implementations available.

So we will create a package for that.

And in that package we will define our service layers.

So what I will do, I will create a package at the root level and I will define a package and that says

it's a service.

OC and within this service package, what I will do, I will define an interface that says employee

service.

So I will define.

Employee service and this is going to be an interface.

So this interface is defined.

Now for this interface, I need to have the implementation.

So within the service package, let me create the implementation for this.

I'll create a class that says employee service implementation.

OC and this particular employee service employer class is going to implement employee service.

Cool right now.

This employee service implementation is going to be my service layer.

So for this I'm going to define this at the rate service annotation.

It is a stereotype annotation.

So with this we'll just get to know the information.

Like this is a service layer, but if you go inside, this is always annotated with a direct component.

So whenever this is defined, spring will know that this class has to be used for our bean creations,

and it would be in this spring's radar.

So you can see that our service layers are created.

Now, whatever the business logic has to be, then we will do all the business logics at this particular

point.

Now, at the controller layer, we will need this particular object so that whatever the business logic

is defined in this layer, we can call those particular methods or the different behaviors for it.

So as you can see that this is the implementation of the employee service interface.

So what we will do is we will inject this particular interface itself.

So what it will happen is whenever we will directly inject the interface, whatever the subsequent implementation

would be, those implementation for that interface will be injected.

If there is only one implementation available, that particular implementation will be directly injected.

But if there are multiple implementation, don't worry, we will see later in the course how to handle

the multiple implementations as well.

So currently we have only one interface and for that interface we have one implementation.

So directly we will use this interface to inject our class.

So we'll go to the employee controller and here I will need the object of an employee service to call

the methods of the employee service to do any operation.

So I'll define private employee service.

This is the interface that I've defined and what I will do is I will inject it using the auto wire.

Once I do this, whatever the implementation is available, if there is only one implementation, that's

fine.

Directly that implementation will be injected.

That class will be injected here.

So I will get the object of the employee service employer.

Cool.

Now what I have to do here is I have to create a method that is responsible for saving the employee.

So whatever the data is sent, that data has to be saved in my database.

Currently we are not using database or anything.

Currently we'll store everything in the in memory, but later in the tutorial.

Later in the course, we will see how to implement the database as well.

So let's create a method that will handle the saving of the employee.

So I'll create a method as we seen earlier as well.

We will declare it public.

And what is the return type?

Suppose my return type is employee.

Whatever the data is sent, we will send the data back that this particular method has been saved.

Sorry, this particular employee has been saved.

So we will define employee and we'll define the name of the method that is saved.

Okay.

Simple.

It is now here.

What will be the input?

We are taking the entire employee object as the input.

So we'll define employee object here.

And this particular employee object will come as the request body.

So we'll define request body annotation.

So we'll get the entire request body and all the fields over here.

Okay.

And this method will be called when we are doing the post request post at HTTP request for slash employees.

So whenever I will do slash employees post, this particular method will be called.

So that's why we have just defined a direct post mapping here and no URL because this is the URL we

want to use.

If I want to modify this URL, what I can do is suppose I want to do slash ABC.

So what it will do is it will always call slash employees slash ABC on that post.

This particular method will be called this, but at this particular point that resource will be handled.

But what I want is I want slash employees post request to handle this.

So I'll just remove this here.

Okay, so this way it will work.

Now, at this point, I will get the employee.

Now, what I want to do is at this particular point, I want to call my business layer at my service

layer.

And that service layer will be responsible for handling this employee.

So what I'll do, I need to have the methods, what I'll do.

I will go to the service layer here and I will define a method.

So at this point, I will define a method that.

I want to return employee on Sue and this should take the employee object.

Look, this is a simple matter that I have declared and I need to go and define and implement this matter.

So I'll go to the implementation class.

That is the.

Employee service appeal.

Here you can see that it started giving me error that you need to implement a method.

So I'll go ahead and implement the method that is the SEO.

Okay.

And this is the method that I need to implement.

Now, for now, what I'll do, I'll create a list here to save all these employees.

So let me just define the list here.

That list of employee M.P., ll 0ye employees equals to new error list.

So I'll just define one error list.

The spelling mistake is there, so we'll just correct it.

Now, I've just defined the empty list here.

Now, whatever the data will come here, I will try to store all those returns in this list.

So now you can see we are in the business layer.

So in this business layer, what we will do is we will try to handle this.

So here we got the employee.

So now you can see that this is the particular save method that we got here.

And here at this particular point, we need to do all the validations and all of our business logic

and everything here.

And accordingly after that, we can go ahead to save into the database or whatever the processing that

we need to do, we can do here.

Now, one validation that I want to do is if this particular employee object is coming, if that particular

employee object is having the employee ID or not.

If not, I will create the employee ID for it and save it.

So that's some validation that I'm thinking we can do here.

So what I'll do, I will just create condition here.

So you can see that this is one of the business logic that I'm implementing in a service layer.

So if employee dot get employee ID equals two equals to null.

Or.

Employee dot get employee ID dot is.

Empty in either of these cases, either it's null or empty.

I want to generate it.

So what I'll do, I will just do employee dot set.

Employee ID with.

You uid dot random uid dot to string.

So I've generated one UUID and I'm saving it here.

Now, once this is done, I will do employees dot add employee.

I am adding the employee in my list and after that I am returning the employee.

You can see that it's a simple method that we have implemented in our service layer.

Now this implementation I can directly call from my controller layer.

So from the controller layer what I can do, I can call return employee service dot save method because

we have defined in our interface and we have also defined the implementation for it.

So we'll call the same method and we'll pass the employee object to it.

Simple.

You can see that how easily we were able to implement our control layer.

And we also were easily able to implement our business logic layer and we are trying to store the data

in our list OC.

Now let's go ahead and start our application and see what is happening here.

So we will start our application.

And you can see the application.

It started with jetty on Port 8081.

So we will go to our postman to check this post request.

So we'll go to the postman will go to the new request and here will do the post request because we have

implemented the post request here and here we will define HTTP colon forward slash forward slash local

host colon 8081.

Slash employees because this is a URL and we have to pass the body.

So we'll go to the body.

The body is raw JSON format.

So we define the JSON format and here we can parse the JSON.

So we'll check what are the fields.

So we'll go here.

Employee object and will define.

First name your.

We have to pass this way as the JSON format.

First name, last name, email ID department.

So let me just copy all those information.

Last name.

Email ID.

And the department.

Okay.

You can see that how simple it is.

Now, I will just define fields here, Shabbir.

Dowdy.

Shall be read the read gmail.com.

My God.

Spelling mistakes.

That's fine.

And department.

I'll say it dept and last.

There will be no comma.

So this is my payload.

Let me just beautify it.

This is my payload and this is my URL.

So once I hit send, what it will do is it will come to my employee controller.

At this particular point, and this is a post mapping, so this particular method will be called.

So let me just hit send.

And you can see that we received 200 OC and we got the employee object back.

You can see that the employee ID we had not pass, so employee ID was created using the UUID and we

got the data back.

OC Cool.

You can see that the post request is completely working fine.

Now let's go ahead and implement the get request.







Transcript - 

So now what I want is in the get request, I want to get all the data which are already saved.

So what we will do is if you go to the service layer.

Okay, So that means we are trying to store all our data in our employee object here.

So I want all the data back from this employee's list.

So we will create a method here for a particular URI that will give us the all the records.

So we are going to create one rest API endpoint.

So what I'm trying to do is whenever I will hit slash employees with a get request, I will get all

the records.

So that's what we are going to build today.

So what I'll do, I will just define the method public and I want a list, so I'll define public list

of employee.

And I define get all employees.

Look and you can see that for this particular method.

I don't need any input parameters because I need all the data, so I'm not going to pass any parameter

here.

This list I will import from the java util and I will annotate this with get mapping because this is

going to be a get mapping for slash employees.

And now what I need is I want to have a method that returns me the list.

So what I do, I will just do return.

I want to return the list where and I want to call the employee service my business layer and it should

have a get all employees method.

Okay.

But currently this get all employees method is not available.

Right?

So we have to create it.

IntelliJ idea or any of the ID which you are using that allows us to create or generate all these methods

very easily.

So what we will do, we will use that method.

If you are using an idea, if you just right click here, go to show context menu and you can see that

you will get the option create method, get all employees in employee service.

So once you hit this, that particular method is created, you can see that this method is created and

if you go to its implementation, you can see we have one implementation for this employee service.

So if you go here and.

You can implement that method.

So I'm just implementing this method and you can see that implementation is there and now it is returning

null.

So rather than returning null, what I will do, I will just return my employees, which is my list

here.

Right.

So you can see that the implementation for this is also completed.

We added the method in our employee service interface.

We also added added the implementation for it.

And if you go to our controller layer, you can see that it is also completely fine.

All right, so now let's restart our application to check our API.

Or if you are using DevTools, then let's wait for this to get processed to rebuild the application.

So for me it's already done.

So what I will do, I will go to my postman and I will try to save the same request.

So I will try to click on send.

One request is saved.

I will try to save one of the data that is one.

Kumar Shivam at gmail.com and IT department.

Let's try to save that as well.

You can see that two records are saved.

Now what I will do, I will change to get request because we have created the get request here, Right.

This is the get request for employees list.

And if I hit send, you can see that I should be getting list.

You can see I'm getting list.

This is the first request.

This second request.

If I use this in the browser.

You can see that you are getting both.

The record is the first record and it is the second record and in the JSON format.

Cool, right?

You can see that we have just implemented the post mapping and get mapping for a particular CRUD operation.







Transcript - 
Now let's implement one more method where we will try to get a particular employee based on the ID that

is provided.

So currently you can see that we are getting all the employees currently right now.

What if, if I do slash and if I give any of the ID, then for that particular ID I will get a particular

record.

So that's what we are going to build in the next API, where based on the ID provided, we will get

a particular employee record.

So let's go ahead and go to the idea and here I will create a new get mapping.

So what I will define here is public employee, because I just need one employee.

And I'll just define the method name as get employee by ID because we are going to get the employee

based on the ID provided.

And what I will do is I need to get the idea as well.

So in the input parameter, I will mention that I need a particular ID and you're in the get mapping.

We will define your get mapping.

And here is something that we need the ID, right?

Because after a slash employee is mentioned here, I need a particular ID, So after this I need.

Ideal as a path variable.

And you already know that whenever we mentioned path side, it's a mandatory field, right?

So that's a path variable.

So we need to define a path variable annotation here.

So we'll define a path variable annotation.

Now from here we need to return the employee object.

So we'll define return employee service dot.

Get.

Employee buy ID is a simple method that I defined, but this method is not available in our employee

interface.

That is employee service and the implementation is also not available.

So you can see that create method.

I'm getting a suggestion that is create method, get employee by ID in employee service and which is

something that I intend to do.

So I'll just create on that, which will create a method here.

And then I will go to the implementation class for this interface and I will implement the method as

well.

Okay.

So you can see that this particular employee by ID is created.

Now, what I want is from this list, I want to get that particular object.

So I'll just define employees stream.

To implement this using streams that are multiple ways, but I'm just using a filter operation to filter

that out and to get that result.

So I'm just trying to do filter.

Filter based on what?

Filter Based on the employee where Employee dot.

Get employee ID.

Okay.

And one more thing we forgot here is that we had to pass the employee ID, right?

Because with what we will compare.

So this type of mistakes always happen.

Don't worry.

You can always go to your controller and you can pass the idea.

So pass the ID.

And.

Go to D and get employee buy ID method.

And here, just take this as a string ID.

Go to the implementation for it.

And let's take this string idea and I need to compare this ID dot equals with the ID provided.

Once that is done, what I need to do is dot.

I just need the first object of it.

So I'll just do find first dot get.

So you can see that it's just a simple stream that I provided where with the streams of employees,

I'm just filtering out the particular object with which the employee ID is matching.

I'm taking the first out of it and I'm just returning the data back.

So if I go back to my controller and you can see that this particular method is implemented.

And what I will do.

I will go back to my post, man, and let's do this.

So before this I need to add the data because we are using the in memory for currently.

So I'll just hit send to save the data.

And if I do get request for that, I'll be getting that particular record here.

Right.

Now what I will do, I will just copy this ID.

I will do.

Get employees slash particular ID and if I hit send.

There is some issue here.

Right.

Let's see what is the issue.

Let me just go back to the previous one.

If I do get employees and you can see that this is a particular data.

And if you go back to our method, what we had done here in the implementation.

Okay.

Let me just try to do with the equals ignore case here with the ID and let me restart my application

and check.

Okay.

The application is already restarted.

If you go if I go to my controller layer, I'm getting the path variable as the ID itself.

And if I try to pause the data first, the data is posted.

This is the ID.

If I go with the get request for all the employees, I'm getting this request and if I copy this.

With Slash.

And I'm not able to get the data.

That is great.

You can see that we did the wrong thing here.

And the issue was we use the get email ID should we get employee ID?

So that was a mistake.

So don't worry guys.

Sometimes we will make mistakes.

So this is the employee ID that we are using now.

Now it should completely work.

Fine.

If I go back to my post, man.

Okay, let me just try to store the data.

Let me just remove this URL.

And we will do posture and we will hit send.

The data is saved here now with this ID.

If I do get, I should be able to get the data.

Your post is not allowed.

That's fine.

We need to do get here.

So we'll do the get method and we are able to get the record here.

Cool.

You can see that it's completely working fine.

But what if the data is not available?

Right?

We were getting the earlier as well.

So if I'm giving any particular ID, the data is not available and you can see that I'm getting a huge

error here, right?

So this is something that we need to handle.

So in the next video, we will handle this.







Transcript - 

So now in this session, let's handle the exception that we are getting when there is no records available.

So we'll go to the intelligent idea.

And within this method itself, you can see that whenever we are doing get, that's fine.

But what about if the data is not available?

Right?

So at the time, what I can do is rather than using get because you can see that this is fine first

is something that is sending the optional.

So rather than using get what I can do or else here or else throw the exception.

Okay.

This is something that I have defined so I can define this way.

Like rather than getting if the data is not available, let's throw the exception.

So by default I can use new runtime exception and you can pass that employee not found with ID.

It provided.

It's a simple thing that I'm doing here.

But generally when you are building your application, you will not use the standard exceptions or the

runtime exception.

You will create your own custom exceptions to handle that.

So what we will do, we will create our own custom exception, and that custom exception we will throw

from here and we will handle that exception in the rest API is how to handle that.

So to handle such kind of scenarios, we need to have custom exception.

And what I will do, I'll create and for this, what I will do, I will create a new package.

And in that package I will store all the exception related classes.

So in this package I will just mention this is an exception or an error.

Whatever you want you can define here.

Okay, I'm just defining the error here.

And in this error package, let me just define a class that's a custom exception and I'll define employee

not found.

Exception.

This is a simple class that I've defined here and this particular class.

Let me just refactor this spelling mistake.

It should be.

Employee not found exception and this will extends.

Runtime exception.

And this particular class will have one constructor which will take the message as the input.

So I'll just define public constructor.

Employee not found exception which will take strong message.

And we will call the super of it.

So you can see that it's a simple exception that I defined here.

And this is something we need to pass back.

So what I will do is if I go to my service, I MPL, and whenever that particular employee is not available,

rather than throwing runtime exception, I will throw employee not found exception.

You can see that now code looks really clean and from this we will be able to understand as well.

Okay.

This is an exception where employee is not found.

Runtime exception is very generic.

Right?

But here you can see that from here we are able to understand.

But what about from the API?

You can see that whenever we got the request it's the use request and we are not able to understand

much from here.

So we have to create a particular response as well.

So whenever there are exceptions and whenever we send the response back for this kind of exceptions,

there is a particular way that we can represent it.

So that representation also we need to define like what would be the response body when there is an

exception.

So for this, we have to use the controller advice.

So if you don't know what the advises, it is the concept of the EOP that is the aspect oriented programming

where you can create the crosscutting concerns or handle the crosscutting concerns outside your standard

code.

So what I'm trying to do is form a controller.

I am trying to create the advice around it.

So what that advise will do is that's a cross-cutting concern, which I'm trying to write outside my

outside my controller.

So whenever there is an exception, I don't have to handle all those exceptions within my controller.

There is an advice created and for all those controllers which I have in my system, the advice is there.

And that advice will run when, whenever there is an exception to it.

So whenever a particular exception occurs, that particular advice will run and it will send the response

back.

So that's what we are going to do.

So we are going to create a controller advice that will handle all those particular things for us.

So what I'll do, I'll create a class, that's the rest request response handler exception.

So within the error package itself, I'll create a class that is addressed to response entity.

Exception handler.

So this is a simple class that I'm defining.

You can give any name.

And this class will extend.

Response entity exception handler.

Simple it is.

Now, this class is something that we are using to handle all our exceptions, right?

So we will annotate this with the controller advice as I have defined.

Earlier.

Okay, so this particular will run this particular class will run whenever there are crosscutting concerns

to handle for a particular controllers.

And now what is my cross-cutting concern here is I want to handle that exception.

So whenever for a particular request that a particular exception is thrown, that is the employee not

found exception, this class should be able to handle that.

For me, I'm not writing all those logics in my controller layer because it is not part of the controller.

Controller is just to handle all my requests for me.

So that's why we are doing this way.

And now here we will create a method that will handle that particular exception for me.

So I need to create a method.

And what is the return type?

So what I need in a return type is I want to send a particular object back.

That object should have some of the information from which we should be able to understand what is the

particular error message.

So ideally we want to send a particular message back.

That is the exception message that we have defined and we can also send What is the status code for

it?

You can define all the different fields as well what you want to send.

But for now what we are going to do is we are going to define these two fields.

So you will get the understanding how those things work and you can always implement on it.

You can always enhance this functionalities based on your requirements.

So what I will do, I will create the model for it because I want to send the data back, right?

So for that I'll create a model and I will name this as the error message.

Simple.

Now, within this error message, what I want to do is I want to have HTTP status and this is the part

of the OR spring framework HTTP status.

And private string message.

You can have other properties as well, which you want to pass the back.

But for now, this is more than enough to understand how that particular thing works.

And for this, what I will do is I will create the getter setters.

So you can see that the getter setters are created.

And I can use this error message to send the data back.

So within this rust response entity exception handler, I will use error message to send the data back

and I will name the method employee not found.

Handler OC.

And this is my method.

Now, for this particular method, we need to inject which particular exception we want to handle.

So we want to handle employee not found exception.

So we are injecting that exception.

Now this particular method also needs to be annotated for spring to understand what we have to do.

So this particular method is the exception handler for what?

So we need to define the annotation.

That is the exceptional exception.

Handler Exception handler.

So this is the exception handler method and for which exception.

So that is the employee not found exception class.

And this method will return the response body in return.

So that's why we are defined response body.

And we also want to have a particular status as well to return.

If you don't define this by default, it will send us a 200.

But I want a different status message to be returned for I should be status message.

So I'll just define response status.

Again, that status has to be HTTP status.

Not not found.

That is the 404 not found.

Now you can see that we have annotated with all the details which we need now.

We need to create the error message object and that we need to send back.

So we'll define error message message equals to new error message.

And now you can see that there is for this error message, there is only a default constructor available,

but I have to fill.

So for that I will create a constructor for it.

So I'll go to the error message and there are only get the status created.

Right.

So what I will do, I will create a constructor.

First, I'll create a constructor without any fields that is a default constructor and I will create

a constructor with my two fields as well.

That is the status and message.

Now if I go back to my crash response entity Exception handler here, I can pass two things.

That is the HTTP status DOT not found and the message itself.

Exception, not get message.

Now these two things are defined and now what I have to do, I just have to return this message object

so you can see that it's very simple thing.

What we did was if you go to my service layer, whenever the employee is not available, I am throwing

the employee not for an exception, but this employee not found exception has to be handled somewhere.

Okay.

If we try to handle all this kind of exceptions at the controller layer itself, it's not feasible because

we have to handle a lot of things.

So for that we are using the EOP concept.

That is the aspect oriented programming concept where we are defining the advice for our controllers.

So that's why we are using at the rate controller advise annotation.

So that advice will work whenever there are any exceptions.

So we are creating that class for our exception handler only.

So whenever we are raising the exception or whenever there is an exception in our program, that particular

class will kick in.

It will try to figure out which particular exception handler has to be called, based on the exception

that has been thrown.

And it will create the response object for us and it will send the data back so that from the client

side, whoever is getting the request, they will get the exact information which we needed to send

from our application.

So rather than passing the generic data, this will be the proper data that will be sent by the application.

So now let's go and check this.

So if I just remove this and if I.

Convert to post.

And if I hit send, that means my data is created, right?

So if I try to get the data with this ID itself using the get method, I should be able to get the data.

But if I change this ID, that means when the data is not available I should be getting the proper error

message.

You can see that now I'm getting the proper error message.

Earlier you had seen that it was really big and it was not understandable as well.

Now you can see that it's very simple.

We got this status message as well, status code that is for zero four not found rather than 200 and

status.

Also we got that is this is not found and message also we got that employee not found with the ID this

now you can see that with this it's easily understandable what is going with our application.







Transcript - 
Now, what if there are generic exceptions?

So we need to handle the generic exceptions as well.

Right.

So what I will do is we had already handled one exception, so let's try to handle generic exception

as well.

So if you go to our IntelliJ idea, this is something that we implemented for one of the exception that

is the employee not found exception.

Now let's handle for the gender exception as well.

So what I'll do, I will copy this entire thing.

Okay.

It's very simple.

Okay.

Don't worry.

So what I'll do rather than employee not found exception, I will define that general exception itself.

So whenever there is exception overall this should be called and rather than employee not found exception,

we will pass the exception here and rather than not found, this will be internal server error because

this is a general exception.

And for that we pass the 500 that is internal server error and we will pass the same thing here as well.

So you can see that we created the another method.

And you can see it's very easy, right?

Earlier it was employee not found, exception clause and all those things.

And yet what I will do I will change the name handle Kendrick or rather than.

Kendrick Exception.

HANDLER This is a generic exception.

Handler.

You'll pass the error message again, which will handle the exception class.

And in return, this will be also getting the exception as an injection.

So we are injecting the exception here and we are passing the data with the internal server error.

Now, to test this, what we will do is rather than passing the employee node found exception, I will

just pass the runtime exception here.

Now let's test this.

I will go to my postman.

And let me just directly test this so you can see that earlier what we had got was not found, an employee

not found with ID and where we were getting the status as 404.

But now if I hit send, you can see that I'm getting internal server error with 500 internal server

error as this status course.

So you can see that this is handled by my new method that I had created in my rest response entity.

That is this one.

Okay.

Now, if I again change back.

To my employee not found.

So this time this should be sent back.

Okay.

The changes are reflected now.

Let's test this again.

And you can see that now.

I'm getting for zero for non form exception.

So if you want, there are multiple exceptions.

You can add all those multiple exception handlers and accordingly you can send the data back and your

application would be able to handle all your exceptions.

So this is how you would be implementing exception ending in your spring boot with the rest APIs.






Transcript - 

Now let's implement the another API for deleting the data.

So for that we will go to our IntelliJ idea.

We'll go to our controller, that is the employee controller and here we will create a delete mapping.

So we had already created the get mapping here for getting the employee, for getting all the employees

and posting the data.

Let's do for deleting.

So here I'll define public and what I want to return.

I want to return string here and delete employee by ID is the method.

Okay.

And here as well.

What I want to do is I want to do delete mapping.

So I'll just define delete mapping here.

And what I want to delete, I want to delete based on the ID provided.

So here as well, I will take the path variable.

That is the ID.

And your I will define a path variable annotation and I will take the string in the ID year.

You can see it's simple thing.

Now I will do return employee service dot delete employee by ID.

Okay.

The method is created.

I will pass the ID year.

And I will create a method in my employee service.

Okay.

Create a method.

Yes.

Now, if I go to my implementation.

I need to implement this method, so let's go ahead and implement the method.

Okay.

Okay.

What I'll do now, I need to take the employee object from this employee list.

And I had to remove that employee.

So that's something that I have to do.

So what I will do, I will create the employee object here.

And equals to I'll loop through employees dot stream dot filter.

Filter for the employee.

I'll mention.

That would be easy.

So get.

Employee id dot equals ignore case with the ID provided.

Once I get this I will do get sorry find first then get.

So you can see that at this point I'll get a particular employee.

So that particular employee I will remove from the list so employees don't remove employee.

So that particular employee has been removed from my list and I will send the data back that.

Employee deleted.

With the.

With the ID.

Past.

Look, you can see that the simple method we created, we took the employee from the list and then we

removed the employee from the list and we send the data back.

That employee deleted with the ID which was passed, and the devtools has reflected the changes.

So we will check this.

So we'll go to the post manager.

We will just change the URL, we will do the post request here and we will save the shim here first.

Okay, the data is saved.

I'll save Shabir.

Dowdy.

Shall be read gmail.com and I will save the data.

The two data are saved if I get the data.

You can see that I'm getting two records here, right?

One is Shabbir and one is Shivam.

So let's delete Shivam from here.

So I will get the idea from your.

And I will pass the ID slash ID as it is a path variable and I will change the mapping to delete OC

because we had mentioned delete mapping.

And once I hit send, you can see that I'm getting the data that is emploi deleted with the id this.

If I now do get request without the ID, I should be only getting one request.

That is one response, one data.

So you can see that I'm getting only one record.

That is the schedule.

So you can see that we are easily able to delete the data using the delete handler as well.







Transcript - 

Now let's talk about content negotiation.

So what is content negotiation?

So content negotiation means.

Currently, you have seen that we are getting all the data in the JSON format, right?

So by default you can see that all the data is in JSON format itself.

But what if I want to get the data in a different format, right?

So for that reason, we need to give all our rest APIs a capability to generate the data in the different

formats.

Now, by default, rest APIs provide us the capability to generate the data in the JSON format and in

the XML format.

So these are the two ways that we are able to generate the data currently by default.

You can see we were sending the data in the JSON format.

How about generating the data in the XML format?

Now there is an option in the spring for each and every API to be defined which type of data needs to

be generated.

So suppose if I go to my IntelliJ idea for any of the APIs, suppose let me go to the employee controller

and here I can define what it produces, right?

So you can see that there is a property available produces.

So here we can define what type of data this particular API will produce.

Either it will produce XML or JSON or both.

But this is something that we need to define at each and every handler.

Right.

Which is not feasible.

Suppose your application contains thousands of handlers, then it is not feasible to do such kind of

things right, because two to come in each and every point and just to define okay, what type of handler

it would be, it doesn't seems a good idea.

So there are different options available with which we can implement such kind of things.

So ideally there are a couple of options.

So one is using the parameters so you can define the parameters, so you can parse the parameters in

your UI.

So with that we can identify which type of data needs to be generated.

The other way is using the HTTP headers you can define in the HTTP headers as well, like what is the

content type and based on the content type, your API should be able to parse those data and based on

that you should be able to generate those requests, generate those data as well.

There are other ways also would be available, but these are the standard and general ways that the

content negotiation happens and based on that, the data is generated.

So what we are going to do here is we are going to take the example of the parameter type of data generation,

how we can render the data based on the parameters.

So what we can do here is for any type of URI, so any URI.

In your application, whatever the UI is there in your application if.

That you or I.

Is adding the parameter type that is the media type.

This is something that I'm defining.

You can define anything.

If Miss Media type is equal to XML, then generate the XML data.

If you are a contains.

Media type.

Equals to JSON.

Then generate JSON data.

If empty.

Right.

If no parameter, then by default create the JSON data.

So this is how you can do something related to the URI with the parameters for content negotiation.

So suppose if a client application is only handling the XML data, right?

So whenever that particular client is calling the data from your API, they can pass this parameter.

That is media type equals to XML and you will be generating the XML data and you will be giving back.

If they want a particular in the JSON format, they can parse the media type equal to JSON and you will

be sending data in the JSON format.

If they don't pass by default.

We will be always editing in the JSON data so you can see that we are giving a way to a client to get

the data in that particular format.

So whatever they want, they can be able to get the data with our application with very simple implementation.

So with 5 to 6 lines of code, we will be able to implement this entire implementation for all the client

application.

Right?

So you can see that how robust our application would be when we are allowing the clients to generate

the data based on their request, based on their need.

Right.

So these are the things that we should be considering when we are creating the rest APIs other than

the functional implementation, what technical implementation you are doing and what are the feasibility

fees you are giving to your client applications as well.

So let's go ahead and implement this content negotiation for our APIs, like what type of data they

need.

So what we will do is we will go to our IntelliJ idea and we have to do some configurations.

So to do some configurations, what we generally do is we create a config package and in that we create

all the configuration files.

So here as well, we are going to create a config package in our application and we will add one configuration

file.

So let's go ahead and create a package that says config.

Och.

And yeah, within this config package let's create a java class that says web config and within the

springboard all the configuration needs to be annotated with the configuration annotation.

And this particular class that is web config is going to implement web MVC configurator.

Now if you go to this particular class web MVC configurator, you can see that there are different ISPs

interface, right?

So there are different default methods available.

We can always go ahead and override the method which we need.

So you can see that there is a method that is called configure content negotiation where you are passing

the content negotiation configure, right?

So this is something that we need because we need to handle this content negotiation.

So what I will do is I will just copy this method and I will go to my web config and I will pass this

method.

Where I'll just remove the default from your.

I'll make this public and I will annotate this with override.

Look, this is a simple thing that I've done here.

Now, let's override the configurations.

So this is the content negotiation configuration we are getting.

We can override this.

So so what I will do is I will use configure and I will do favor parameter.

We want to favor the parameter here because what are the parameter that we are passing With that parameter,

I want to do the content negotiation, so that's why I'm defining favor parameter.

Then I will define parameter name.

What is the parameter name?

So in this parameter name, I will define media type.

This will be my parameter.

Then I'll define what will be my default content type.

So my default content type would be.

Media type dot application underscore JSON.

This is my default OC.

Now I'll define media type so whenever I will pass XML.

My media type would be media type application.

XML and whenever I'll pass media type as.

JSON.

My media type.

Would be application JSON.

That's it.

You can see that with file line of code.

Now we have implemented the feature in our rest APIs to do the content negotiation by the client.

OC.

Now one thing is still remaining by default.

Your spring application doesn't have the dependency to convert your data into XML.

By default you can do JSON, but for XML we need to add the dependency.

So let's go ahead and add the dependency.

But all the configurations are already done.

So we'll go to the XML file and add the dependency for that.

So we will go to the dependency list.

And here we will add the dependency.

And this is Jackson.

Data format XML.

This is the dependency that we need.

Now, if you're not getting the suggestions in your ID, don't worry.

This will only come when you are using the ultimate version of the IntelliJ idea.

But if you're using community version, you won't get.

You can always go ahead and Google it.

You will easily get this information.

So I'm just adding this and the artifact is com dot faster XML dot Jackson dot data format.

That's it.

Now refresh your MAVEN project.

And.

Restart your application, so I'll just close my application.

And I'll start my project.

Okay, So I'll go to the postman again.

I'll go to the post request and I will try to save this data.

So if I hit send, you can see that the data is saved now.

And if I go to my get request.

And if I hit send, I'm getting the data in the JSON format.

But now if I add the parameter that is media type, media type equals to XML, then I should be able

to get the data in the XML.

You can see that.

I'm not getting the data, so something should be wrong.

So let's see that.

So, yeah.

So that was a spelling mistake.

Here you can see that as a spelling mistake.

So if I do media type equals to XML, then I am able to get the data right.

So you can see that I'm able to get the data as a list in the XML format.

But if I parse JSON here, I should be able to get the data in the JSON format.

Right?

Is the JSON if I don't pass anything?

Then I should be able to get the data in the JSON format itself so you can see that how easily we were

able to add the content negotiation in our application.


Transcript - 

Now let's talk about data filtering.

So data filtering is really important when we're working with the rest APIs and when we are creating

the different modules.

So if you take the example here, currently, if we take the example of our employee model, you can

see that there are different fields available, right?

There is the employee ID, first name, last name and all those information.

So what we can do is suppose if you go to the postman and you can see that we are getting all this information

here.

So what if suppose you don't want all this information, you just need this particular information for

your internal processing department as the internal processing, but you don't want to send the data

back when you are processing.

So for that, if you want to filter out some of the data's, what you can do is you can use JSON, ignore

annotations to do those filter things.

So if I go to my IntelliJ idea, and this is something that my employee is right, so for this, what

I can do is for this department, what I can do is I can define JS and ignore once I define this JSON

ignore.

What it will do is for this particular department, the processing will not happen.

So data when we pass and won't be retrieved.

So we are filtering that particular static properties here without moving those properties or without

removing those properties from our classes.

So if I go back to my postman and if I do post request for this, okay, so I'm passing all these requests

and if I hit send, you can see that I'm not getting the department back here.

That means the filtering is happening for that particular properties that I mentioned.

Now if there are multiple properties that you want to do.

You can define all those JSON ignores for all those properties.

Or what you can do is you can define this.

Or let me just comment it out.

So when you are referring the code, you will get this information.

So if you want for multiple properties, you can go to this particular class level and you can define

JSON, ignore properties.

And here you can define all those properties.

So one of the properties was department, right?

So I'm just wanted to.

Department.

So let me just copy this department and I'll mention the same year.

So I am ignoring the properties here.

If you want multiple, you can add comma, separated multiple properties here.

For now, I just want one property.

So I define one property and let's wait for DevTools to check for the changes.

We can see that the changes are reflected.

Now, if I go back to Postman and if I hit send again, you can see that I'm getting the same result

here.

So you can see that how we implemented the data filtering for the static properties in our APIs.




Transcript - 
Now let's talk about EPA versioning.

API versioning is very important when we are developing our APIs.

So when we are developing the APIs, what generally we tend to do is we give those APIs to the different

users or developers to consume those APIs.

Now, to understand more on that, what we will do is we'll take one example of a Twitter.

We already know that Twitter gives the APIs to all the different developers or the users to consume

those APIs, to do all the different operation, to tweet, to get all those tweets and everything.

So suppose they have one API that is the tweet API.

Okay.

They have a tweet API.

This tweet API.

What it will do is it will take some of the properties.

Suppose it will take the ID, It will take the text.

It will take the photo or anything.

This is just the example.

This is not the exact tweet API, but you get the idea.

There is a tweet API from Twitter which allows us to tweet from our and from our client.

Now they have this API for a long time.

Now what?

The thing is, they had to do some changes in this.

So what they did was rather than giving this photo here, rather than giving this photo as a field,

what the thought of giving is they will give the tweet API.

In such a way that they will give the ID, they will give the to it tax returns, and then they will

give the photo ID or the tweet ID and for posting the photo.

There might be another API that they thought we will go for posting a photo where you will give the

URL of the photo the size information.

And all those things.

So once you complete this, you will get the ID in return and that ID you will provide ID here.

So you can see that at the end both of these things are doing the same thing with API, but the implementation

is different.

Earlier you were just passing the direct photo, but now you are passing this photo ID and you are passing

the photo differently.

Now my application consider I have application and I have developed the application which with which

I have used this particular API.

Now, as Twitter decided to change this entire implementation, all of a sudden my application stopped

working, right because they changed the API.

And now this API is not working.

Now to avoid this type of scenarios, we do API versioning, so that means at any given point of time,

if I had to use this particular API and whenever I'm creating this new API, this API is already available.

So that means I will have a version of this API as well.

And version of this API as well available at a given point.

And we can integrate the users that you have one year or six months to implement your our new implementation.

Till then this API is available and after that this API won't be available.

You have to move to the new API.

We will be decommissioning the older APIs now to handle this versioning.

There are different ways as well.

The first foremost and the most simpler way where everyone is following is using the resource itself.

So they will be having the resource UI itself.

Like suppose this was the first version available, right?

So it will always slash V one, slash two it.

This was the older API.

Now, when they decided to do the changes, they would have changed to slash V two.

Slash two it.

You can see that the UI is different now.

So that means they will never contradict with each other.

If you want to move ahead to the newer version, that is second version, you can always go ahead and

do this.

If you want to use the older version, you can always go ahead and use this.

This is one of the methods.

The second method is using the query parameters.

You can parse the query parameters and based on the query parameters, you can decide what is the version.

So you can pass question mark version and you can pass the version it is V .1.1 or V dot two or whatever

it is.

The third way is using the HTTP headers.

So in the headers itself you can parse the version information like which version you are trying to

call, but a standard one where I have seen and I have used mostly is using the path variables.

So hope you get the understanding what is versioning and why we use versioning.

So now let's go ahead and try to implement versioning in our APIs.

So we'll go ahead with this type of versioning that is the path level versioning.

So what we'll do is we will go to our intelligent idea and if we go to our employee controller, you

can see that we have just defined the employees here, right?

So what I will do is I will change this to slash V one simply so you can see that whatever I have implemented

here, that is my first version of API.

So whenever I'll call slash V one slash employees, this particular version will work.

And what I'll do is I'll create a new controller to define the second version of it.

So I'll define a new controller and I'll define employee v two controller.

Okay.

And here I will define this is a rest controller.

And what I will do is I will just.

Copy this as well.

That is the request mapping.

But this would be.

My V2 and this particular V2 controller will also have a same method.

So suppose let me just try to give one post mapping here and what I will do.

I will just go to my v two and I'll define the same thing.

I will not need this currently.

So what I will do, I will just pass the employee back so you can see that now I have two controllers

available for the employee controller and both of these APIs.

You can see that both of these resource handlers have two different APIs.

One is slash V, two slash employees and one is slash V one slash employees.

And you can see that this first one, first API post mapping that is going to.

Employee service saving method, which is trying to save the data in one of the records.

And the other one is just returning the data back.

So if you want to test this, what we will do is we will go to a postman and test this.

So now, rather than employees directly, we need to pass slash V one slash employees.

So you will see that whenever we are using version one, we will get the employee ID back.

But for V two we won't be getting the employee ID because employee ID is set at the service layer on

the V one service.

So if I hit send, this is the V one service.

So we got the employee ID back and the rest of the details are same and whenever we will use the v two.

Slash employees.

You can see that we are not getting the employee ID we are getting now because we have not set those

details.

So you can see that both of the things are working parallelly now and both the V one and V two versions

are there and we can always go ahead and implement the newer versions.

And once that is implemented, we can always pass on that information that these are the new versions

available and whoever wants, they can use those versions.

So this way, ideally the versioning will happen in your APIs and you should always do versioning.

You always have to define your first APIs with version one and accordingly you can change for the new

APIs.






Transcript - 

Now let's understand what is GPU?

GPU stands for Java Persistence API.

Now why this is important and why we use GPA.

So whenever we want to work with a database, what we do is we have to change our data, which is in

the object format to store in our database that is in the tabular format, right?

So for that reason, what we generally use is we use the ORM frameworks.

That is the object relationship mapping.

So that means your objects are mapped to your tables.

That means your entities, right?

Now for this autumn, there are different frameworks available like Hibernate, diabetes and all these

other frameworks are available.

Now they can go ahead and implement their own implementation of this autumn framework now to make sure

that everyone uses the same standards and same format.

JPA comes into the picture, so JPA is nothing but a specification, so it just defines the specification.

Like whatever you are implementing, whatever your own framework would be, it should be according to

this particular APIs and according to this particular implementation.

That's what JPA defines.

Now, based on this JPA, all these different frameworks like Hibernate.

I.

Battis And all those that are multiples available, right?

All those.

Go ahead and use this GPU specification and try to implement their own implementation.

That's generally would be.

So whenever you're using your code, there will be a GPU layer that GPU layer you would be using.

And this GPU layer is just a layer.

And beside this there will be any of the frameworks available.

OC You might be using hibernate or diabetes or anything, but that there will always be a framework

that you are using with your GP.

So you are using your GP clear and GPS player will be responsible to call those JDBC APIs and JDBC drivers

to connect with your database and to store your data to convert your objects into your HTML formats.

Now, by default, with Springboard, whenever we're using the JPA Hibernate Framework is used.

So that's the default implementation whenever we implement the JPA.

Hibernate has been used, but if you want to remove, you can exclude and you can add your own framework,

whichever you want to use.

But hibernate comes in handy because most of the functionalities, most of the APIs are very similar

and you'll be easily able to implement all those functionalities.

Now, until recently this JPA was implemented for all the relational database itself.

OC Relational database.

But with Springboard, you can also implement this using the No SQL database for your know SQL database

as well.

So if you are using the MongoDB, you can always use the Mongo repository that is in terms of internally

using the JPA repository to give you the same functionality.

So if you implemented hybrid with my SQL, you can use the same code, but with the same code if you

just give the implementation differently for or you give the data source as a MongoDB data source,

your entire application will work easily with the same code for your MongoDB as well with MongoDB repository.

So you can see that how easily it would be to use all the methods.

So I will go through the different methods available.

With all those methods you will be easily be able to switch between the databases and switch between

the implementations.

So with this API, it gives a lot of flexibility and a lot of different functionalities out of the box

for us to use.

We don't have to worry about the internal implementation or the internal logic.

You can go ahead and directly use this jpgs and we are good to go.

So that's all about the basics of JPA, but let's go and implement JPA in our project.





Transcript - 

Whenever we are working with GPA, what we need is we need an entity.

So there will be always an entity.

This entity is nothing but your class.

And this class is annotated with the rate entity.

Okay.

And it will have all your properties.

And this property will include one of the properties that will be annotated with accurate ID to make

it as a primary key or a unique key.

So this is this general structure of your entity.

So for each and everything that you have to work with JPA, you have to create the entity class.

That entity is nothing, nothing but a bajo annotated with other entity, and it will have one property

that is the ID now for all these entities to directly get the advantages of the JPA, we need to create

one repository.

Now this repositories will be extending different by default repositories available.

So we'll be using a JPA repository.

This JPA repository takes two things as a gender X value, which is what is your entity for which entity

this particular repository is created and what is your ID type?

Okay, so once you implement this repository with extending JPA repository or the different flavors

of the GPA repository with the type that you define, you will get a lot of functionalities by default.

We will see all those functionalities, but you will get all the, all those functionalities by default

and you can implement your code very easily.

So let's go ahead and do this.

What we currently have is the employee with which we are working.

So we'll create the entity for employee and we will create the repository for the employee as well.

So we will go to the IntelliJ idea and what we will do is we'll create a new package.

That says entity.

Now you can see that you might be confusing, like we have a model as well and where we have created

the Pogos.

But what if now we are creating for the entities?

So what we are going to do here is we are going to use a DTO pattern per se.

We can say that where what we generally do is we create the two different objects for the two different

layers.

So for database layer, we create the entities and whenever we are working with the database, we will

create the entities and with that we will try to store the data, but with the controller layer, with

the view layers, like whenever we want to pass the data back at the time, we will work with the models.

Generally model has been also called as the view objects or the value objects.

So this type of terms will be used frequently whenever you are working.

But the idea is simple.

It is just to have the two different objects to work with the two different layers, one with a view

layer and one with the database layer.

So for that reason we are going to create two separate things over here, and these will be used in

our database activities and the models will be used in our view layer whenever we are passing the data

back.

So the entity package is created.

So with this entity package, we will create the class that is the employee entity.

You can give any name.

I'm just giving employee entity.

Okay.

Now, as we discussed earlier, this employee entity needs to be annotated with entity.

Now you can see that we need to annotate with the entity, but nothing is available here.

Right?

So for that, what we have to do is we have to implement or we have to add the dependencies of the JPA,

right?

So we will go ahead and add the dependency.

So ideally what you can do is if you don't know what dependencies are, I will show you one secret.

What I generally do is rather than going to the MAVEN repository and search for all the different dependencies,

I just go to spring initialize, I'll go to the dependencies and I'll search for the dependency.

JPA And this is something that I will copy, so I will go ahead and copy this.

JPA So you can see that it's very easy.

Okay, so I'll just copy this.

I'll add to my IntelliJ idea.

I'll go to the XML file.

And this is the GPA repository editor.

Now, whenever we are working with the GPA, that means we want to work with our database.

So we need to have our database dependencies as well.

So for now, what I will do is I will use the edge to database.

Edge to is the in-memory database so we do not have to install a database separately or database server.

It will be used in in memory and all the data will be stored in in memory.

Or we can also store the data in a particular file.

So for that reason, we need to have the driver for that.

Right?

So what we will do is we will.

Go ahead and add the dependency that is the h two db h to database.

So this dependency is also added.

So let's copy this dependency from your.

You can see that this is the H2 dependency which will be called at runtime.

Let's copy this and add in or IntelliJ idea.

Look, you can see that we have copied this.

Now, once we add the database driver and our GPA, we need to add the configuration.

So once we add that configuration springboards, auto configuration will kick in and it will give all

the functionalities to us.

So add the properties.

We have to add the properties in our application that properties.

So there are a few properties here.

So what I will do, I will just copy those properties and I will explain you.

So you can see that these are the properties.

So what I'm doing is I am by default enabling the edge to console here.

So that means with this property spring console that enabled equals to two, I will be able to get the

API in the browser.

So that means I can go to the edge to console and I can query the data, I can get all the information

which I need.

Then the next is spring data source URL.

That means your URL of your database.

So what I'm doing is I'm just passing the H two file.

So all my data, which I'm trying to store, it will store the data in my slash data folder and in my

slash employee underscore DB file so it will not store in the in memory.

It will store in a particular file.

You can always go ahead and store in the in memory as well the same configuration you will get when

you Google it.

It's easy.

So I just added this configuration to add the data in my employee DB file.

The next is driver class name, so I'm using H two, so I provided the H two driver or H two driver

and then I give the username and password.

So I gave the essay as a username and password as the password you can give anything.

And then I gave the database platform, which is my database platform.

Database platform is edge to dialect.

So you need to give the dialect for whatever the database that you use.

So I've given this dialect over here, don't worry about all these properties and don't worry from where

you will get it.

You just search on the Google and you will get all these properties by default.

All these properties are also available in the documentation.

So you if you go to the documentation, you will get all these properties.

But if you just Google it, you will easily get it.

So don't worry about it.

And the next thing I have added is hibernate, ddl auto equals to update.

So that means whenever I do any changes in my entities, the same thing will be reflected in my database.

So I have created this employee entity here, right?

So if I go this employer entity, so this employer entity is created.

So whenever I do any changes or anything, the same will be reflected in my database.

So I have not created an employee table in my database.

So with this, the employee table will be automatically created in my database and it will be everything

available.

So for that reason, I have added all these properties.

Now, once you add all this property, now once you add all these properties here, you're good to go.

Now you can implement.

So if I come here to my employee entity, I can use the edit entity annotation.

You can see that other entity annotation is available from the JPA that is the Java Persistence API.

Use this and you're good to go.

Now.

I need the same properties as using the employee, so I'll just come here.

Copy this.

Properties.

Let me just copy the properties as well.

That is last name, email and department.

Look.

I added all these properties here.

Now what I have to do is for my entity, I should always have one unique key, so my employee IDs should

be my unit.

So what I will do, I will annotate with a direct ID here so you can see that the error is gone now.

So now my everything is done.

Now what I have to do is I can use one more annotation to make sure that whatever the table name I need,

I can define.

So if I define table and within this table I can define name equals to table name.

So I'm just defining the table name should be table underscore employee.

If you don't provide this by default, it will create the table based on the class that you are provided.

So now if I define this, it will create a table with table underscore employee only.

So that's it.

Your entity is created now with this entity.

What we need is we need our getters insiders as well.

So we'll create the status.

Okay, all the gutter setters are created.

Now, as we discussed earlier, for each and every entity, we should have a repository as well.

So to create the repositories, what we will do is we'll create a package first.

And in that package we will create the repositories.

So we'll use the repository package here.

And within this repository package we will create a new Java class.

And this Java class should be employee repository and this should be the interface.

So we are going to create the interface for the repository and there is a spell and there is a spelling

mistake.

So let me just refactor it.

So it should be.

Repository.

Now this repository will extends.

GB repository.

There are multiple repositories available, but we will be using JPA repository and for this JPA repository

we need to pass the gender X.

First argument would be the entity that you want.

So my entity is employee entity and the next argument is your ID, the type of the ID.

So my type of the ID is string.

So once you define this, you can see that everything is done.

Now this class will by default not be available in the spring radar.

So we need to tell spring that this is the repository that I've created.

So I need to annotate with the repository.

Annotation.

Okay.

So once I defined this, the repository that the stereotype annotation, this will be available in the

spring radar and the particularly whenever the application starts, it will be added in the application

context.

Now, as I told earlier as well, whenever we're using the employee repository that is extending JPA

repository, it will go by default functionalities.

So whenever you open this you will get all this information.

You can see that these are all the different methods available.

That is find all, find all by ID, save all flush, save and flush.

You can see that these are all the methods available, right?

And all these methods are by default implemented so you don't have to give your own implementation.

You just have to define this and all the implementation is available.

So if you want, suppose for the find.

All right.

So you can see that there are implementations available, find all so you can see the implementation

is already there and you will get the data.

So you don't have to buy yourself out of the implementation if you want.

You can always go ahead and add the implementation you can use the entity manager and with that entity

manager you can do all those things.

But we are not going to complex anything.

We are just going to do the JPA repository because all the standard specification provided by JPA has

been implemented and will get the best performance out of it.

So you can see my employee repository is created, my entity is created.

And now we can go ahead and use these things.








Transcript - 

Now let's go to our employee V2 controller itself, because in the employee controller with the version

one we had already implemented over all the functionalities where we are storing all our data in our

in memory, that is the list that we have created.

But with the employee controller, the newer version, we are trying to save all of our data in the

database itself.

So you can see now how the different versions are benefiting here, right?

So what we will do is we are going to store the data with this save method here, but we already have

the service layer.

We already have a business layer itself, right?

So if you go to the employee service, this is your interface where you have all the different methods

and you have one implementation of it, which is the implementation where you are storing all the data

in the in memory.

That is the end list.

Now what you want to do is you want to have your other implementation of the business layer where you

want to store the data into your database, right?

So what we can do is we can create a new class that implements the same employee service.

So with that we can have the two different service layers and we can use these two different service

layer at the two different places.

So what I will do is I will create a new service layer here.

That is Java class, that is employee V two service in this class.

There is a spelling mistake.

I will just refactor it.

E.

M.

P.

l0y.

And this class will implement employee service.

Once you do that, you need to implement all the methods.

So I will go ahead and implement all these methods.

Now, this particular class also I need to annotate with other service annotation because this is my

service layer.

Now, once you have added this, that means you have now two implementation of your employee service.

Now, at this time your application will freak out.

So if you go to your employee controller now you can see that the employee service that we have acquired.

Now it freaks out, right?

It has the error.

Like, okay, now I found two implementations which one to use.

So to handle this type of scenario where you have multiple implementation, you can use the qualifier

annotation to qualify which particular class you have to use.

So if I go here, you can see that could not auto wire.

There is more than one bean of employee service.

There are two beans.

The beans is employee service MPL also and employee V two service MPL also.

So by default, the name of the bean is created in the spring by the name itself.

So suppose employee service employee is there, right?

So it will be the same name, but the first letter would be in this small.

For the second the letter would be in the small.

So same way the bean name would be created.

So whenever you do qualifier, you can give that name accordingly.

So what I will do, I will add the qualifier year and year.

I can add employee service.

MPL, that is the first one.

Okay, So you can see that for my V one, I'm using my business layer with the employee service.

But if I go to the employee V two controller, I need to use the same thing.

Right.

So what I'll do, I will copy this.

And I go to my employee to controller, I need to use the same business layer.

Now you're also it freaks out, so I need to add the qualifier annotation.

That is accurate qualifier.

And here I will use the employee V to serve service MPL OC.

So you can see that now I have two controllers, two different controllers.

Both the controllers are using two different business layers.

One is using the layer that is employee service, MPL and employee V two controller is using the employee

to resign.

You can see that both are separated out and with one interface we are able to have two implementation

with the two different versions of the APIs.

Right now I can directly call employee service dot save method and I can pass the employee year.

Okay.

But if I go now to my V to service layer.

This does nothing, right?

Currently.

Right.

So what I have to do is I have to use the repository that I've created, right?

The repository that I've created to get all the functionalities and to store the data in our database.

So in my employee v two service MPL, I need to have the object for it, so I'll just create the object

for it.

And I'll just define employee repository and this is the employee repository object of it.

And same year we will do the auto wire auto injecting gear.

Once we do this, it's really simple.

So what we'll do is we'll just use employee repository dot and you can see that you will get all those

methods by default.

You don't have to do any implementation of it.

So I will just call the save method here and I'll pass the employee.

Now you can see that if I do this, I'm still getting the error right, because you can see that inferred

type as is not off parameter as that means the employee that you are passing here and the employee repository

that you have created, that is of the type employee entity, not the employee.

Right.

So you need to convert it.

So I will show you how easily we can do the conversion.

So first thing what we will do is we'll go to the employee service and the first thing is this condition.

I need this condition here as well.

So I will add this condition here.

This condition is nothing, but if it's null, we will create the UUID for it.

Same thing.

Whatever we have defined here.

Whatever we have defined there, we have defined here.

Now what I want is I have this entire object created, right?

Employee with all the fields.

The same field is available in the employee entity as well.

So what I'll do, I'll create the employee entity object.

And it equals to new entity.

Okay.

And what I'll do, I will use the be neutral property being utilized not.

Copy properties here.

I need to pass this source.

That is the employee and the target that is the employee entity.

So all the values from employee will be passed to the entity.

And I can.

See the data in the entity.

Look.

Once this is done save, I can return my employee back.

Because now employee and entity have the same data.

So you can see that how easily we were able to save the data.

It is only one line of code.

This is just the other thing that we have had to handle the functionality or handle the technicality

of our application.

Now this is implemented.

Now let's go ahead and start our application and test this.

So what I will do is I will.

Stop our application.

And start my application.

Look, your application is started now.

Now what you have to do is you have to go to the browser and you go to the local host.

80, 81 and you have to go to church to console.

Okay.

And here you can see that your edge to console this is enabled where we have added the property.

Right.

In the application that properties we had added this property that is enabled as to console.

So with this we are able to see this here we have to give driver name, JDBC URL, whatever you had

given as a username and password is password once you give this.

You can connect to it.

So you can see that once you're connected, you'll be able to see everything.

So you can see that you have the tables created.

Don't worry, there is no two table.

It's only one table.

This is the table that I was working earlier, so that's why the table is there.

So if I go here, the entity name which I have provided in my employer entity is the TBL employee.

That means this one.

So if I do this, if I select one, there is no records available in my database.

Now, if I go to my post man, and if I do post request on v two employee with the Shabbir.

You can see that the data is created and I'm getting the data back as well.

And if I go to the browser and if I hit run again, you can see that the data is stored in the database

as well.

Okay.

Now you might be seeing the department is coming null.

Now this null is coming because we had added the JSON ignore on the department.

So that's why that is that property has been ignored.

So don't worry, we can remove that.

But you can see that this is how you are able to create the table as well.

You have not created the table.

It was by default created for you when the application started and you are able to save the data using

the JP repository.

Now let's implement the method to get all the data.







Transcript - 

Now, what I will do is I will go to the employee controller here and there is a gate method available.

Right.

So I will copy this gate method and I will copy this.

All the three methods get mapping, get mapping with a particular ID and a delete mapping.

I will copy all this method.

I will go to the employee v2 controller, I'll paste all these methods and you can see that I'm not

getting any any error because I already have the implementation of the service layer for the different

qualifier.

Right.

That is employee V service.

So that means my controller rail is almost ready now.

Everything is done.

The only thing remaining is I need to implement my business layer.

So what I will do is I will go to my business layer that is employee V to service MPL, and here I need

to do the implementation for my list of employees and a particular employee.

So let's implement first for the list of employees.

Now, with the help of the employee repository, what will be getting is will be getting the list of

employee entity.

So once we get the list of employee entity, we have to convert the list of employee entity to the list

of employee and then we need to send back.

Okay.

So what I need is I will be getting the list of employee entity here.

That is the employee entity list equals to I can use the employee repository dot find all method.

You can see that the final method will return the list of employee entity.

So once you do this one line of code, you will get the all the data from the database directly easily.

Now, from here I need to convert to my employee list.

So what I will do, I will define the list of employee here employees equals to I will stream through

the employee list here.

I'll stream through it and I will map it.

So what I will do, I will map to the particular employee entity here.

Okay.

And here I will define object of employee.

Equals to new employee.

Once I've done that, I will use bing utils.

Not copy properties to copy from the employee entity to the employee.

Once you do this, you will do return.

Employee.

Now, there are multiple ways to do this, but I'm using this way.

You can use any way to convert your one of the type to the another type.

You can use the builder pattern or you can use the constructor arguments, anything.

There is no right or wrong way.

Once you do this, you need to collect everything and you will use collectors to list, to convert each

and everything your entire stream into my list.

Once you do that, you can directly return the employees.

Your goal, right?

You can see that your entire method is implemented.

Now what we will do is we will implement the get by ID method as well and we will check.

Now what we will do is we need to get the employee by ID, right?

So here as well, we will be returning the employee entity, right.

So employee entity will get from the employee repository and there is a method defined by ID.

You can see that there is a method defined by ID here which will take the ID and this will return the

optional object of Java.

So that means we need to use get to get the object.

And you can always implement that.

If the object is not available, you can throw the employee not found exception accordingly, whatever

you want.

But simply I'm showing you here that with the GPA, with this one method, you will be easily able to

get the data.

Once you get the data, you create the employee object.

And.

You will use bing utils.

Dot copy properties to copy from the employee entity to the employee.

And once that is done.

You will return the employee.

Okay.

So you can see that both the methods are implemented.

Now, let's restart our application and let's test this out.

So what I will do, I will go to my postman again.

And if I go to the get method with the V two slash employees, I should be able to get the data.

You can see that this data is available in the database itself.

So if I go to my browser, you can see that this data is available.

Okay.

And if I.

Try to get a particular record with the ID.

I should be able to get that as well.

You can see that I'm able to get a particular record as well and if I pass any other thing.

You can see that I'm getting the internal server error with the controller, advise the exceptional

handler that we have created.

So everything works fine, Everything is smooth now.

So this is how you will be implementing your GET APIs.







Transcript - 
Now let's go ahead and implement our delete APIs.

So if you go to the IntelliJ idea, this is your DELETE employee by ID method.

So what I will do is simply I will go to the employee service MPL.

This is the.

Delete employee by ID created.

Right.

Let me just copy this because this is something that I will.

Return.

Okay.

And to delete the data.

That is a simple method.

I will just use employee repository dot delete by ID and yet I will pass the ID.

That's it.

You can see that how easy it is to do it right and will just return the employee deleted with the ID

here.

That's it.

Let the devtools reflect the changes.

Yeah, the changes are reflected.

Now let's go to the postman and check this.

I'm getting the.

I'm not getting any records here.

So what I will do is I will change it.

I am getting the records here, right?

So if I go to the to console, I have one record if I click on Turn.

Let me give the password.

Okay.

The password is wrong.

Sorry.

Okay.

So.

If I click on this and I should be able to get one record right.

So now if I go back and if I use the delete method on the same URL with the version two and if I hit

send, I'm getting the record that is employee deleted with the ID this.

And if I go to the H to console and if I run again, you can see that the data is deleted from the table

itself, right?

So you can see that we were able to easily do the all operations in our database itself using all of

our APIs.

And you can see that we have learned a lot of different things as well throughout this journey.




Transcript - 
Now let's implement the my SQL database in our application.

Currently, you can see that we have implemented the actual database that is in memory, but now let's

implement the my SQL.

Now, whatever the operating system that you are using, you can always go ahead to search with the

My SQL Workbench and server and you can download the MySQL workbench and install the My SQL server in

your system is real easy process.

You just have to download it and install it and it will ask you for your credentials, like first time

credentials like what credentials you want to use to connect to your database.

Just give that information and remember those information to connect to your database.

Once you install this, it's very easy to install.

If you are facing any difficulties in installing the MySQL database, then you can always ask in the

discussions.

So once once you install this, you can always open the MySQL workbench, just open the my SQL workbench

here.

And what we have to do is to connect to this MySQL workbench from our application is you go to your

application and you have to use the my SQL driver.

Now earlier we had used the edge to driver, right?

If you go to the XML file, you can see that we have added the edge to database for edge True Driver.

Now we have to use the my SQL.

So how to get the my SQL, you can go to the MAVEN repository and search for it, but for me the easiest

way is to use the spring initialized.

So what I'll do, I'll go to the spring initialized.

And I will search for the.

My equal.

Now be sure that you will not get all ways, all the dependencies here.

These are all the basic dependencies that you will get here.

But if you have the custom requirements, you have to go to the documentation or you have to go to the

MAVEN repository for searching all those dependencies.

So you can see that I need my SQL driver, so I will search for my SQL.

This is a driver.

I've added it and if I go to explore.

And this is the my SQL connector.

I will copy this.

And I will go to the IntelliJ idea and I will add the dependency here once the dependency is added.

Refresh your MAVEN project and then you have to go to the application properties file and you have to

add the configuration for my SQL earlier, you can see that you have added all the configuration to

connect to EDGE two, but now you have to give the configuration to connect to my SQL.

So ideally what I will do is I will comment this out so you can refer all this later.

Okay, So these are the things that you will be needing, so I'll just comment it out and I will add

the property for my SQL.

Okay.

So you can see that these are the same properties.

Okay.

Data source URL, driver, name, username, password and platform.

Okay, so you just define the Hibernate dialect.

What is the dialect that is the my SQL fi in DB dialect and the username password is the root tool that

I have defined and the data source URL is JDBC colon, my SQL column localhost 3306.

This is the URL where your application is running.

This is the port.

My database server is running.

Sorry, and this is the database that I'm using.

So this is the schema name.

So this schema I need to create, but the tables and everything will be automatically created.

So if I go to my, my SQL and this is the schema that I have to create, so what I will do, I will

right click create schema.

I'll give the name apply.

Okay.

So my schema is created and currently you can see that there are no tables available.

Okay.

So now if I go back to my application, I have defined the fields here.

You can see that the only thing I did was I added the my SQL driver and I added the properties to connect

my application to the my SQL rather than the edge to rest.

Everything is same.

Now.

Just simple.

Start your application and everything should work.

And now your application is started.

You can see that now you have connected to my SQL.

Now, if I go to Postman and if I use post request with the same thing.

Okay, before that, if I go to my in my SQL workbench and if I refresh.

You can see that the table is created OC.

TBL employee table is created and currently there is no data available.

Now if I go to Postman and hit on send here, the data is created.

If I go back to refresh here and you can see that the data is also inserted here.

So you can see that how easily it was to change the database as well using the JPA.

Now if I hit on delete.

I should be able to delete as well if I'll change to delete mapping.

Hit on send.

The employee is deleted if I go back and if I refresh the data is gone.

So you can see that everything works fine.

We were able to connect over to database as well and we were able to connect to my my SQL database as

well.

So both the databases are working fine and these are all the things that you need to know in Spring

and Spring data.

JPA To start working with the microservices, this is more than enough to start working with the microservices.

We will focus more on the microservices and that we will be using all these functionalities multiple

times.

So you will get all the understanding in detail.

Now if you still feel any difficulties in any of the things you can always ask in the discussion.

I have a full 10 hours on the springboard, so if you want you can check out on our channel as well.

So that's all about the Springboard and JPA.

Now let's go towards the microservices.




Transcript - 
So now, as we learn about the spring booth and spring GPA, let's see what is microservices now before

directly jumping towards the microservices and see what is the fuss about the microservices.

Let's first understand the history.

Now.

Earlier, whenever we were building the application, what we used to build the application is using

the monolithic architecture.

What do you mean by that?

So what we used to do is till now we used to create the application.

Suppose let's take the example of the e commerce application.

So this is the e commerce application.

And this is the entire application itself.

My drawing is really bad, so please don't mind.

This is the application and this application would be consisting all the different modules.

Suppose there is a shopping cart module.

OC shopping cart is there then orders is there.

Then payment is there.

Then product catalog is there.

OC users are, their shipments are there.

All these different modules are there in your particular project.

And all these modules are there within the one architecture means one codebase itself.

Forget.

That means you're all this particular modules are built using one tech stack.

Okay, suppose we are.

Suppose this application is built using the Java text stack.

So everything is within the Java Tex stack and you deploy everything using a one bundle.

OC.

Everything is deployed using one model.

So suppose this entire box is one model.

So this is something you will be deploying.

So this is something that we used to be doing till now.

That means if you are just doing the change in your shopping cart services, if you did any changes

your team, then also your entire application is going to be deployed.

If there is any change in your order services, then also your entire application is going to be deployed

because your entire application is one code base and all the different modules are there within one

single code base itself.

One application means one application contact itself.

So this was something that we used to be doing till now.

Now doing this, there are disadvantages to it.

Suppose we take the same example of a shopping cart like we did some changes in just the shopping cart

itself.

We didn't do any changes in any of the other modules or any other or any of the other services, but

still we have to deploy this entirely.

We have to deploy order product and everything all together, bundled and deployed.

So that means rather than just deploying this one particular service, we are deploying each and everything.

So that means there is a headroom as well added to it.

If I would have been deploying this shopping cart, it would have taken only 5 minutes or 10 minutes.

But now I'm deploying each and everything, so it's adding a time to it.

It is also adding the complexity layer for the development purposes.

Whenever there is any changes in shopping cart and developers are doing changes in this shopping cart.

So those developers has to interact with each and every other developers who are doing some changes

in the other models as well.

Like these are the changes that we are doing and we are going to deploy this particular thing at this

and this particular time of period.

So all this collaboration has to be done when you are building a monolithic applications and if it comes

to scaling as well.

So at the time of peak sales or anything, your shopping cart is something that is used a lot, your

orders and your payments and everything.

It's been used a lot.

So what happens is to scale the entire application, you have to scale your entire application itself.

So there is a lot of headroom in terms of cost as well, in terms of inefficiency as well.

Your your application is not efficient enough for your cost savings and your performance as well, because

you are deploying your application entirely where some of the modules are not needed that much.

Still, those are scaled up, so there is too much cost involved in it.

So this is the scenario that you see and you have developed using the moral application now to solve

this type of scenarios and to solve this type of situations, microservices comes into which what is

microservices?

Microservices are nothing but micro applications.

It is nothing but micro applications divided into small parts.

So what we can see is all this particular applications are there, right?

All these different services, those are divided into separate small applications.

So within your e commerce platform.

There is a shopping cart is one application.

Borders is one application.

Payment is one application.

Product catalog is one application.

Look, you can see all these different applications and all these are connected by any of the.

Protocols.

Now, as all the services are separate applications, separate standalone applications, we can do whatever

we want.

So the time of pixels, if I just want to scale up my shopping cart and orders, I can just scale that.

I can have multiple instance of shopping cart and multiple instances of orders at the time.

Product catalog or payments can remain the same.

So you can see that we got the advantage.

Like whatever we want, we can scale accordingly.

We don't have to scale each and everything, whatever is required that only we can scale.

The other thing is all this particular applications are not dependent.

Now.

Those are not tightly coupled, which means that the deployment process or deployment timelines for

each and every applications can also be different.

In the earlier there was one application deployed all together, so all the applications were deployed

at a same time.

So if some of the application was not having any changes, but still those were deployed.

But now suppose there are a few changes in the shopping cart, so that shopping cart is only deployed.

So all this particular services can be with a different version.

Suppose for the shopping cart there are 60 deployments done for these orders.

There are only 40 deployments done for this product catalog.

Only 20 are done.

So you can see all those particular services are with different versions, with a different timeline,

with a different development team altogether.

So you can see there is no direct connection between all the services.

Everything can be independently work along.

Now alongside all these advantages, like you can scale those services independently, you can work

on those independently.

There are disadvantages as well.

So what you done is your entire model.

The application has been structured into the different microservices now to maintain all those microservices

and to work with all those microservices and to debug any issues or to trace any of the issues that

you find.

It is very difficult because all those are different microservices now and all those different microservices

call each other using any of the other protocols and to trace each and everything is really difficult.

So there are hundreds and thousands of microservices involved in your project and to trace each and

everything like what happened at what particular point is difficult.

There are a lot of tools available to handle this, but it is the thing that we have to implement in

our applications.

In Monolith that was not needed.

Everything is a single flow and everything is maintained at one place, yet everything is different.

Everything has to flow via one of the things all the, all the things are distributed.

So we have to handle those scenarios as well.

And as in the microservices, all the different microservices will be calling the other or each other

microservices.

So to maintain which particular microservices fail or to handle all those error scenarios, we have

to do a lot of monitoring tools.

So there are a lot of monitoring tools involved and we have to make sure that all of our microservices

are monitored correctly and whatever the issues are there, we should be able to figure out it as quickly

as possible.

So this is the extra thing that we have to implement in our microservices architecture.

So that's the extra hiccup that a developer has to face to implement all those things.

Now how you will be maintaining all this microservices when you are working with the monolithic architecture,

you all the things are within one place and you will be having one repository for this, right?

All the things are there in one repository.

Everyone is working in this repository and for this repository there will be a second pipeline that

is the continuous integration and continuous development pipeline where everything has been deployed

all together.

It's very simple though.

It's time consuming, but it's simple.

But what about in the microservices?

So microservices can be maintained using two types of repositories.

That is the mono repo.

And political.

What do you mean by Monotype?

Monotype means all these different services are there, right?

Service one.

Service to service three.

All the services are different altogether, but maintain in a single repository, everything is maintained

in the single repository.

That's the one thing.

And the other thing is what that means for each and every different services.

That is a different repository altogether.

So suppose your order service is one of the applications.

So for all other service that is the different repository for that, for shipping, that is a different

repository for that.

And if this is payments for payment, there is a different repository for that.

All the different services have the different repositories in the repo and in the mono repo, though

all those are different services, but those are maintained in the single repository all together.

Now both of these have its own advantages and disadvantages.

Now, whenever you're going with the monarch repo, what it does is how you would be doing is you will

be maintaining everything in the folder structure.

So for shipment there will be one folder in the repository and all the shipment related code will be

there in the shipment folder for orders, there will be order orders folder created and in this order

folder your order service will be there.

All the code for that, if it is for payments, payment folder would be there and the service for payment

would be there in this payments folder.

And with this type of thing, the advantages is you just have to clone one repository, only one repository

which is containing all this particular services and you can work easily on it.

That's the advantage you will get.

And what issues you will be getting is to handle your pipeline.

So what you have to do is you have to create this pipeline for all this particular services.

Within one report, there are tools which doesn't allow multiple pipelines for a single repository.

You have to go ahead and do the work around to skip everything.

Suppose if there is no changes in the shipment repository, you have to skip this particular step.

If there is no change in the orders, you have to escape.

If there is only change in the payment, you have to do the or you have to run the pipeline for payment

only.

This is all can be done, but you have to write the scripts for it to do all those changes.

So you can see there is an extra complexity added in terms of the pipeline when you are implementing

it for the mono repo, but for the poly repo for each and every different repositories, you can easily

create this pipeline.

Och and all the things can be worked separately for this ICD.

It will work differently for this.

It will work differently for this also.

It will work differently.

All those repositories can work independently, can be deployed independently.

The only disadvantages would be this that you have to clone all this repository separately and you have

to maintain your code.

It will be difficult when you are working with multiple repositories, so this will be the issue that

we will face when we are working with the poly repository.

So this is all the general idea about the microservices.

What is microservices advantages of it, how differently we can handle the microservices, like how

we can create the report and everything.

So because of all this, all the tech giants and all the companies are switching towards the microservices

architecture and to start building the microservices architecture.

So at each and every point they can scale and scale down according to the user requirement, according

to the functional requirement.

And one more advantages also we get when we are using microservices, I forgot to tell you earlier,

is that this type of microservice architecture, we can create the polyglot architecture as well.

That means that in the earlier times in the monolith, you can see that we can only use the one tech

stack, right?

But within the microservices we can use multiple tech stack.

Suppose your order service.

Is created in Java.

Your payment service.

Is creating it in Golang.

Your shipment service.

Is created in Python.

All this is possible when we are working with the microservices because all these are separate entities,

all these are separate services.

They connect each other using any of the protocols.

So using any of the protocols, this all connect so we can create all these microservices in different

language based on the requirements.

So this we will get the advantage to create the polyglot services and the microservices architecture.

So there are multiple ways as well to connect the microservices.

One of the most used ways and most popular is using the HTTP protocol.

OC actually protocol is used and for this rest APIs are created.

We already saw how to create the rest API.

So the rest API is created and all the APIs are exposed.

And with those APIs, each and every microservices will be able to call the other microservices.

And the other way is using the messaging system.

Using the brokers.

So there are a lot of messaging systems are available, so you can use any of the messaging systems

available to make the asynchronous connection.

This will be synchronous, right?

And this will be a synchronous connection.

So whatever the requirement would be, we can go ahead with those requirements and we can build our

system.

We can create the messaging system.

And one would be producing the messages and one service will be consuming the messages.

And based on that, it will do some operations and it will pass on the data.

That way.

Also, we can develop our microservices.

So you can see that we have a lot of flexibilities in terms of text tech that we can use in the microservices

architecture.

We can do whatever we want, we can create or we can use any of the languages, any of the tools, whatever

we want using the microservice architecture.

But in the monolith that is not possible.

The only disadvantage in the microservices is to handle all the things to monitor each and everything.

So that's a really difficult part, but there are tools available for that.

So we would be able to implement those tools and we can monitor those services as well.

So that was all about the microservices architecture and the advantages and disadvantages of it and

the different ways we can communicate and we can handle all the services from the next video sessions.

We will start building our microservices.





Transcript - 

So now, as we have understood what is microservices and what are the advantages and disadvantages of

microservices and how we can create the repos and everything.

So now let's understand what we are going to build in this particular course.

So this is the architecture over here and this is the entire architecture that we are going to build

in this course.

So you can see there are a lot of components involved and these all components work together to create

the entire microservices architecture.

So we will see all the different components here.

Let's focus on all the individual services over here first so you can see there is an order service,

user service, product service, payment, service, config server and all those things.

So first class understand all these things.

So what we are going to build today is we are going to build a shopping cart application where you can

order any particular product based on the pricing information, whatever is there, and you can do the

payments for it.

Now that particular payment is going to be a third party.

We'll just assume that there is a third party payment integration.

But from our end what we are going to do is we are going to just mimic that particular thing, that

there is a payment service and we are doing a payment for that.

So we are going to mimic it.

We are not going to implement the actual payment interface, but we're going to mimic.

So let's understand this.

So we are going to create one product service and that product service will be having its own database.

That is the product database.

Now you can see that for each and every services we have one database, so we are going with the per

service database design pattern here.

So this is one of the design pattern.

There are multiple design patterns available, but you can see this is one of the design pattern that

we are following.

So for the product service, we have a product.

DB And what we are going to do with the product service is we are going to create one of the API with

which we are going to add the products to our system.

So that means if I'm selling iPhone, then I can add that I'm selling iPhone.

I have this much quantity and this is the price for it.

So user will be able to purchase that.

If I'm selling MacBook, I'm selling PC or anything, those type of information, those type of product

details I'll be able to add from this product services.

That's the one thing that we will be doing from the product service.

The next thing is the order service.

From the order service, I will be able to place the order.

That means from the order service there will be one API exposed that is the place order API and that

place order API will take the particular inputs that those inputs will be like.

I'm taking the user ID for which user this particular order is what is the product ID like for which

particular product I'm doing the order, What is the price of that order and how many quantity I'm doing

the order and what is the status of the order?

This all the general information that we are going to pass and based on that, the place order API will

do some operation and it will save the data in the order database alongside.

What it will do is alongside it will call the product service API.

Also from this particular microservice, it will call the another API that is the product service to

reduce the quantity or to place the stock aside.

So suppose if I have two iPhones available to sell from my product services and I'm ordering one iPhone

from my order service, then that particular one stock should be kept aside.

That particular quantity should be reduced.

And then for the new user it should only show that one quantity is left.

And if I'm ordering more quantity than I have already in my stocks, then I should tell the user that

we do not have that much quantity to order.

So for that you can see that we are calling the product services and the data will be called from the

product database.

Once that is done, once the order is created, once the product service is reducing the quantity,

we will call the payment service.

Payment service will have the API to make the payment for the particular order that we have specified.

And for that order ID, the payment will be completed, the payment will be done and that payment will

be stored in the payment database.

Like what are the details like this is the payment, what type of payment it is, what is the amount

and against which order I did.

This payment was completed.

Those information we will store in the payment database and once payment database or once the payment

service is completed to make the payment, the result will be sent back to the order service.

And then the order service will mark.

The order is completed and it will send the response back to the user that your order is completed.

So you can see that from one order service that is using all the databases based on the different services

it is calling.

So from one order service, it is calling the multiple microservices, and that's how generally your

application will be working in the real time, like you will be calling multiple services to do some

kind of operation.

And we are going to handle all those things in detail over here.

So you can see we covered all these things and there is a user service as well.

We are not going to concentrate more about the service.

We are not going to build user service now.

But think about that.

There is a user service which will have all the information about the user, and that user information

is stored somewhere in one of the database or any security provider like in this particular case.

In this example, we are using the Okta security provider.

So if you are not aware about the octet, don't worry.

We will go through each and everything in detail in this video.

So we are using the OCTA service provider and we are assuming that OCTA is responsible.

To give me my user details and to authenticate my user.

So these are the services and all the services you can see all the services are inside one service registry.

So we are going to create a service discovery pattern here.

So where will be creating one service registry?

And that service registry will be continuously reading for the different services available.

So there are multiple services available, right click order service, product service, payment, service,

config servers.

All this particular services will attach to the service discovery that we are going to create.

We will see more about what is service discovery and how it works.

But this is a general idea where service discovery will try to connect all the services it has and it

will try to help to communicate with all the services, and from that we'll be able to get the information

like what are the services we have and what are the services are up and running and all those information

we will get from the service discovery.

So this is something that we are going to create now.

You can see that we have also created a config server now as we are creating multiple services here,

right?

So there might be chances where some of the configuration that we add in all our microservices, right?

Those configuration may be repeating.

Right?

So all those repeating configuration we can add in our config server and that config server is responsible

to give all the standard configuration to all our microservices.

So that means all the default configurations are there, all the common configurations are there, all

those common configuration, rather than adding all those common configuration in all the different

services and duplicating the code.

What we can do is we can create a config server here as we have created and that config server will

get the data from any of the tools.

So in this case we are using GitHub to provide the configuration.

So config server will take all the configuration from the GitHub and it will provide that configuration

to all the microservices that it has.

So we are going to add that particular configuration as well where all of our microservices will try

to get the common configuration from the config server.

So for that reason we have created a config server here.

So we will see in detail how to create that configuration server as well.

And you can see that we have the API gateway also available.

So what it does is you can see that we have different clients available, so all those clients will

try to call the API using the API gateway itself.

And API Gateway is responsible to authenticate or authorize all the requests that is passing through

the API gateway, so any of the client won't be directly be able to call any of the internal services.

So if any of the client is there like multiple clients is trying to execute or trying to call our or

trying to invoke our services, they won't be directly be able to call order service or product service

or payment service.

They have to come via API gateway.

So from the API, Gateway API will be exposed and from the API gateway only the request will follow

through and all this particular request from the API gateway will first be authenticated against the

Okta authentication.

So we are going to use the Okta authentication.

Okta is a security provider authentication provider, and from this Okta we are going to use the JWT

token based authentication.

So we are going to implement this spring security in our project.

And for this spring security we are going to use the JWT authentication.

We are also going to add the role based authentication.

So whenever a request is made, so it will always check if that particular user has a particular permission

or particular roles to access a particular API or not.

If the user has those accesses, then only that particular request will be allowed to go through this

API gateway and to get all the information.

So you can see that we're going to develop a complete end to end application with all the required components.

So you can see that we are adding the service registry to connect all of our services.

We are going to add the default configuration or the common configuration using the config server.

We are also adding the spring security with JWT authentication and we are also adding the API gateway

so that all of our traffic through the API gateway itself.

So this is all something that we are going to build in this course.

Also alongside each and everything, we are going to add the distributed lock tracing as well.

I not mentioned yet, but each and every service will have the implementation of the Zipkin and Sleuth

distributed lock tracing.

So that means from each and any point of time we will be able to identify from where the request was

traverse from which point to which point request was going.

And we can see the logs very easily.

We will understand more in detail about Zipkin and Sleuth when we are going to implement it, but this

is the general idea that we are going to build.

Now with that being said, let's first understand what is service registry and how everything connects.

And then from that on we will start implementing our services.




Transcript - 
Now let's understand what is service registry?

Now, ideally, if you consider one of these simple example, we have one other service and this other

service has the API exposed as.

Please order.

Look from here we are doing the place order and there is a service called Product Service.

And this product service has the API exposed as the get product to get the product information.

And there is a API that is called the radius quantity.

Look.

And that is the API.

Sorry, there is a service called the Payment service and this payment service has the API exposed as

do payment.

So you can see that all these different services are available.

Now this particular order service is deployed on.

192.168.1.1.

This is deployed in 192.168.1.2.

This is deployed in 192.168.1.3 and this is deployed on Port 88 one.

This is deployed in port number 8082.

And this is deployed in port number 88.

So you can see that these are the standard application that I have.

And all these applications are deployed in different machines on the different ports available.

Now, suppose I have done the scaling of my order services because I have multiple orders available

and or I have done the scaling of my payment services because I have multiple payments available.

So what I will do, I will create another payment service.

Okay.

And this payment service, what I will do, it will be deployed in one 92.1 60 8.1.4.

You can see that there is a different IP address in that different machine.

This particular payment service is deployed.

Same goes for the product as well.

Suppose product is also multiple and it is deployed in the other services.

So you can see that there might be multiple services deployed in multiple places and there might be

multiple IP addresses that we need to call.

So ideally, whenever we want to call the product services using the HTTP, what we do is generally

we try to call the.

HTTP.

Colon, whatever the domain name is.

Okay.

And then whatever the EPA is exposed.

So suppose this has the get product API.

Okay.

So this is something that how we will be calling our apps.

Now, you can see that this domain is something that we have multiple that is one domain as well here,

like it is exposed in one of this domain.

It is exposed in one of the other IP addresses as well.

So how we will be able to configure all these particular multiple things.

So for that, there has to be a way where we are not worried about where it is deployed or what is the

IP address or what is the domain for that we should be only focused about I want a detail from the product

service or I want the detail from the payment service for those reasons and to get the availability

of all the different services service industry pattern is used.

So what we will have is we will have a service registry pattern.

So we'll create a service registry.

Once we have the service registry, we are going to implement this service registry using the US server.

Once we have this Yureka server implemented and alongside we have other services like order service.

Product service.

And payment service.

Look, all these different services I have now, what I will do is we will implement the client.

In all these services.

You are also a client.

You are also your client.

So now you can see that all the services are implemented within the client and the main service registry

is implemented using the Yureka server.

So now what will happen is all these services are there, right?

All the services will try to connect to the service registry.

So this other service will try to connect to the service registry available.

This product service will try to connect with the service registry and the premium service will also

try to connect with the your service registry.

So you can see that whatever the services that I have, all those services is trying to connect with

the Service Registry and Service registry will know all the information about all the services.

It will know that there is one instance of order service.

I have one instance of product service, one instance of payment service if there are multiple.

Then also a service registry will know that there are multiple instance of other product services available

with me.

And what it will do is it will allow us service registry will allow us to call each and every API or

each and every services using its name itself.

So whenever I'm calling any of the services, I do not have to worry about what is the IP address,

what is the port information or which domain that particular API or that particular service is deployed

or how many services are there.

Everything will be handled using the service registry so we can directly call from the order service

that OC from the product services I need to get the product.

So in the order service I can directly define that, get the information from the product service.

With Slash.

Get product API.

Look directly.

I can define this and it will connect to the service registry.

And service registry will get the data using the load balancer to us to get all those information.

So you can see that how easy it would be using the service discovery pattern to implement all our microservices.

So you can see that this is just the general idea about the service registry pattern that we are going

to implement in our microservices architecture, but how they will be knowing to where to connect.

All those are the configurations.

So those are the basic configuration that we are going to add in our different services.

So we are going to see each and everything in detail when we are implementing this service registry.

So this was all about the service registry.

Now let's go ahead and create the product services and let's build our application.




Transcript - 
So now to create the product services, as we have seen in the earlier examples, we are going to use

the spring visualizer to create our services.

So let's go to the spring visualizer that is start spring IO.

And yeah, we are going to add our information.

So we are going to select the MAVEN Project languages Java and the Springboard version as 2.7.3.

So whenever you are seeing the course, there might be different versions of spring available, but

I would suggest to use the same version as I'm using.

So there is no compatibility issues, but if you want you can use any other version.

Just try to check the compatibility with all the different components that we are using.

So here in the project metadata, we will add the information.

We will add common dot daily code buffer.

As the group name and the artifact we will give as the product service.

Okay.

And the description is fine.

Package name is product service, the packaging is JAR and the Java is 11 that I'm using.

Now let's add the dependency.

We are going to add the first.

That is the web dependency that as this is a springboard web project, then we are going to add the

JPA dependency as we are going to add all of our data in the database, we are going to use the JPA

and alongside we are going to use the my SQL database.

So we are going to add the MySQL driver and then we are going to add the Lombok dependency.

You can see that Lombok is the Java Annotation library, which helps to reduce the boilerplate code.

So this library will allow us or this library will help us to write our boilerplate code.

So with this library, we do not have to worry about creating the getter setters, the constructors

or adding the loggers, creating the builder design better or anything.

We just have to annotate our classes when we are creating the modules and all those information will

be all those methods will be available to us using the Lombok dependency.

So we are going to add the Lombok to reduce our boilerplate code.

We are going to add our cloud dependency as we are going to use the spring cloud in our project.

So we are going to add the.

Cloud bootstrap dependency.

So you can see that these are the dependencies that we have added.

Now let's go ahead and generate the project.

You can always go ahead and explore the project.

What are the dependencies that is already added.

You can see each and everything here, but this is all okay.

If there is any dependency missing, we can add those dependencies later as well.

So we will just click on Generate Project and open this project in our English idea.

So let's enter the project.

You can see that I have just unzip the project here, product service and we will open this in our IntelliJ.

So let's go to the IntelliJ idea.

And open the project.

And we will go to the documents, springboard microservices, product services and open the project.

So it will take a couple of minutes to download all the dependencies.

Once that is done, we will start working in our project now.

We are going to create the API which will allow us to add the product to our application in our database.

So we will create the ADD product API.

And with that API we will add the products in our system.

So for that, what we have to do is we have to create the different packages as we had added in our

previous examples when we were learning Springboard.

So we will create all those packages and we will continue from there.

So I will open the CRC folder in Java.

Here we need to add all the different packages.

So your first of all, we need the controller package to create the controller.

So we will create a package that is controller.

So until then I will need the.

Service layer.

That is my business layer.

So for that I will need the service package.

Then I will need the repository package to create the repositories.

Then I will need the entity package to create my entities that will interact with our database.

And we will need the.

Model package to create the videos that will interact with the UI.

Interact with the APIs.

And one more thing.

What we are going to do is you can see that in the resources we have the application properties file.

But what we are going to do is we are going to use the application YAML file.

Springboard also supports the YAML configuration and the properties configuration, but in this microservices

example we are going to use the YAML configuration.

YAML is widely used in the industry and whenever we are doing the deployment of these microservices,

it will help us in that as well.

So we are going to convert all these properties into YAML configuration in all of our projects.

So let me just rename this property to.

YAML, so I'll just change properties to Jamal here.

Okay, so you can see that the YAML file is created.

Now what I will do is I need to add some configuration now as this product services is going to connect

to the database, I need to give the configuration to connect to my my SQL and we will go to the application

HTML and we will add the configuration.

We are also going to add the configuration like in which port this application is going to run.

So in the earlier properties file you used to write this way, server dot port equals to 88.

Right.

But you're.

But you're in the Yamal.

All your dot converts to the column.

So here we will write server column port column added.

Okay.

And we will remove this.

You can see that the indentation is also proper.

You have to add the proper indentation and whenever you are facing any issue, it is always because

of the indentation.

So always make sure that your indentation is always correct in the YAML configuration.

So I've just added the first property that is server port, which will work my application on port 88

using Tomcat.

Now let's add the configuration for the MySQL.

I will add the configuration, I'm just copying the configuration, but these are very easy.

We had already covered all this in our springboard examples, so you can see that we just added the

default configuration here.

Basic configuration where spring data source URL that is spring data source or the URL equals to the

URL mentioned.

OC and the username and password you have given the driver class name that we have given and using Spring

dot GPU database platform we have given as my SQL Python DB and the JP hibernate diddle auto equals

to update.

So whenever there is changes in our entities, it will reflect those changes in our database.

Now this is something that you might be seeing different.

So this is something that I have added because what this will do is it will check if the DB host environment

variable is available, then it will use that environment variable, otherwise it will use local host.

So that's the configuration that we have added, that this is supported by the configuration.

If I, if you just want to remove this and just write local host here, that will also work.

It will try to connect my SQL local host Colon 3306 and the database product.

DB.

So this is the configuration that I've added.

What I have to do is I have to create the product DB in my SQL as well.

So let's go ahead and do that.

Okay?

Here you can see that I opened my SQL workbench here and what I will do, I will just try to create

a new schema.

So I'll just right click and create schema and I will give the name.

I'll copy from here.

That is the product.

DB Product DB and I will just apply the changes.

Apply, close it and you can see that the product DB is created.

So we are good now.

Now let's go back to the IntelliJ idea and what we have to do is now we have to create all the different

classes, so let's go ahead and do that.

So the first thing, what we need is we will need a controller to handle all of our request.

So we'll create a controller here.

So we'll just do right click new Java class and I will say product.

Controller and this product controller.

I will annotate with the rest controller.

And at the rate request mapping to map a particular request.

So within this, what I will do is I will pass slash.

Product.

Now, after this, once the controller is created, we need to have this service layer as well.

So let's go ahead and create the service layer.

So what I will do, I will create the interface and the implementation for that service.

So I'll just create a class and I'll just say this is a product service and this will be the interface

and I will create the implementation as well.

So product service, impl, this is going to be class and this is going to implement.

Product service and we are going to annotate this with direct service annotation.

So spring will know that this is a service.

Now, once this is done, we need to create the entity like which particular entity will be created

in our database.

So for that, let's go ahead and create the entity.

So we are working with the product.

So we are going to create a product here.

So I will just in the entity package, create a Java class that says product.

Now this particular product will have some of the entities, so let's add those entities.

It will have private long product ID.

Private string product name.

Private long price.

Private long quantity.

So you can see that these are the standard fields that I've added in the product here.

Now, to make this entity, what we have to do is we have to annotate with it and the annotation, right?

So now this is going to be the entity and we have to add the edit ID annotation for one unique key.

So we have added that as well.

Now we will add the annotation how this unique key is going to be generated.

So we want this product to be incremented one by one for each and every product been inserted.

So we will just use added generated value annotation and here in the generated value we will define

the strategy.

Strategy should be generate type dot auto, so it will be automatically generated on the sequence itself.

Now what I will do is I will also define which particular column name should be for this.

So for this I can define that the column name in the database for this particular product field should

be product name equals to product underscore name.

Okay for price it should be column.

Name equals to price and for quantity it should be.

Column name equals to.

Quantity.

OC You can see that it's very simple.

Now generally what we do is we create the status of all this particular fields, right?

So as we are using Lombok, we do not have to add the getter setters.

We just define at the rate data annotation.

You can see that the data is from the Lombok.

If you just add this other data annotation, all they get a setters equals hashCode method and the required

field constructor will be by default created for you.

So if you just open this, you can see that getter setters required ax constructor to string equals

and hashCode and value.

These are all the annotations part of this one annotation that is added data and it will create all

these methods for you.

Now, whenever you're using the Lombok dependency, you need to add the plugin for Lombok in your ID.

So whatever the ID that you are using for that ID, that is a Lombok plugin available.

So within this IntelliJ idea, if you go to the plugins here.

Okay in the plugin section if I search for Lombok plugin.

You can see that this is the plug in Lombok.

And you can see I have installed it.

So you can also go ahead and install this plugin.

This plugin is available for IntelliJ Idea.

Eclipse sees vs code.

All those text editors and IDs support this plugin so we can add that plugin and it will allow us to

generate all those errors and everything by default.

But if you are not using ID and at the time of building your application, all those methods will be

created for us.

So make sure that you install this Lombok plugins then only all this annotation will work for us.

Now, alongside all the guides setters equals method and hashcode methods.

I need to have a constructor with all the fields, so for that I will just define at the rate all args

constructor and the constructors will be created for all the fields.

And whenever we define the constructor we need to also define the default constructor as well.

So I will define no args constructor.

That will create a default constructor as well for me, and I will also define the rate builder annotation

that is also from the Lombok, which will give me the builder pattern implementation of this class.

So with the help of builder pattern, I can add all the properties in the product whenever I'm using

so I do not have to worry about calling the entire constructor itself if I want.

I just want to add a couple of fields.

I can use that using the builder button as well.

So you can see that with just 2 to 3 line of code.

Using the Lombok.

I'm getting a lot of functionalities in my application now after adding the entity, we need to add

the repository as well.

So let's go ahead and add the repository.

In the repository package, we will go and create a class and we'll say product repository.

This is going to be the interface which is going to again extend JPA repository.

So you can see that this is nothing new.

We have already covered all this part in the Springboard videos and this JPA repository is for the product

entity, right?

And the long is the ID type and we are going to annotate this with repository annotation.

So you can see that your entity is created, your repository is created, your controller is created,

and your services class and interface are also created.

So all your classes are created.

Now let's go ahead and implement our API.





Transcript - 

So what I'm going to do is I'm going to go to my controller layer and we are going to create the API.

Now, as we already know that our controller layer needs the object of our service layer, because from

the controller layer we are going to call our business layer.

So for that I will create the object of it that is private.

Product service and I'm going to inject it using the auto wire annotation.

Now, let's create a method that will handle the ADD product for us.

So I will create the method public and I will just use response entity as my return type.

Of type long and I will just name add product as the method name.

Now this ADD product will be taking any of the request body in the form of object.

So I need to create a class that can handle that particular response body, that particular JSON format

that we are sending into a class.

So for that, let's create the model and let's create the class for it.

So here in the model I will create the product request class, which will help us to take the request.

So your I will create a new class and I will define this a product request and this particular class

will have what details it will have private string name.

We are going to pass the name of the product.

We are going to pass the price of the product.

And we are going to pass the.

Quantity of the product, how many quantity that we have.

Once that is done, what we are going to do is rather than creating the status for this class, we are

going to annotate with accurate data.

So you can see that with just one annotation, one word of code, we are able to generate all of our

code.

So your product request is done.

Now we can use this product request in our controller to get a request.

So we just define product request here.

Okay.

And this product request is going to be my request body, Right?

So we'll just define the request body.

And now what I will do is this particular method that is ADD product is going to be the post mapping,

right?

Because we are going to pose the request, we are going to save the data.

So I have just defined as a post mapping.

So whenever I'm doing the request to slash products post, it will come here and we are going to handle

this.

Now let's call our business layer to add all these operations.

So what I'll do, I will just define long product ID.

Equals to product service.

Dot ad product.

And I'm going to pass the product request here.

Look.

So you can see that I have just simply defined a method here which is going to take the product request

and it is going to send back the product ready.

Now, this method is not available in the product service, so we need to create this method.

So let's go ahead and create a method here in the product service.

You can see the method is created.

Now we'll go to the implementation and we will add the implementation for it.

Now year within the product service, we will need the object of the repository because from our business

layer we are going to interact with our repository, with our database.

So here I will need to have the object of the repository.

So I'll just define private product repository.

And the object of it and I'm going to inject it using the auto add annotation.

So you can see that the injection is done here.

Now, we can use this product repository to do my code here.

Now what I want is in my service layer is I want to log as well.

So I want to implement the logger.

Now implementing logger with Springboard and Lombok is very easy.

So what you have to do is rather than creating the object of a logger and passing your product design,

build class, you can just go ahead here and annotate this class with log four g to.

Okay.

Once you do this, you can have the object of lager here.

You can see that logger object is created for you.

You can see that you now you can use all this logger information, logger methods directly to implement

your loggers.

So yeah, we will add logger info adding product.

Okay.

Now, what we need to do is we need to create the object of a product from the product request.

So to create the product.

Object because this is the product entity and this product entity we can save in our database.

So I'll just create the product here equals to we will use the builder button here.

So you can see I can directly use product dot.

Builder my third year and I can pass all the values so product name I can pass as product request dot.

Get product name.

So you can see that all this method get name and everything is coming by default, right?

We have not added all this gutter setters.

It is coming by default using the data annotation by Lombok and the plugin that we have added.

And then we will add the quantity here.

So we'll add the quantity quantities from the product request.

Dot get quantity price.

Prize will get from the again product request get price.

OC only three information product ID will be auto generated and then we will build this object.

So you can see that the object is created now.

Now I can use product repository method to parse this product to save to the database.

Once that is done, I will return product.

Get product ID.

So you can see that your entire implementation is completed, Right?

I will do one more thing.

I will just log your log dot info product created.

So your method implementation is completed.

Now let's go to the controller and let's send the data back.

So what I will do is I got the product ID here.

Now, now I need to send back to the client.

So what I will do is I will just do return new response entity.

And yet I will pass the product ID and I need to pass the HTTP status.

So HTTP status would be strip status DOT created.

Because we created the product.

So you can see that your implementation is completed now.

So what we will do is we will go ahead and run our application and we will see that variable to save

the product or not.

So let's start the application.

And will enable the annotation processing.

And your application is started.

You can see that on Port 88.

So we will go to our postman.

So let me just start the postman here.

And we create a new request.

Will do HTTP.

Local host call an 8080.

Slash.

Product.

Okay.

This is our API slash product and this is going to be the post request and we'll go to the body raw

JSON format.

And we're going to pass the JSON data.

So if you go to the product request, this is going to be pass this one, that name.

So this is going to be name.

And name is there then price is there.

And then quantity is there.

So I will give the name as I phone.

And the price is suppose.

Uh, 11.

1100 dollars and the quantity I have is three.

And if I click on send, you can see that I'm getting one year D 201 created and if I go to my database.

Product DB tables.

And this is the product table.

And you can see that the product is created with product ID one price 1100 iPhone and quantity is three.

Now, if I go back again, if I say MacBook with price.

$2,000 and the quantity I have is ten.

And if I click on send, I will get two.

And my second product is also created.

So you can see that we just created the API to add the product in our database.

So one microservices created now.





Transcript - and 

Now let's create the other API to get the data, to get all the products back.

Right?

So let's implement that.

So for that, what we have to do is we have to again go to our product controller and create a new API

for it.

So here we will create a new API.

So what we'll do is let's create another method with response entity.

And what is going to be passed back here earlier, you can see that we passed along, but now we need

to send the product based on the IDs provided, right?

So we need to create the model here where we can return the class back.

Right.

So that particular will have all the product information because we are not going to send the entity

object because that is something that we will work with the database or service layer for sending back

to the client.

We are going to have the different Podio.

So here in the model, let me just create the another class that says product response, because we

are going to send the response right, product response, okay?

And this particular product response will have some fields.

So let's add that.

So here what it will have is it will have private string product name.

Private long.

Product ID.

Private long quantity, private long price.

You can see that all this information will have.

And what I will do is I will annotate this particular class with other data annotation to make sure

that I have all the letters.

And I will also annotate this with a builder so I can use this class with builder pattern to build my

object because I have multiple fields here.

So now if I go back to my product controller, I can pass in the response entity as product response.

Okay.

And here I can define get.

Product by ID as a method name.

OC.

And this is going to be my get mapping.

Now, this get mapping, what we will do is whenever I hit slash product with GAT, with Slash idea

of a product, I need to send the data back.

So I need to have a path variable for it.

So I'll just define the path variable here.

This is going to be ID and here I will define.

Long.

Product ID.

And this is going to be the Eldoret path variable.

And this path variable connects to ID.

This is the ID.

This ID.

It will come here and it will store the data in the product ID variable.

Now I need the data back in the product response format, so I will create the object of product response

equals to I will call the product service dot.

Get product buy ID and I'm going to pass the product ID here.

And this particular GAT product by ID method is not available in my product service, so I need to create

this method.

So I will go ahead.

And create the method in my product service.

And I will go to the implementation class that is the product service, MPL, and I will implement this

method.

So the method is implemented.

Now let's.

Complete the code here.

So here I will just add the logger information log dot info.

Get the product for product ID.

Product ID.

Right.

And now I need to get the product.

So I'll just define product.

Product equals to product repository, not find by ID method.

And I'll pass the product ID here and I will do DOT or else.

Because this is going to return the optional find.

My ID is going to return the optional.

So here I will define that if I get the product, that's fine.

Otherwise I need to throw the exception.

So I'll throw the exception here.

So here I need to return.

New I can throw runtime exception.

I can just say.

Product width given ID not.

Found.

So you can see that I have just added the exception.

Suppose we got the product here, right?

We got the product.

There is no exception.

So we need to convert this product into my product response object here.

Right?

So let's do that.

So what I can do, I can use the builder pattern to create the object here, or I can use the bean utils

as well to create our object.

So what I will do, I will create the product response object equals to new product response.

So you can see that it doesn't have the new args constructor, right?

So what I will do is I will go here and I will add the annotation that is the new args constructor.

So now you can see that it has the constructor now.

Right?

So what I will do, I will just do been utils, dot copy properties, copy all the properties from product

and.

This and the target is product response.

Now, this will only do when there is a matching properties.

So let me just check that all the properties are same.

So if you go to the product.

It has the product ID, product name, price and quantity.

If I go to the product response, it has product name, product ID, quantity and price.

So I think those are the same.

So it should work.

So if I go back to my service, I MPL this particular thing should work and ideally what I will do,

I will just go here and add the on demand static import for being utils.

So you can see that the code looks a little bit cleaner.

Okay.

And then I will just return this object.

So you can see that the method is completed now.

Now, let's go to the controller layer.

And here, let's complete the controller.

So here I can say return new response entity and I will pass the product response with HTTP status code.

Okay.

So you can see that this method is also created.

So what I'll do, I will just restart my application and test this out.

Okay, there is some error here.

So let's see.

So I don't know.

It needs all us constructors.

We can add it.

So with all args constructor, the issue is well, so now let's run this again.

Okay.

Your application started on Port 88.

Let's go to my postman.

We already have two records available.

Right.

That is the iPhone and MacBook.

So let's try to get the data.

So with.

Get mapping here with product slash one.

You can see that I'm able to get the iPhone right.

And if I pass product ID too, I'm able to get the MacBook.

But what if I pass any other value?

So.

Okay, so you can see that I'm getting the internal server error.

So now in the next session, let's try to handle this type of error scenarios.

Let's try to handle the exceptions in our application.






Transcript - 
And now I'm in.

My intel is the idea.

If I go to my product service, I'm clear.

What I'm doing is I'm passing the runtime exception here.

That's a standard exception.

But what I want is I want to pass the custom exception.

So what I will do, I will create a custom exception clause and I will handle that exception from you.

So for that, what I will do is I need to create exception class.

So for that I need to have the another package for it.

So let me just create the package here that says exception.

And within this package I will try to create the exception class.

So within this exception package, let's try to add the class and here's what I will do.

I will.

Just create a class and I'll just say.

Product service.

Custom exception.

So you can create multiple exception classes to handle different scenarios.

I'm trying to create one to simplify everything because we have to cover a lot of other details as well.

As we've already covered this in the early stages, so I'm just giving a simple one name over here.

So I'm just creating product service, custom exception class, and this is going to extend runtime

exception.

And I will just add one more property here that is private string error code.

And what I will do is I will add the other data in addition to get all the boilerplate code, and I

will create one constructor here that is public.

Product service which will take.

String message and string error code.

And yet I will call super for message.

And.

This dot error code equals to error code.

So you can see that it's just a simple class.

And we have created a constructor here.

So this particular product service custom exception I will throw when the data is not available in the

service layer in the get product by ID when there is an error, I will throw this exception rather than.

Passing this now it has the constructor which will take two parameters, so I'll pass the other code

as well and I will pass the error code that is.

Product not found.

You can give any error code here.

So I'm giving one of the other code here.

Code right now, what I have to do is alongside this is I need to handle the exception as well.

So.

As we have seen earlier, that we have to add the controller, advise along the controller to handle

all the exceptions, so a proper response is sent back to the client.

Currently, you can see that this is the exception that we are getting right?

We need to have our custom exceptions or custom response exceptions that we need to send, right.

So for that, what we have to do is we have to create a configuration class, we have to create the

controller for it.

So let's go ahead and do that.

So what I will do, I will add in the exception package itself, I'll create a new class which is rest

response and exception handler.

And this is going to extend.

Response entity exception Handler.

Handler.

So this is going to extend this particular class and this particular class I will annotate with the

rate controller advice.

That means this is going to be a controller device.

And whenever there is exception on my controller, this will handle it.

Now let's create the handler for my custom exception.

That is product service, custom exception.

So I'll create a method here public.

Response entity.

And I need to pass a custom object as well.

Right.

So custom object is there.

So let's go ahead and create the custom object that I want to send earlier.

You remember that we have created the error response error message.

That is the custom class.

You also will do similar thing.

So we will go ahead and create the model as the error response.

Okay.

And this response will have some fields.

So let's add that.

So what I will do it will how?

Private string.

Error message.

And private string.

Error code and I will annotate this with other data.

With it all args constructor with no args constructor and add the rate builder.

So you can see that your class is created.

Now I can use this class in my rest response and the exception handler as error.

Response, and this is going to be my handle.

Product service.

Exception.

Now, this handle products is exception, has to inject the product service, customer exception class.

So I will just inject that.

That is product service custom exception.

As the exception, and this particular method is the handler write exception handler.

So I need to annotate with the exception handler.

And this is going to be my.

Product service Custom Exception dot class.

Now this is your exception Handler.

Now I need to create the object of the error response, and I need to send back.

So let's do it.

So I'll just define your return.

New response entity.

This response entity.

And here I'll create new.

Error response.

Dot builder dot build.

So let me add all these details here.

So it will need error message and error message will get from the exception that is exception dot get

message.

It will have dot error code we'll get from the exception, not get error code.

And after this, we need to pass the status quo.

So http status dot not found will pass as not found.

So you can see that your handler is created for your customer exception.

That is product service, customer exception.

So whenever the product is not found, we will send this error message and this status would be not

found and we will send a particular error code as well.

So from that we will be able to understand or the calling service will be able to understand what is

the issue with the API.

So now let's restart our application and let's see if everything is working fine.

So your application is restarted.

Now let's go to the postman.

And here in the postman that is the same API localhost 88 slash product slash.

I'll pass one year with the gate mapping and if I hit send, you can see that I'm able to get a product

back.

So that is working completely fine.

Now if I pass three, that product is not available in my database and if I hit on sand, you can see

that I'm getting the custom error response now, which says error message as product with given I do

not found and the error code is product not found and the HTTP status is for zero four not found.

So these are all the details which we configured in our exception handler with the controller otherwise.

So you can see that my exception handling is also implemented in my product service.

Now let's go ahead and do the fun stuff.

Let's implement the service registry.





Transcript - 


In this session.

What we are going to do is we are going to create a service registry application using the spring initialized

and we are going to add the configuration for it and then later for each and every client application.

We are going to connect all those applications to my service registry.

So let's do that.

So what we will do, we will go to the spring visualizer and create the project.

As last time we had added the same configuration, similar configuration we are going to add here.

So we are going to add the group as the liquid buffer.

And the Artifact Demo Service Registry.

And the name of the application would be the same service registry and the package name I'm giving us

Service registry and the Java version I'm selecting as Java 11 as Java 11 is installed in my machine.

And the packaging will be of jar type.

Now I will go to the dependency and I will add the cloud dependency.

OC Cloud Bootstrap.

And alongside that I will add the Eureka server dependency.

You can see that it is a spring cloud discovery tool.

So it's part of the Spring Cloud, Netflix, Eureka Server.

I will add this dependency and this will help us to create the service registry.

Once this is added, we will generate the project and open in IntelliJ Idea.

So let's generate the project and the project is generated.

Let me just open this an IntelliJ idea.

So here you can see that I have copied the project that is the service registry.

I will open this in English.

I will go to IntelliJ idea and what I will do is here I will go to this Maven year.

You can see on your right side you'll be having this go to Maven and click on the plus button here to

add a Maven project.

If you do this, all your MAVEN projects will be opening in the same window.

If you don't want everything in the same window, you can go ahead from your file open and that will

open that particular project in a new window or the existing window where you will be only able to see

that one single project.

But if you want all your multiple projects in the same window, you can add the MAVEN project from here.

So I will add the MAVEN project Springboard microservices, and here I will add the Spring Registry

Project year sorry, that service registry.

So I will just open this and it will be reflected here and in the project structure.

Also it will just pop up.

You can see that service registry is pop up there.

We'll wait for a second to complete the process.

It will download all the packages and everything.

So once that is done, what we will do, we will open this and we'll go to the XML file and you can

see that we have got the required dependency here.

That is the.

Spring cloud dependency and the Spring Cloud starter Netflix Eureka Server dependency.

Now we have to do the configuration.

So what we will do is within the service registry we will go to the CRC Main.

Java.

And this service registry application file here.

What we will do is we will enable the Unica server.

So here we will add the annotation that is enable Yureka server.

So now this particular application will behave as the US server that is this service registry server

or a service discovery, whatever you want to say.

Now this particular server will act as a registry and all the client application can connect to this

server.

So this server will get to know that what all the applications are there and those are connected with

the server.

Now once that is done, we need to add the configuration as well for this.

So what we will do is we'll go to the resources folder and we will update this application properties

files to the application YAML file.

So let me just rename this, I'll go to the refactor rename and I will update to YAML file.

Once that is done, I will open this and I will add the configuration here.

That is server dot port and this should run on 8761.

Ideally by default.

It runs on 8764 only, but I have added the explicitly.

Now we need to add the configuration for the US ca server so I will add the configuration for yureka.

And instances and hostname.

So by default I'm running on localhost.

So I will give my hostname as local host and then I will give the client information and yea, I will

add resistor with Eureka because this is the server itself.

I don't want this server itself to connect with itself.

So that's why I will make this as a false because this is a server and I'll also make sure that it won't

fetch the registry.

So now this configuration will tell that this is a server and other clients can come and connect to

it.

This is the default thing that we have to add.

Once you add this, you can start the server, you can go to the services, you're at the bottom section.

If you don't see the services, you can go to the View tool windows and you can select the services

from your OC.

Now you can see that it is in the not started.

So we'll go here and we will start the application.

So you can see that the application is started.

And if I hit this eight, seven, six one, it will open the application.

You can see this is my search history and you can see you will get all the information here and the

application connected to it.

Currently you can see there is no instances available and all the other information.

Now this is the server created.

Now we will go ahead and implement the client information in our already existing application.

So we have product service.

So in that product service we will implement the service client.







Transcript - 
Now to add the service client.

What we have to do is we have to add the dependency.

So ideally, if we go here in the spring neutralizer and if I add the dependency, so what generally

I try to do is I will try to get all the dependencies from the spring initialized itself so I can directly

copy from it.

Otherwise you can go to the MAVEN repository and copy from that as well.

You can search there and you can get those information from that as well.

But ideally we have this open, so let's search for it.

So I'll just search for Eureka and I will search for Eureka Discovery.

KLEIN So I'll add this dependency and you can see that this dependency is added.

I don't want Eureka Server, so I'll explore here and I will scroll down and I will copy this dependency.

You can see that this is Spring Cloud Starter.

Netflix Eureka Client.

So I will just copy this client dependency and I will go to IntelliJ Idea.

I will go to the product service and within the product service I'll go to the XML file and I will add

the dependency here.

Look, you can see that we have added the dependencies.

Now we need to add the configuration for this product service like to which particular Eureka server

it should go and connect to OC.

So I will go to the.

Product service.

And within the product service I'll go to the application YAML file.

Let me just put this down here.

And these are the configuration that's our port is this and these are the database configuration.

Now, what I will do, I will add the unique configuration.

So your I will add the eureka configuration.

So you a client fetch registry as true and.

Register with Eureka is also true because you can see that in the server.

We had made this as a false, but here with the client will make sure that this is true.

So it will try to connect and within the Eureka instances and will make sure that preferred IP address

equals to true.

And we will add the service URL where it should go and connect to.

And within the service URL, we will add the default zone and then we need to add the configuration

or where it should go and connect to.

So what I'm doing is I'm just copying the earlier.

So what I've done is I just copied the URL and I have added in the curly braces here.

So that means whenever there is an environment variable available with this at that time it will use

this.

Otherwise, when the environment variable is not available, it will use this URL that is http local

host Call an 8761 slash Eureka.

So you can see that this is the same URL which we have added for the service registry.

If you go to the service registry, that is the same local host call an 8761 port, right?

So that URL we are giving here to the default zone of the service URL in the client application.

So that means my client application will go and connect to this particular service URL.

So you can see that these are the basic configuration that we have added.

Now, what we'll do, we will restart this application and go to the product service and I will restart

the application.

Now we can see that the application is restarted and you can see that it tries to connect to the Discovery

client with the unknown as the name to this port number.

So if you go here to the browser, and if I refresh this, you can see that we got one application that

is with the name unknown with the IP address and the port number, which we already know.

But from here we are not able to understand which this particular application is.

So let's try to give the name of the application so we can easily understand which particular application

is connected here.

So what we will do will go again to the idea.

And.

To this product service in the application YAML file.

Again, we will add the name of the application.

So here we will add spring dot application name.

Equals to.

Product hyphen.

Service.

So we gave the name here to this particular application.

Now, if I try to restart my application.

Earlier, you can see that it was connected with the unknown name.

Now I'll restart the application.

Now you can see that it tried to connect to the Discovery client with the name Product Service, because

that is the name we have given in our spring application name.

Now, if we go back to the browser and refresh my page, you can see that it's coming as a product service.

So now from here we will get to know that within this Eureka Discovery client, we have one client application

connected that is the product service and this is the URL for it.

So now you know that how we were able to create the service discovery and the client that can connect

to the services.






Transcript - 

Now let's go ahead and create the another service that is the other service from which we can place

the order for the different products that we have created.

So for that, what I will do, I will again go to the spring initialized to create the project.

I will close this year and what I will do, I will keep these two dependencies.

That is Spring Cloud and Eureka Discovery.

KLEIN Because this is something that we will need.

What I will do, I will change the configuration here, the group ID.

I will make sure that this is the daily code before and the artifact name.

I will make sure this is the order service.

OC The name of the application would be again order service.

The package name would be order service and the Georgian would be 11.

Now with this we will also need another dependency, so we will add those dependencies.

So here we will go to the add dependencies and we will add the web dependency because this is going

to be my spring web project.

So that has been added.

Now alongside we are going to connect our application to the my SQL Server, that is my SQL database.

So we will add the MySQL driver dependency.

And as we are using my SQL, we will need to use the GP as well to connect our data database and our

application.

So we are going to add the spring data, GPA dependencies as well.

And to do our boilerplate code we are going to use the Lombok dependency.

So we added all this dependencies.

That is the spring cloud dependency, that is cloud bootstrap.

And we also added the Eureka Discovery client dependency.

So my order service can go ahead and connect to my service discovery.

And then I added the spring web dependency to make sure this is, this is a web application hosted on

a Tomcat server.

And then I added the MySQL driver and this spring GPA to connect my application to the database.

And I added the Lombok dependency to make sure that I can generate the boilerplate code for all my modules.

So these are pretty much all that I need currently.

So what I do, I will generate the project and open in IntelliJ idea.

So let's enter the project.

And I will open this in the Intel area.

I copied the project order service here and I will open this in the English area.

I will go to IntelliJ idea.

I will go to Maven again.

And here I will add the MAVEN project.

And from here I will select the.

Order service.

Okay.

And I will open the project so you can see that all the service is created.

Now we have to create the default packages and the default configuration file for our order service.

As we did in our product service.

So let's go ahead and do that.

So we will open the CRC folder main resources and we will convert this application properties to the

application YAML.

So I will just refactor and rename.

I will convert to YAML.

OC And now within this Java, within this other service package, we will need the different packages

for the different activities like controller service and all.

So let's go ahead and do that.

So here I will need the package that is the controller to create my controllers.

Then I'll need the service layer as a business layer.

Then I will need my repository layer.

I will need the.

Entity layer to create the entities for a database.

I will also need a model layer model package to create the models that will interact with my clients.

And what we will do is we will add the application YAML configurations as well.

So what I will do is I will need this server port information and I will need this data source information.

So I'll just copy this information.

Tellier and I will go to the.

Order service and application that yaml file.

And I'll piece this information OC.

And you're my server.

That port should be 8082 because I want to run this application on Port 8082 and spring data Source

URL.

I want to change the schema name.

I don't want product db.

I want order DB.

Okay, so my database is changed.

Username and password will remain the same.

Driver class name, database platform.

Everything will remain the same because that is the same thing that we are using and the name of the

application will change.

So rather than the product service, I need to add the order service.

So you can see that pretty much all the details I've added, but I need to create this database as well,

so I will just copy this name.

Go to the MySQL.

Database.

I will right click here and I will create the schema.

OC.

The schema is created now.

And apply the changes.

I will close here and the.

Database schema is created Order DB.

So now from our order service application, we can use this order service and we can create the different

tables and we can add the data to it.

So now, as all the configurations are done now let's go ahead and create the different classes.

So here within the controller, I would need the.

Class.

That is the order controller.

And this other controller.

We will annotate with a rest controller to make it a rest controller.

And we will also do the request mapping to map a request to my controller.

So I will do slash order and to add the loggers.

I will use the box and the red log for G two annotation to add the loggers to our class.

So these are the configurations then.

Now I need to create the service layer.

So I'll go to the service package and I will declare one interface and I will declare order service

as the interface layer.

And for this order service interface, I will add the implementation class for it.

Order service appeal.

And this is going to be class, which is going to implement order service and this class is going to

be the service layer.

So I will annotate with a direct service.

And for this as well, I will add the log forge to annotation.

So when we add log to an addition, by default we will get the log us.

Now we are going to work with the entities.

We have to create the entities and that entity is we are going to store in our database.

So in the entity model, as is the order service, we want to create the orders, we will create the

order entity here.

So I will right click here and create the new Java class and I will make sure this is the order entity.

Now, within this order class, we need to have some fields, so let's go ahead and add some fields

here.

So I will add private long ID that is going to be my order ID and private long product ID for which

particular product this order belongs to private long quantity.

How many quantities of this particular product ID belongs to this order?

Private instant.

Order date.

Private string order status.

What is the status of the order and private loan amount?

What is the total amount for this particular order?

So these are the general fields I need in my order entity.

So once it's done, we will need to add some of the annotations to make sure this is the entity.

So to make it as entity, we will make sure this is annotated with other entity and we will make sure

this annotated with red Table so we can give the exact name of the table which we need.

So I will give you name equals to order underscore.

Detail.

And then I will annotate with other data annotation.

So this other data annotation is part of the Lombok, so it will add all the getter setters to string

method and add the rate equals and hashCode.

Alongside that, I need a constructor with all the parameters, so I will add with added all args constructor.

And once we add this, I need a default constructor as well.

So for that I will use no constructor and I also need a builder pattern for this class.

So I will annotate with a builder.

So these are all the annotations that I've added now for each and every entity.

There should be one unique key.

So for that I need to annotate with other ID here for my primary key and how my primary key will be

generated.

So I will just define generated value.

Strategy equals to generation type auto.

So by default, auto primary key will be generated based on the values and for each and every fields.

Here I can annotate with a column to make sure which particular column name it should be, so I can

define the rate column here.

And I can define name equals to product underscore ID.

So in the database for this product ID, the name of the column would be at the product ID for quantity.

I'll add column.

Name equals to quantity.

At the rate column for order date would be.

Name equals to.

Order date at the rate column for the status equals to.

Status and for the amount I will just add column.

Name equals to.

Total amount.

So you can see these are the fields and the other column annotation that I've added for each and every

fields here.

So my entity class is ready now for the entity classes.

We need to have the repository as well, so we will get all the methods to interact for this entity.

So we'll go to the repository layer and I will create the repository for it.

So I'll generate a Java class that's going to be an interface.

So let's define order repository as the name for it and I will declare this as a interface.

Now this interface will extend GP repository.

Now this JPG repository is of type order because that's the entity and the idea for the entity is long.

So I'll define along here and I will annotate this with the repository annotation.

So this is going to be my repository in my spring project.

So you can see that my repository is also created.





Transcript - 

Now, what I want to do is I want to create the API to place an order.

So for that, let's go to the order controller and we can create a method to call that particular API.

So what we will do, we will create a method here, but for this particular controller layer, what

we need is we need the object of the business layer.

That is my order service.

So let's create the object of it.

So what I'll do, I'll just define private order service here.

And I will auto wire it.

That means I will do the injection here.

Okay, So you can see that my order service is here.

Now we can create a method for placing the order.

So I will create a method here that is public response entity and the.

Return type would be long because I want that particular order which is generated.

I want to return that back.

So that's why I am using the long year and the name of the method I will define.

Place order.

And what I will do.

I will annotate this with post mapping because I'm going to create this as a post API.

Which will have slash place order.

So I have just defined the post mapping with slash place order here.

Now, whenever I'm trying to do place order, I will need to parse the request body itself.

Like what is the request?

Like which particular order like for which particular product.

I want to create the orders and how many quantities are there.

Right.

So for that I will need the request body.

And that request body we can represent using a class.

So for that, let's create the class.

So I will go to the model year and here let me just create a class that says order request because I

am taking the request of an order.

So I'm just naming it that way.

Now within this order request, which particular fields would be.

So I will have private long product ID, private long, total amount.

Of the order that is there.

Private long.

Quantity, how many quantity we are ordering for that particular product.

And one more thing I want to add here is the payment word.

So how I am going to do a payment for this order.

So for that, what I'm trying to do is I will just.

Make sure that it is a private.

And I am going to create the enum.

So.

Payment mode.

So this payment mode, I am going to create the enum.

So within this model, let me just create the enum for it.

Payment mode.

And this is going to be the enum.

Now here I can add some fields.

What type of payment mode would be so I can have cash, I can have via PayPal, right?

I can have via debit card.

I can have via.

Credit card I can have via Apple Pay.

So you get the GS, right?

These are the different payment type you can add.

So I've added the payment module and in the order request I can use this.

Now, what I will do for this order request, I need to have the setters and the constructors and everything.

So we will use the Lombok to add all those information.

So I will annotate this class with other data to get my status and all the boilerplate code.

Also, I will annotate with that all args constructor to get me the constructor with all the parameters

and the constructor to get the default constructor and build a pattern to get me to build a pattern

for this class.

So you can see that my order request is created with all the fields and all the implementation that

I need.

Now, for this order request, I will go to the order controller and I will take this order request

as the.

Request body.

So I will annotate this with the rate request body because this is going to come as a request body entirely

for my API.

Now what I want to do is from here I had to call my business layer to place the order.

And my return type is long.

So what I will do, I will just define long order ID equals to order service.

Dot.

Please order, because I am going to create the same method here.

And within this I will pass the order request.

And once we get the order I.D., let me just log it.

So log that info.

I will do.

And I'll just mention.

Order ID.

Calls to order ID and once the order ID is there, once we get the order ID, I need to wrap with the

response entity and I need to send it back.

So let me just do that.

I will do return response entity and I'll pass the order ID here and alongside what is the status of

it.

So HTTP status dot.

Okay.

And this is going to be sorry new response entity OC So this is my return value with the order ID and

HTTP status code as the OC for it.

Now you can see that we are getting the compilation error here because it doesn't know there is a place

order method created within the service layer.

So we need to create the method.

You can go ahead and create the method manually, but we will leverage the ID here, so we will right

click here and you can see it's telling that create a method place, order in order service.

So we will create a method there and you can see that the method is created and now we will go to the

implementation class for the order service and there we will implement this method.

So here we will go ahead and implement the method order service.

So now here we can give the implementation.

Now this place Order API needs to connect with the repository layer because whatever the order object

or order entity object that we are going to create that we need to save in the database and to save

to the database, we have to use the repository layer.

So we need to have the object of the repository layer.

So I'll just define that object private order repository

and I will annotate with added auto wire to inject it.

So now within this class we can use the order repository now within the place order.

What we can do is.

Ideally we can create the order entity and save the data with status order created.

Look, this is something that we can do.

After that, what we what we can do is we can call the product service to block our products.

To reduce the quantity.

This means just to check that whatever the order for that particular product is done.

Like suppose I'm selling iPhone or MacBooks and I'm ordering two MacBooks, so I will need to check

that I do have that particular stock handy or not.

If that stock is handy, then we can go ahead and proceed the order.

But if we do not have that much stock that we can tell that we do not have that much stock and all those

information.

And when I'm whenever I'm selling that to iPhones, I need to reduce those quantities as well in my

stocks.

Right.

So those all information or those all processing also we need to do here.

Right.

Once we do that, we need to call the payment service to complete the payment as well.

And if and what we can do is the payment is successful, we can mark the order as complete.

Else we can mark this as a.

Cancelled.

So you can see that these are the general thing that we can do when we do place order, because I need

to place the order.

I need to, by default, create as a order created.

Then I need to reduce the quantities or need to check that all the fulfillments are there properly that

we need.

We have a proper stocks or not, or we have all the business logic handy for that particular product

service that we will do.

So for that, we are going to do a call to the product service.

Then once we have a successful call to the product service, we will do the call to the payment service.

We do not have created premium service yet, but we are going to create the payment service later.

So we are going to create the payment service which will do the payment for that particular order.

And once the payment is done, whatever the status would be, success or failure will get the status.

Then we'll proceed ahead with the implementation, how we have to proceed for this order.

But we can always go ahead with complete or cancel and whatever the status would be, we will store

the status in our database and we will send the ID back to the client that this is the order created.

And we are also going to create another API with which they can get the order details with the status

itself, like what is the status product and all this information.

We are going to create that as well later.

But this is something that we are going to do here in this microservices.

So let's start a simple, let's just create the order entity from here and let's try to save the data.

Then we will call the product service and the payment service.

So here, let's go ahead and do that.

So here's what I will do.

I will just log those information, log dot info, because logging is really important.

With this logging only, you will get to know what is happening within your application.

So here I am just adding placing order request and I will pass the order request object itself.

So here we will get the logs.

Now we need to convert this order request into the order entity and we need to save.

So I'll create the object of the order equals to order.

Dot Builder.

We are going to use the builder pattern to create the object order dot builder and we are going to pass

everything.

So we are going to add the amount amount will get from the order request dot get total amount dot order

status.

We will get us created dot product ID we will get from the order request, dot, get product ID, dot

order data.

Right.

This is going to be data.

So let me just change it.

The spelling mistake.

So if I go to the order request, this is.

Sorry.

This is in the order entity, right?

So if I go to other entity, if I scroll down, this is going to be my order rate, Right?

Okay.

So now if I go back to my service layer and if I say order date, right.

Yeah.

Order date is going to be instant dot now.

Dot.

Quantity we need.

So quantity is going to be order request, not get quantity.

So I can see that all the fields are created already is something that will be created directly.

Right.

So order.

Object is created.

Now I can use the order equals to order repository dot see and I can pause the order here.

Once it's done, I can add the logger again.

Log dot info order placed.

Successfully with order ID.

Order Do get order ID.

Get ID.

Once it's done, I can pass Order dot, get ID back.

So you can see that I have created the Place Order API which will convert my order request into the

order object.

I have added a couple of loggers and I have saved the data into my database.

Now this entire implementation is completed.

If I go to the order controller, you can see that from the order controller, entire flow is completed.

Now we will test this, so let me just start the application here.

So ideally you can go to the orders of this application and you can start your application.

So let me just start the application and you can see that the order services started here on Port 8082.

So.

We will go to the post man to test this post API.

So here what we'll do is we will create the new request.

We are going to create the post request here and we'll do http call an local local host, call an 8082

slash order slash place order.

So this is our request.

We'll go to the body section raw, then JSON format, and here we have to add the data.

So let's add the data.

We'll go to the IntelliJ idea.

We are going to go to the order request and this is something that we need.

That is the product ID.

So let me just copy this total amount then the quantity.

And the payment mode.

I'm going to do cash now.

Amount would be 100, quantity would be one.

Let me check the product ID.

So what I will do is I will go to the database.

And let's go to the product DB tables.

And what product that we have.

So we do not have any products yet.

Right.

So let's go ahead and add the product first.

So what I will do, I will go to the postman again.

I'll create a new request.

Again post request, http call and local host.

This is the one.

Let's go ahead and check again.

So if I go to the IntelliJ idea, if I go to the product service and if I go to the controller, this

is the product, not product.

Right.

And this is also going to be body raw JSON format.

Let's add the information.

So for the product request, what, what are the things we have?

Name, price and quantity, rest, all information I don't need.

So let me just quantity and make sure it is ten prices 1200.

And the name is iPhone OC.

I'll hit on send and you can see that the product is created.

Now I'll go back here and the product idea will pass it as a one year OC and the total amount is how

much the amount was.

1200.

Right?

So I'll pass on 1200 a year for one product OC one quantity and the payment mode is cash and I will

hit on sign here and you can see that I'm getting one year because my product, my order is saved.

So if I go here to my database and if I go to the order B and the table is created, that is the order

details.

And if I use this, you can see that my order ID is created with total amount.

The order date and the status is created.

Product is one and the quantity is one.

So you can see that my order is created now.







Transcript - 
Now, what we will do is, as we've added the service district client in the product service, we are

going to add the same configuration in the order service itself.

So let's go ahead and do that.

So for that, what I will do is I will go to the product service.

I will go to the application of the product service and these were the configuration.

So I will copy this configuration.

I will go here.

Then I will go to the order service, back to the application or the YAML file of that.

And I will.

Are the configuration.

Okay.

Now, once I've added the configuration for that, I will restart my order service.

So now my order service will go and connect to my service registry.

So my order service started.

Now I'll go to my browser.

And I will go to the Utica and I will refresh it.

And then you can see that we have now two services.

One is our service and one is product service connected to my service registry now.

But with this implementation, that is a tricky thing that happened.

If you notice correctly, if you see if you go to the IntelliJ idea and if you see that in my order

service application or HTML file, we have added the configuration right to connect to the service registry,

the same exact configuration I added in the product service as well.

So if I go to the application or HTML, that's the same thing that I added, right?

It's a configuration.

You can see that it's a redundant configuration that I've added in to services.

So now let's see how to remove this redundant configuration in our microservices architecture.







Transcript - 
Now let's understand what we did.

Till now, we have one service discovery, right?

That is your server, and then you have your product service.

And then you have your order service.

Right.

These are the things that you have currently now.

Now, this has some configuration, right?

That anyone can come and connect to me.

Now, what we did is in the product service, we added the config to go and connect to this service

discovery.

And in the order service as well, we added the config to go ahead and connect to this discovery.

Right.

This also had some configuration that go and connect to this.

This also had some configuration and that can go and connect to this address discovery.

Those these both configurations are exactly equal.

So we have done the redundant code in the product service and the order service.

So that is something that we have to handle.

How to handle the redundant code.

Now to handle the redundant code, there has to be some server or some configuration centralized configuration

that can give me this type of centralized configuration and all the services can use that configuration

to add to that particular microservices.

Right?

So what if we add some configuration?

Suppose we just change the color.

Suppose I am adding some configuration.

Consider this is some configuration that I added somewhere.

Consider.

I've added this in the GitHub.

OC.

I write this configuration in the GitHub.

Now this configuration, there's all the configuration that are there in the GitHub I can pass or I

can generate those configuration using one config server so I can go ahead and create one.

Config server.

That can get all the configuration from the centralized repository.

Okay.

It can take all the configurations from the centralized repository that has been added in the GitHub

and can pass all the information to the different services that I have created.

I do not have to add the configuration separately in the product service or the order service or multiple

services that I've created.

I can just create a config server and add the configuration of the config said or that might be just

a dependency and one line of code.

But the default configuration may be one thousands of lines of code as well.

Currently we have very less, but in the ideal scenario there will be a lot of configuration.

So with just one line of code or a couple of line of code that will just say that take the configuration

which is provided by this config server and this config server will have the configuration, like take

all the configuration that has been added in the and you have the central repository.

So you can see that how we just extract one of the component, one of the redundant data and create

a config server and we are passing all the configuration from that itself.

Now all this product and order will now not have to add all those common configuration they can directly

use the config server and they are good to go.

So this is something that we are going to develop today.

So let's go ahead and create a common configuration in the GitHub itself and let's create a config server

which will solve all the configuration added added in the GitHub and pass on as those information or,

or those configuration to the different services that we have created.

So let's go ahead and build that.






Transcript - 
So to create a config server, what we will do is we'll go to the spring initialized again, right?

And we will remove all this dependencies because we don't need all this dependencies.

Whatever we need, we will add.

But here's what we need.

Let's go ahead.

Let's add the group as same.

That is daily code buffer, but our defect name will change to config server.

Because it is going to be my config server.

Right?

And the name is same, the description is same package information is config server.

That's the same charge an 11, that's the same.

Now we need to add the dependencies like which dependencies that will need to create a config server.

So we'll go to the add dependencies and here we will search for config server.

So you can see that this is the config server that is a central management for configuration via gate

SVN or Hashicorp vault.

We are going to use the git for now.

So let's just add this config server dependency and also I'm going to add the client dependency because

this config server should also connect to my yureka client.

So I'm just adding the Eureka discovery client as a dependency.

Now let's generate this project and open it.

IntelliJ Idea.

So I'll go and generate this project so you can see that I've added the configs over here.

Now let's go to the IntelliJ idea.

Let's go to the Maven year and let's add the project.

So we'll click on Add Maven Projects.

And we will add the config server here.

So let's open this project.

Now you can see that this config server is added as a project in your IntelliJ idea.

Let's go, let's go to the three main resources and let's change the application of properties to the

application YAML.

So we'll refactor it.

And we'll change to YAML.

Okay, so now my YAML file is created.

Now let's add the configuration.

So within this config server, let's go to the java and let's go to the main application file that is

the config server application.

And here's what we will do is we will enable the config server with the enable config server operation.

So now my this application is going to enable as a configuration server.

Now let's go to the application YAML file and add the configuration there.

So here's what we will do.

We will add the port information that it should run on.

9296.

This is the port information that I added that my config server will work on Port 9296.

Now I will add the name of the application that is spring application name and this is going to be config.

So and the spelling is wrong.

Let me just correct it.

And then with this, we need to add the cloud configuration.

So I will just add cloud config and server and then get.

And then you or I.

So here we need to add the UI of our git.

So as I'm going to create a GitHub repository, so that GitHub repository URL we need to add here.

So for that, let's go ahead and create the GitHub repository and we will add the configuration for

that.

So let's go ahead first, go to the browser here, Let's go to the GitHub.

This is my GitHub account here.

We will create the repository.

So let's go ahead and create the repository.

So what I'm doing is I'm just giving the name Spring app config ock.

You can give any name, but make sure that from that name you are able to identify what this particular

repository is.

So I'm just adding app configuration.

So this is the configuration for my spring application.

I'm just making a public repository for now and I'm just creating the repository.

So you can see that this is the URL that I got.

So I'll just copy this URL and I'm going to add this URL here in my UI that from https GitHub dot com

slash IBD WD 53 spring config.

I want that configuration.

And then what I will do, I will just add one configuration that is clone on start as true.

So whenever the application is start, it will clone it.

So these are the basic configuration.

Now what I will do, I will go to the order service, I will go to the CRC main resources and you can

see that in the application YAML file.

This is the configuration that we added to connect to the service registry.

Right.

So let me just copy this because this is something that will need by the config server to connect to

the service discovery.

Right.

So I have just added this configuration.

Now you can see that this is a configuration that has been constantly repeating, right?

So what I can do is I can add this particular configuration in a config server that is the central repository

that I've created and add this configuration there.

So from there itself I will be able to get all those configuration.

So I've just copied this here, right?

So what I will do, I will go to the browser, I will go to the spring up config that is a repository

I've created and here I will create a new file.

And I will name this as a application yaml file.

OC and in this YAML file I will add this configuration.

You can see that I added this configuration and I will just commit this new file.

So you can see that my application to the YAML file is created.

Now what I will do, I will go to my IntelliJ idea.

I will go to the order service and I will just command this configuration.

So this configuration I commanded because I am now going to use from the config server.

I will go to the product service also.

And within the product service mean resources application.

I'm going to.

Comment this as well.

So you can see that from both the places I've just commented.

Now, once we have commented all this applications should also know that now, as you have commented

from where we need to take the configurations.

Right.

So let's go ahead and add those configurations as well.




Transcript - 
For that.

What we will do is we will go to the browser and we need to add one dependency.

So let's take the dependent dependency in the config server.

We've added the config server dependency.

Right.

But for this we need a config client dependency.

You can see that this is the config client, right?

Client that connects to the spring cloud config server to fetch the applications configuration.

So this is the dependency that we need to add in all our clients.

That is the product service order service payment service.

Those are going to take the configuration from the config server.

So let's add this dependency here just to copy paste.

You can actually go to the MAVEN repository and get that as well.

But this is something that I feel easier.

And one more thing, you won't get all the dependencies here, but if you don't get, you can go to

the MAVEN repository and copy from there.

So this is something spring cloud starter config, right?

Because this is the server spring cloud config server.

So I will just copy this config dependency.

I will go to the IntelliJ idea.

I will go to the.

Order service.

And I will go to the XML file of the order service and I will scroll down and add the dependency here.

In the dependencies and I will refresh the MAVEN project.

Same thing.

I'll go to the product service, I'll go to the PubMed XML file of that product service and I will add

the dependency.

Okay.

So you can see that the dependencies I have added now.

Now I need to tell these two services from where to connect.

So for that I will go to the application, the YAML file and from the spring year after the application

name spring ten config, then import configuration from config server and then Colin, we need to give

the URL of the config server.

Currently it is local host.

Right.

The earlier that http call and local host call 9296.

So you can see that from here you need to pull the configuration.

So this configuration I will go ahead and add in the order service application or the YAML file.

Scroll up and hear this configuration here as well.

So you can see that I've added the configuration here.

Now, what I will do, I will start my config server application because once the configurable application

is started, then only you will be able to connect your order service and the product service.

So I will start the config server.

And your config server is started, right?

And it is connected to the discovery client as discovery server as well as a config server.

And now I will go to the order service and product service and I will restart my applications.

And now you can see that my order service and product service both are connected.

That means order service and product that is, both are taking the configuration from the config server.

So if I now go back to the browser, if I go back to the record and if I refresh, you can see that

all the servers are able to connect or the service product, service config server server, everything

is connected to my Yureka server and everything is completely working fine.

So that means my all the configurations are coming from this application YAML file that we have created

in the spring app config.

So this is how you will add a common configuration in a config server and you will pass all those information

to all your clients using this country client.

So this is how you are going to start all the common configuration at one piece.





Transcript - 
Now let's implement this reduced quantity API in the product service and once that is done, we can

call that API from here.

So let's go to the product service and let's implement this reduce quantity product API.

So we'll go to the product service and we will go to the Java controller, product controller, and

here we are going to implement.

So here in the controller itself, let's declare a method.

Public response entity and I'm not going to pass anything back.

So let me just declare as a void for now and I will declare as a.

Reduce quantity.

And I will declare this annotation as put mapping here because we are updating something right and we

will give the URLs reduce quantity, we need to reduce quantity of which particular product.

So for that we are going to add the path variable here and here we are going to take those path variable.

So I'm just going to define long product ID and for this product, sorry, it should be comma here and

for this product ID it is going to be path variable.

So I'll just define as a path variable notation.

And this path variable I need to take from the id OC.

And after this product ID, I need to take the quantity as well.

Like how many quantity for this particular product I want to reduce.

You can go ahead with the path variable as well, like multiple path variables, but you can also add

the query parameter.

So yeah, let's add the query parameter so you will get the understanding of using both at the same

API.

So here I will add the request parameter.

It will take the quantity.

OC.

So you can see that two things you are taking here, that is the product ID and the quantity for which

product, how many quantities you want to produce when there is a place order.

So from here you will call the product service dot reduce quantity method.

And you are going to pass the product ID and the quantity that you want to reduce.

And then you are going to return new response entity from your and you are going to just pass as HTTP

status dot.

Okay.

Right.

So this method is created.

Now let's create this reduced quantity method in our service layer.

So I just right click show contact menu and create method, reduce quantity in the product service interface.

So you can see that this method is created.

Now let's go to the implementation and let's create the method here.

Okay, so this method now we need to implement.

So let me just log this information.

Log dot info, reduce quantity this for ID this and here I'll pass quantity and the product ID.

So this is just us normal logger that we have added.

And now, yeah, we need to get that particular product based on the product ID.

Right.

So here I will define the product equals to product repository dot find by ID method and yet I'm going

to pass the product ID and this is going to return the optional.

So I will just define or else throw.

If we didn't find the product, we need to throw the exception.

So I'll just define throw new product service exception and I can define product width given ready,

not found and I can define here.

Product not found.

Right.

So product with given I idea not found and product not found as the error code.

So this is something that I'm going to pass but if not error we got the product right so we need to

reduce the quantity.

So I will just check if product dot get quantity is less than the quantity provided, that means you

cannot place the order right, quantity is less.

So we need to throw the another error so I can just define throw new product service exception and I

can define here product does not have sufficient quantity and the error code would be insufficient.

Quantity OC.

Symbol.

Now, if still it's not there, we can update the quantity.

So I can just set product dot set quantity, product dot.

Get quantity minus the quantity and product repository dot seal product again.

And we can log info that product quantity updated successfully.

Cool, right?

You can see that we have just completed the method.

We just did a very simple thing.

We just added the logger for the reduced quantity and in the product we tried to find the product from

the database based on the product ID, and this will return the optional.

If you get the product ID, that's fine.

If we didn't get the product based on the product ID, we are just going to throw the exception that

product with a given.

I did not found with the error code product not found.

If product quantity is less than the quantity that has been mentioned, that also we are going to throw

the error that we do not have the sufficient quantity to process this.

If everything is okay, we are going to update the quantity, reducing the total quantity that has been

passed and we are going to save the data back.

So you can see that this is normal API that has been created for the product controller and this is

something that we are going to use in our order service to reduce the order.

So now let's go ahead and do that.





Transcript - 

Now before implementing this in our order service API, let's first test this out so we will just restart

the server here.

So let me just go to the services.

And restart my product service application.

So you can see that product service application is restarted.

Now, let's go to the postman.

And let's test it.

So I will just create a new request, http call and local host.

An 8080 slash product slash reduce quantity.

Hope the spelling is correct.

Let me just reconfirm it so reduce quantity.

I'll just copy.

Paste it.

And the product, right?

So and this is going to be output request.

And what we have to do is we have to parse the idea of the product and then as a request, we need to

parse the quantity.

So slash one we are going to parse because if we go here in the product, when we added the iPhone,

we have got one as the product ready.

So that is something that we are going to change.

We are going to pass one as a product ID and then after question mark, we are going to add the query

parameter.

And the query parameter is quantity.

And the quantity I'm going to reduce is suppose 20.

Okay, so that means my already quantity is ten and I'm passing 20.

That means at this time I should get the error right that there is the insufficient quantity.

So that is something that we are going to test.

So when I hit submit here, you can see that I am getting the error message.

That is product does not have sufficient quantity.

Insufficient quantity.

Okay.

So now what I will do is I will change this and I'll pass it one.

But before that, let me just change the quantity as well.

If I change to two and if I hit send, then I should get the different error message.

That product with the given idea not found and the error code is product not found.

So you can see that my both the exceptions are working fine.

Now if I'll pass the same product that is product ID one which I have the product quantity with one

as the query parameter.

So now both the values are correct, so that means they should save the correct values in the database.

And if I hit send, you can see that this is 200 I'm getting.

My written type was white, so that's why I'm not getting any value back.

But if you go back to the database and if you go to the product database and if I hit the query again,

you can see that now my quantity is nine.

That means earlier it was ten, now it is reduced to nine.

So this is my EPA, which we created to reduce the quantity.

When we do the order, it's completely working fine.

Now we will implement this in our place Order API.

So let's go to the integer idea and let's go to the order service and let's go to the order service.

I mean, because at this particular point when we do the order entity after that we need to call the

product services to block the product to reduce the quantity.







Transcript - 
So now how we are going to call the product services.

That is the question that we will get in mind, right?

Because this is one of the API and product services, the another API.

So how we will call the other API from one of the APIs.

So for that we are going to use the frame client.

So first let's understand what is in client.

Now if you are aware about the rest template, that is something is also a API client with which we

can call the other APIs.

But in this tutorial we are going to first use the friend client and later we are also going to use

the rest template that is going to be the different API.

So I will help you to understand both how we can use the client as well and how we can use the rest

template as well to call the different APIs.

Now why paint client?

Now client is the rest client, which allows us as a Java developer to use the API calls using the declarative

function.

So ideally whenever we are calling the API is what we do is we define that.

Let's take the simple example of our order service only.

So we have the order service, right?

So within the order service we have to call the product services, right?

So let's define that.

We are going to call the product services to reduce the quantity, to reduce the quantity.

Now to call this, what we are going to do is we are going to call the type of the method that we have

defined.

Suppose we have defined the put right.

So we are going to do the put request on HTTP column for forward slash localhost or any of the ID IPO.

Right.

Or it might be IP as well.

Right.

So whatever it is, call an port number.

Suppose port number is 8080 slash.

What we are going to do Slash, we are going to call the service name suppose product and after that,

whatever that is, suppose one and after that, whatever the quantity.

So you can see that we are going to call based on the URL that we have mentioned here.

Now suppose rather than this, rather than passing as a local host, what we could have parsed as well

is product service, right?

Because that is something that we have defined in the Yureka server, right?

Because my product service will be referred to as a product service in the Utica client.

So we can call HTTP product service slash product slash one as well, because if we do that, we do

not have to consider about what port number or what IP address my application is running on.

So we can directly call it.

Now this is the way that we would have called when we were using the rest template where we are going

to pass the API, where we are going to parse the URL is the API and we are going to tell like which

type of code it is.

Now this is something about this template where you are going to call based on the IP address.

Now, what if I can tell you that you can create the interface out of it?

I can create the interface that means a declarative approach that this is my product service I can define.

Right.

And within this I can define the methods like how you do in the interface, like you define the method

and you have the implementation for it.

So in the product service, you have the implementation right for this API.

But here what you will do is you will declare that within the product service you need to call the.

Reduce.

Quantity API, which is going to take any of the parameters, whatever you define that is path variable,

right path variable.

And the request param that this is going to be at the rate put mapping.

Write all this thing you will declare here.

And what Finland will do is you will just define that this is a fin client, and this fin client will

represent your entire API is whatever the URL or everything you view as a declarative approach.

And you can directly call all those API using this product service.

So you can see that how usually you are able to declare all those APIs within your application and easily

you are able to call you.

You don't have to worry about declaring the static values or the API parameters or anything.

You just declare that these are the methods that I'm going to call and this method represents the API,

and those APIs are already declared using the fin client.

So what we are going to do is we are going to use the fin client in our order service to call the product

service.

So we are going to declare the product service as a fin client and we are actually going to use it.

So let's see how we can call this product service very easily rather than passing all the URLs and ports

and all those information.

Okay, So let's do that.






Transcript - 

So within a year we are going to call the product service for the block products to reduce the quantity

API.

And if you go to the product service in the controller, this is the implementation, right?

That for the slash product, slash reduce quantity, this is the implementation.

So you can declare this and you can call it directly.

So to implement this, we need to add the dependency of a faint client in our project.

So let's add it.

So what I will do, I will go to the browser again, I'll go to the spring initialize there to make

it easier.

Here I will add the dependency and I will search for fain client.

Let me just remove this so I can search it easily.

I'll search for open fain so you can see that this is the open fin declarative rest client.

Open fin creates a dynamic implementation of the interface decoded with Jacques's R's or spring MVC

annotation.

So with this open fain addition, we will get the functionality to declare all our APIs in the declarative

format.

So let's add it.

Let's go to the explore.

And if you scroll down this is the dependency, right Spring cloud starter, open fin, copy this,

Go to the IntelliJ Idea, go to the order service and within the order service, go to the XML file

and add this dependency.

Okay, Once you have this dependency, refresh your project load making changes, and once you do that,

go to the order service application.

That is your main springboard application file and your go ahead and enable fin clients enable fin client.

So what are the fin plans that we will declare in order service?

All will get all the auto configuration and all those functionalities.

So this is something that we have defined enable fin client.

Now we need to declare our product service.

That is the external service, right?

So what we will do is we will create the external package and in that external package we are going

to add all our required files.

Now, one thing you need to understand here that as we are using the microservice architecture, there

will be some classes or some files that are repetitive that are redundant.

Currently, whenever such scenarios will occur, I will just copy paste the files.

But ideally what you can do is you can create the another service that will contain all the common classes

and that common classes you can import as a library in your project.

So what are the common classes are?

There you can add in a common service and that common service you can use.

But to see over time, ideally we need to learn microservices, not the common configuration.

That is something very easily that you can do just to add the common configuration.

So I will be just giving you that as a home exercise for you to add that.

But currently we are going to add everything in this project in the external package.

So we will have a bifurcation that this is the external packages or these are the external files that

are not part of the order service, but we are using it to make sure that our application works correctly.

So you are in this order service.

Let's create a package that is external.

Okay.

And in this external package, let me just create another package that says client, because we are

going to create a product service client here to call the product service.

So let me just create client here.

Now, within this client where it is your external client, you're I'm going to create the product service.

So let me just create a Java class and let's define product service.

And this is going to be the interface.

Now, this interface, I need to make it as a fain client, so I will just declare a fee hygiene as

a client.

This with the name, what is what is going to be the name?

So the name would be product service.

That means the name of the application.

So if you go here in the product services, if you go to the application YAML file, this is the name,

right product service.

So I will just copy this and this is going to be the name and which which API you are going to call

from here.

So if you go again to the product service to the product controller, this is something that we are

going to call, write, request mapping.

So copy this and add this information so you can see that you just define a product service.

So whenever we will call the product service, it will go to the product service and hit the product

endpoint.

Now which particular endpoint that we need to hit here, that we need to have the declaration from the

product service.

So if you go to product controller, this is something that we need to call, that is the put mapping

slash reduce quantity, this reduce quantity, right?

So I will just copy the declaration part, not the implementation part.

So I'll just copy the declaration part.

And I will add here so you can see that it's simple.

I just declared that I will be removing public because by default the methods are public, right?

So you can see that within the product service where I will be calling product service slash product

in that I have a put mapping API that is reduced quantity ID and this is going to be the declaration

and this implementation is where in the product service and the product controller where we will find

the declaration of it.

So this interface declaration is there in the product service application.

So now we can call this product services easily.

So what I will do is I will go to the order service I MPL.

Right.

As we have added the order repository.

I can go ahead and.

Are the.

Private.

Product service.

And I will auto it and I will just optimize the imports so you can see that I have acquired the product

service.

Now, that product service, I can use it here.

Now, what I want to do is before I do any processing on the order, before I save the order, I want

to call the product services.

So what I can do here is I can call.

Product service, reduced quantity and what we need to pass, we need to pass the product idea so I

can get from the order request, order, request DOT, get product ID and the other thing is quantity.

So I can get from the order request dot get quantity.

So you can see that we're using the declarative approach.

I can directly call using the product service so how easy it is and how clean it looks in the code as

well.

So what we did I call the product service to reduce the quantity, this is going to be the rest API

call using the fin client and the product services will be handled here.

After that, we are handling the order information here and after that let me just add the loggers as

well here.

So log dot info and I will add and I'll add creating order with status created OC Simple thing.

Now my API is complete and from my API place order API, I am able to call the product service release

Quantity API as well to reduce the quantity.

Now I will just restart my order service application and we will test this API.

So let's just restarted.

And my application is restarted.

Right.

So we will go to the postman.

We will go to the order service.

That is a place order and we are going to place the order here.

So currently.

If you go to the database.

Currently we have nine quantities left, right.

And we are going to place the order here.

So let's place the order and you can see that the order is successful.

If we go to the IntelliJ idea in the order service, you can see that we have logs as well, right?

Placing the order request for this order request, creating the order with status created and all those

information that we are getting here if you go to the product service.

You can see that we are getting the logs at radius quantity one for the ID one and product quantity

updated successfully.

So you can see that in both the services, we can see the logs and the request was also successful.

If we go to the my SQL again, and if you run the query, you can see that the product quantity is updated

to eight.

And if we go to the order details and if we run the query again, you can see that the order is also

newly created and the order status is also created with the quantity one.

So you can see that I am now easily be able to call the other microservices using the first microservices

using the fin client.






Transcript - 

Now, what about the different errors?

Right.

So if I go again here and if I pass the quantity.

Yes.

20.

And if I hit send, you can see that I'm getting the internal server error now.

Right.

So my order service in the UI when we are getting the status back, this is something internal server

error.

So I'm not able to understand what happened.

And if I go to the logs and if I see the order service in the order services, I can see that the error

message was product does not have the sufficient quantity and the error code was insufficient quantity,

right?

So in the application, in the backend, I'm getting the correct status, right?

So you can see that whenever I'm hitting put request on this, I'm getting the correct error.

But in the UI we didn't propagate that error perfectly.

Right?

So we need to handle those error scenarios, error decoders to propagate this error information back

to the calling service as well.

So let's go ahead and implement that.

So what I need to do is I need to implement the error decoder here.

So for the error decoder I will go to the external client and here I will add one more package that

is decoder.

So let me just go ahead and create a new package in the external that is decoder.

And within this decoder, let me just create a new class that is custom error decoder.

Now this custom error decoder will implement.

Error decoder.

And you can see that this error decoder is part of the fine dot codec.

So we'll use this.

And once we add the implementation, there is a method that we need to override and implement.

So we'll implement the method that is decode so that whatever the error messages are there that we are

getting in the frame client we can decode and we can show.

So let's handle that.

And for this class we are going to use the Log Forge tool to add the loggers.

And we need to now implement this Decode method.

So let me just use the object mapper here from the first XML to handle the object mapping.

And I'm adding the loggers here.

You can see that it's just a simple loggers where I am adding the URL and the header information.

Whatever we are getting in the response now, you can see that whatever the response that has been sent

back, right?

So that response is in one of the objects.

So if you go to the product service and if you go to the model, it is in the error response because

when we whenever we had implemented this, it was sending the error data in the error response, right?

So this is something that I need.

So what I will do, I will just copy this and if I go here in the external, I will create a new package.

That is called.

Response.

And within this response I will just copy that class that is error response.

This is the class now, this class error response I can use in my custom error decoder because this

is something that is going to return.

So I can just use error response equals to I can use using the object mapper dot read values.

I can use response dot body dot as input stream.

This is something that we are going to get and we need all the data in the error response dot classes.

And you can see that once we have this we need to surround using the try catch.

So let me just surround with try catch.

Okay.

Now once we have handled the error response here, we need to throw that particular exception as well.

So whenever there is an exception here, right, that we are handling using the error decoder, we need

to throw back the exception as well whenever there is an exception.

So we need to have the custom exception how we have added in the product service.

In the product service, if you go, we have added the product service custom exception, right?

So here as well, in the order service we can have it.

So let me just create the package here that says exception.

And within this exception I can create a custom exception.

Now you can create different types of exception based on the different things, but to simplify here,

to save the time as well, because our main focus is the microservices.

I'm just making one custom exception that we can use all the places for now, this is going to extend

runtime exception and I have private string error code and.

Private end status.

These are the two fields I can call the constructor now.

Custom exception which is going to take string message.

String Error code.

And in.

Status.

This is a constructor that I'm creating and I can call super and pass message here and I can set the

values this dot.

Error equals to error code.

And this dot.

Status equals to status OC and to have the status and everything.

I will just annotate with a data annotation so you can see that my custom exception is created so I

can pass on the custom exception from the custom error decoder here.

So I can just do return new custom exception and I can pass the values that is error response dot get

error message error response dot get error code and response dot status, whatever it is.

So this is the exception that I'm throwing from here.

And when there is an IO exception, what I can do is I can.

True new custom exception and I can say internal server error and.

Internal server as the error code and 500 as the status OC.

And this return I can just remove because I don't need it.

So you can see that I have just implemented the Decode method as well here now.

Now as we have implemented this custom error decoder, we need to tell spring as well that use this

custom error decoder rather than the error decoder.

Right.

So we need to add the configurations for that.

So let me just go ahead and add the configuration.

So I will just create a new package that is config.

And within this config I will just create a new class that says fin config.

Okay.

And to make it as a configuration, we need to annotate a class with the configuration notation.

And what we need is we have added the customary decoder.

Right.

So.

This is the error decoder error decoder for inclined.

So what I need is so whenever this error decoder is called, we need to return the new customary decoder.

Right.

And this is the bin of it.

So now whenever the error decoder is needed, it will pass on the custom error decoder.

So now let's restart our application and check.

So we will just restart the order service.

You can see that order is restarted.

If I go back to the postman and if I pass 20 again the same payload and if I hit send, you can see

that I am getting the same error here.

Right.

Now, why?

Because we implemented the error decoder so that finish line can understand that.

Okay.

What were the error message that we are getting?

We need to convert it and we need to pass on.

We need to throw the error messages.

But as we have implemented in the product services, what are the errors that we are throwing that has

to be handled by the spring as well, Right.

Like whatever the errors are throwing, that has to be handled by one of the controller ways that we

create and using that advise.

What are the errors that comes through.

That particular controller should return back to the client.

So that is something that is pending.

So let's go ahead and implement that as well.







Transcript - 
So to implement the exception handling in the order service, let's go to the IntelliJ idea.

And what we will do is we are going to cheat something.

We are going to cheat.

So let's go to the product service.

And in the product service you can see that we have added one of the rest response entity exception

handler.

So you can see this was something that was handling the exception for us in the product service.

So ideally, writing all the code by myself and wasting your time and my time as well.

Let's copy this.

So let's copy this class and let's go to the order service.

And within the order service in the exception package, let's piece this class.

Okay.

Now you can see that the class is pasted at the same blast response and the exception handler with the

response entity.

Exception handler.

But the exception handler is for the custom exception.

So let me just change to custom exception.

And the error response here is from the daily code before order service.

So that is something that we have corrected here.

Now, this also this is also should be custom exception and this is HTTP status message.

This is going to be.

As to the status dot value of exception get status because we are parsing the status as well.

Right?

So rather than using the same status, we can handle it that way.

And rather than doing the handle product service exception, handle custom exception.

So you can see that we just change a few of the things here, but we just copied the class and we change

according to what we need.

So ideally when you are doing the development, please try to use the existing code.

Don't write everything by itself.

It will not make you productive or a better programmer.

This is something that you should always do.

So now let's remove this unwanted imports.

Okay.

Okay.

Now my class is ready.

Now, whenever there will be an exception, the exception will be handled by this particular class that

we have created.

So now let's go ahead and restart our order service.

So now your order service is started.

You can see all your services are started now, whenever there is an exception, when you do the place

order that place.

Order will call the product service.

And whenever there is an error in the product service, that product service will throw back the error.

Your client decoder will handle that error and at that time it will throw the error back.

That is going to be handled by your controller advice that is being created here.

So you will get the proper error message.

What happened in your API call rather than getting this internal server error.

So now when I click on send with the same API, rather than getting this error message, you will get

a proper error message.

So when you click send, you can see that we got the proper error message.

That is product does not have the sufficient quantity and the error code is insufficient code.

Now we can understand, okay, whatever the error we got that was regarding the quantity that we have

passed.

So this way you can handle all the exception handling within the client service and the calling service

as well.

So whenever you have multiple services this way you are going to handle all your.




Transcript - 
So you can see that after handling all this exception handlings and everything.

If you go and see the loggers, right, so you can see within the order service, you have this kind

of loggers within the product service, you have these loggers.

So when you're going through the loggers and when you have multiple applications, right, you will

get overwhelmed, like which particular request is for which and what is the exact logger that we have

to go through to find or to debug the issues.

So you can see that for this order service, right?

There are a lot of you can see that for this order service.

This is something where the placing order request started because we know that this is something that

where the loggers have started.

If you go to the.

Order service, right.

Order controller.

And if you go to the order service yet you have added all the logs like placing order request.

So you can see that you're getting placing order request and after that creating order and all those

things you are getting here right now, whenever we're working with the microservices, a lot of microservices

will come into which are currently we have only a couple of microservices, but when you are working

with the production grade applications, you will be having hundreds and thousands of microservices.

So to do the proper log tracing throughout all the different applications like to understand from where

the requests started and to which particular microservices the request went and what is the exact error

and what happened to that particular request to handle all those kind of scenarios.

We can use different tools.

So what we are going to do is we are going to use the Zipkin and Sloot distributed log tracing tools

in our microservices to handle those kind of tracing.

So we are going to create a Zipkin server, and that Zipkin server will maintain the lock tracing for

us.

So whenever the logs happen, all those logs will be distributed tiddly trace throughout all those different

microservices and we will get a proper information.

What particular log happened, at what particular point of time.

So let's go ahead and implement this.

So to implement the distributed log tracing, we are going to install the Zipkin server.

So we are going to add a Zipkin server, we are going to start the Zipkin server and we are going to

add the dependencies in our project as well.

So our projects will be able to connect to our microservices and all the logs will be traced at one

point.

So let's see how we can do it.

So to install the Zipkin server, what we have to do is we have to go to the browser and search for

Zipkin Server.

So let's do that.

I'll just Zipkin install, I'll search.

Okay, so you can see that open Zipkin is there and you can see that you will have the different options.

How to install this Zipkin server or open Zipkin in your machine.

The best way to do is using the docker.

We are going to learn about Docker in the later part of this course, but here we will see the gist

of it.

So we are going to install Zipkin using the docker.

You can see that you can also do using the jar file, you can download the jar file and you can start

the jar file.

But Docker is the simpler way and the cleaner way.

So that's why we are going to run using the Docker now to go through this step.

What you can do is you can either go to the Docker section and see the video, how to install Docker

in your system, or you can just go to the Docker install, just search for it.

And based on the operating system that you use, Windows, Linux or Mac, you can go to the get Docker

here and you will get all the steps.

How to install it is really very easy to install Docker, so you can just go through and install the

Docker, whatever it is, Mac or Windows, whatever, just go through it and you will get the step,

you will get the binary file, you'll get the executable file, just download it and install it and

you will be easily be able to start Docker.

And once you have installed Docker, just search for the Docker and start the Docker.

Make sure that your Docker is running then only we will be able to run all the steps.

So you can see that my Docker engine is starting, so we'll wait for a minute to start.

So you can see that my docker is started and you can see there are a few of the containers or images

that is already there.

So this is something you can see that there is the Open Zipkin slash Zipkin available.

This is something that I have already installed, but what I will do, I'll just remove it so we can

start it fresh.

Okay, so I removed it.

So what you can do is you can open the terminal here, and if you run the command docker, you should

get all the information.

So that means the Docker is correctly installed and your Docker is running as well.

And if you do Docker images.

You will get a list of images, Docker images as well.

Now, don't worry about anything.

We will go through each and everything when we are learning Docker.

But currently what you have to do is simply just grab this command.

Copy this and paste it here.

I will just briefly explain you What it is doing is you can see that it is just telling you that run

Docker and means run an image which image that is open Zipkin Slash Zipkin.

This is the image.

So we have to run this image on your machine on port number nine for double one.

So this is your host port.

That is.

That means your machine port and nine for double one.

The next port after the column is the container port in which the port is open within the container.

So your container port and your host port are connected.

That is 9421, and that has been mentioned by hyphen p and hyphen D means run this entire command in

the detached mode.

So you can see that once you run this command, your Zipkin is installed.

For me it was already installed so directly it started.

But for you it can take up to 30 seconds to one minute to install and start the Zipkin in your machine.

Once you do that, you can see that you will be able to see in your Docker text up as well that this

is open Zipkin that is running and the name of the container as well.

And now if you go to localhost 9471 was the port number.

So when you go to this port sorry what was the port.

941194.

Double one.

And you can see that your Zipkin is running.

So this is your Zipkin.

Now you can see that this has a few of the properties that you can give your you can run the queries

as well.

You can give the values here.

You can search with the trace ID and everything.

So we will see everything in detail now as the Zipkin server is installed.

Now we need to add the dependencies in our project to make sure that it is connected to Zipkin.

So let's go ahead and do that.





Transcript - 


Now, as we need to add the dependencies of Zipkin and Sloth, the best way that we always go is using

the spring neutralizer.

So what we will do is we will go ahead and add the dependencies here.

So we'll search for Zipkin.

So you can see that this is the Zipkin client that is the distributed tracing with an existing Zipkin

installation and Spring Cloud sleuth Zipkin.

So we'll add this dependency will search and we will add this sleuth as well.

So you can see this is the distributed tracing via logs with Spring Cloud Sleuth.

These are the two dependencies we will need to add the Zipkin and Sleuth to implement the distributed

lock tracing.

So once this add, go to the explorer and copy the dependency.

So let's scroll down.

These are the two dependencies.

That is the Zipkin that is Spring Cloud.

Zipkin.

So let me just copy this.

Go to the IntelliJ idea.

I will go to the order service first.

In the order service, I will go to the Palmer XML file.

And I will add the dependency.

Then again, I will go here and I will add this below dependency that is the spring cloud status loop.

I'll copy this and I will add the dependency.

So this two dependency is something that we need to add.

I will just refresh the MAVEN project and this is something that I need to add in the product service

as well.

So I'll go to the product service, I'll scroll down, I'll go to the PubMed XML file and I will add

these two dependencies here as well.

Dependency and dependency.

Let me just rephrase the main project.

So now you can see that Zipkin and Sloot are also added as a dependency.

A client dependency is in our applications.

Now I will go to the services and I will restart my product service.

And the order service.

So let's restart it.

So you can see that both of my services are working fine and you can see a slight difference in the

loggers as well.

So now you can see that you are getting some this kind of information, but we will see clearly what

happens.

So now as my application is started, let's go to the postman and let's hit the same URL.

So we will place the order and we will just hit on send.

So you can see that the request is completed.

We got the error.

And if you go here, if you go to the IntelliJ idea and if you go to the order service, you can see

that this is the order service, right?

And you can see that with the order service, you are getting two values.

This is the one value and this is the other value for each and every loggers.

If you go to the product service that also you are getting the information that this is the product

service.

And for the product service you are getting to values that is this ID and this ID.

So this first ID is your trace ID, and the other ID is your spine ID.

So this trace ID will be unique throughout your request and this pin ID will be unique per segment.

That is, that means unique scope of the application.

So if you go to this ID, you can see that this ID is similar for all this request, and that's the

same ID for this request as well.

So when you are seeing this request, write that it is also same.

So that means for this trace ID the request started from your OC.

All these loggers are for this for the same request.

And if you go to the product service as well and if you search for this, these are all for the same

request itself.

So now you can see that with the trace ID you will be able to get the entire trace of the different

services for a particular request.

And when all these logs are at a single place to go through your logs from this panel, you can identify

that.

Okay, for this trace ID, this is a spanner.

So all this requests are for or are from the one service that is the product service.

And if you go to the order service, you can see that this all requests, this unique span are for the

order service for the this pan ID sorry, this trace ID for this service.

So you can see that you are able to get all this information with this within this IDs.

And if you just copy this trace ID and if you go to the browser in the Zipkin server and.

Run this query, you can see that you will get a particular request here.

You can see that we did the order service request that is post slash order, place order.

That was 2 minutes back.

There were three spans on it and the duration was this milliseconds.

And if you search by trace idea as well, you will be able to search by trace idea also.

So you can see that all the information we did a post request and within from the order service there

was a product service call and there was the error in the put request in the product service.

And you will get all the information like what was the error and everything.

So from here you would be easily able to traverse the error and what happens to this error?

So if I do the proper request here one and if I hit send, you can see that we got the success rate

200 and we got the ID and if I go back, if I go to the Sipkin dashboard and if I run query, you can

see that the other query also we got that was a few seconds ago and it was successful.

If we click on show, you can see that it's green, everything is successful.

So you can see that all your logs are at one place and you are able to visualize those logs as well.

And from here you can use Zipkin or there are other different tools that are also available to do your

log tracing or to centralize all your logs as well.

But this is a general idea you will implement and you will get a clear picture about what each and every

request is doing and what each and every service is doing based on the trace ID and span ID.

So this is how you are going to implement distributed log tracing in your microservices.






Transcript - 
Now if you check this place.

Order service.

Right.

So within this place, order service, we did this step.

That is the order and receive the data with the order status created.

We also implemented the product service to block the products to reduce the quantity.

Now let's create the payment service to handle our payment.

Now, we are assuming that this is going to be a third party payment, but for the simplicity, we are

just going to create one service that is payment service, and we are going to call that particular

API from the payment service to just mimic the third party payment.

But ideally you would be implementing any third party APIs or third party payment gateways to do your

payment.

We are just making it.

So we need to create the payment service and within this payment service we can call up payment API

method to complete a transaction for us.

So let's go ahead and implement the payment service.

So to implement the payment service, what we will do is we will go to the browser, we will go to the

spring initial user and we are going to create a service.

So here we are going to use the product is May one Languages Java Springboard version 2.7.3 Group has

the code buffer and the artifact name is.

Payment service.

The name is payment Service description is same and the package name is payment service.

The packaging is JAR and the Georgian has 11.

Now let me just remove this dependencies from here and let's add the fresh dependencies.

So what we need is we need a web dependency as this is going to be a spring web project.

So let's add the web dependencies and then we are going to add the JPA dependency.

And with the GPU, I need my SQL driver as well.

Once these are added, I need a Lombok dependency to have my boilerplate code.

Once that is done, I need to have the Eureka client dependency to connect my services to the server.

Then I will need a.

Config client dependency so I can connect to my config server.

Then I need a zipkin client and sleuth for distributed lock tracing as well.

So we added spring web data.

JPA, my SQL driver, Lombok, Eureka Discovery, Client config, client, Zipkin Client and Sleuth.

And we're also needing the cloud dependency, right?

Cloud bootstrap.

Okay, so we have added all the dependencies which we need.

But later, if we need anything, we can add it later as well.

So now let's generate the project and open in IntelliJ idea.

So let me just generate it.

So I've just copied the payment service here.

Let's open it in India.

So I'll go to the IntelliJ idea.

I will go to the Maven.

And I will add the Maven project here, and I'm going to select the payment service and open.

We'll wait for the Maven to complete the process.

Once that is done, you can see that the payment service is here.

So we need to add all the configurations that we have added in all the different services.

So let's go ahead and open it.

So we'll go to the CRC main Resources and we will change the name of the application properties.

So we'll just refactor rename to application YAML.

Okay.

Once that is done.

We will go to the product service YAML configuration file and we are going to copy all this information.

And we are going to go to the application of the YAML file of the payment service.

This is the payment service and.

Application, not yaml file and we're going to paste everything here and the server port number is going

to be 8081 because this is going to run on 8081.

We already used 88 and 8082.

So this is 8081.

And rather than product DB it is going to be payment.

DB And the host information and the port number is going to the same and the password username, is

everything going to be the same?

Now the name of the application is going to be the payment service, so I'll just change to payment

service and the config server is going to be the same, but we need to create the database as well.

So let me just copy this and create a database in my my SQL workbench.

So I'll go here.

And I will just right click create schema.

And I going to create the payment schema so I'll apply the changes.

Apply and everything is done.

Okay, So you can see that my payment tab is also created.

So now let's go back to the IntelliJ idea.

And within this payment services, I will need the different packages as we have already added in the

different applications as well.

So we'll go to the Java application and let's create the different packages.

So first thing we will need is the controller package.

And then we will need is the service package.

Then we are going to need is the.

Repository package to create the repository.

We are also going to be needing is the entity package.

We are also going to need is the model package.

Okay.

These are the general packages we need.

Now, let's go ahead and create the different files.

So let's start with the controller.

So we will need a payment controller here in the payment service within the controller package.

So let's go ahead and create new Java class and we will say this is a payment controller and this controller

we are going to annotate with the rest controller and we are going to add the request mapping as well.

And this is going to be slash payment.

Now, once the control is done, you need to have the service layer, your business layer as well.

So let's go ahead and create the Java class as a payment service layer.

And this is going to be the interface and this interface is going to be implemented by the implementing

class.

So I'll just create payment service, impl.

This is going to be class which is implementing.

Payment service and this class is your service layer, so I'll just annotate with added service.

Now you need to have the entity as well.

Now, within this payment service, we are going to store the transaction details of each and every

order, right?

So we are going to create the entity that is the transaction details.

So let's create a class and let's store.

Transaction details as an entity, and this transaction details will have some of the properties.

So let's define those.

So it will have private long ID first.

That's the primary key.

Then it will have private long order ID for which order this payment belongs to.

Then private string.

Which mode of payment?

It was private.

String reference number, the reference number or the transaction number for a particular transaction

that happened.

Right.

And then private instant.

Payment date as well.

Which date the payment happened?

Private string payment status.

What is the status of the payment and private loan amount?

What amount?

The payment was done.

I think these are the basic details that we will need.

So let's go ahead and annotate this class with an entity.

So make sure there's the entity.

And I will also annotate with table annotation.

And I will give the name of the table so I can do transaction underscore details.

And then I can give other data annotation for data centers at the rate of args constructor at the rate

no args constructor or the read builder design pattern.

These are all the basic details I need and I will need the hazard ID annotation to make sure that there

is one unique key to it and how this unique key is going to be generated.

So I'm going to use the added generated value annotation.

And here I'm defining the strategy that it should be generation type auto.

So my class is created and here I can define the column names as well.

So let me just define the rate column.

And the name equals to order ID at the rate column for the payment mode.

I can define name equals to mode.

Rather it column name equals to.

Reference number.

Other IT column.

Name equals to payment date at the writ column annotation again, and the name of the payment status

should be.

Status and other IT column annotation.

Name equals to amount.

Okay, So these are the basic information that I've added for each and every fields.

I've added the column name for that to be created when the table is created.

Now, for this transaction details entity, we need to have the repository as well.

So let's go ahead and create the repository that is going to be transaction details repository.

And this is going to be the interface and this interface is going to extend JPA repository.

And this is going to be transaction details as the entity and the long as the type.

And this class is going to be annotated with repository annotation to make sure this is the repository

and available in the spring radar.

Cool.

Right.

So your repository is also created so you can see your basic classes and interfaces are ready.

Now let's go ahead and implement the API to make a payment.

So we are going to create a do payment API for a particular order ID and the type of the or the mode

of the payment.

So let's implement it for that.

Let's go to the payment controller.




Transcript - 
So within this payment controller, we need to make the API and to make the API, we also need the object

of the business layer that is the service.

So let's first create the object of the service layer, private payment service, and we are going to

annotate it with the auto white annotation so that we can inject it.

Now let's create a method.

So we are going to create a public method that is going to return the response entity of type long.

And I will just say to payment method.

And this is going to be the rate post mapping.

This is going to be post request.

Now, what details we can pass here from the order service, that is the what payment that we need to

do type of the payment and what is the amount and all those information.

So all this will come as the request body.

So for that request body, we need to have a class that is a request class, right.

Because we are going to take in the request body.

So for that, let's go ahead and create a package and add those information.

So we already have created the model package.

So within this model, let's create the class that is payment request.

Now, this particular payment request will have private long order ID, private long amount.

Private string.

Reference number and the type of payment that is going to be payment mode.

So we already have a payment mode class in the order service, right?

So if you go to the order service.

Is going to be order service.

And within this order service there is a model payment mode.

That is the enum, right?

So we need this enum.

So what I'll do, I'll just copy this and I will go to the payment service and within this model I will

just paste it.

Okay.

And if I go to the payment request, then I will need private.

Payment mode.

Payment mode.

Now, this is the model, so I need to annotate with accurate data to get all the data set and everything.

So the data at the rate all acts constructor.

So to get all the parameters constructor and add the rate no args constructor to get a default constructor

and add the rate builder design pattern with the builder annotation.

So this payment request is created, so I need to go to the payment controller and I can use this payment

request as the input parameter here as a request body.

So let me just add a request body annotation.

Okay.

Now here we need to pass the long back, so I will just define return response entity.

This is of type long.

So I just need to return new response entity.

And this is going to be payment service dot due payment which is going to take the payment request and

it will be HTTP status.

Okay.

So this is your return value.

Now you can see that the due payment is not available yet in the payment service, so I need to create

it.

So let me just create the method that is due payment in the payment request, which is going to pass

the long back here.

Okay.

There is some error here.

So let me just remove this and I just added payment request here.

It should be payment service here.

So this is my mistake.

Okay.

Now I need to make this so I'll go to the context menu again, create a method, do payment, the payment

service again.

And this is going to be long.

Okay.

And now I need to go to the implementation and let's implement this method.

So you can see that this is the matter that we are going to implement.

Now, within this payment service, we need the object of the repository layer.

So let's do that.

Private transaction details repository.

And let's auto wire it.

Okay.

Let me just optimize the imports.

Okay.

Now, this is something that we are going to implement now as we will need the loggers.

So let me just add the log g to annotation to our loggers as well.

And let me add the loggers here in the new payment.

So I'll just inform logger dot info.

Recording Payment Details.

Payment Request.

Now from here I need to create the entity that is the transaction details from the payment request that

we got so that we can save the data into our database.

So let me just create the transaction details close to transaction details, not build a dot build.

Look, if we are using the build up pattern here.

So what all details we need, we need payment details.

Sorry.

Dot payment date.

This is going to be instant dot now dot payment mode.

This is going to be payment request dot get payment mode.

Dot name.

Dot payment status.

Let me do success.

Dot order ID order.

It will get from the payment request.

Payment request dot get order ID dot.

Reference number will get from the payment request, get reference number, not amount will get from

the payment request that is payment request get amount number get amount.

Sorry.

So you can see that my entity is created.

That is the transaction details here.

Now what I can do, I can save this transaction details.

Right.

So I can say a transaction details repository dot save and I can parse the transaction details.

So this object is going to be saved now.

Now let's add the logger so that we can get that this particular transaction is completed.

So I can add log dot info, I can see a transaction completed with ID transaction details, dot get.

Heidi.

Okay.

And once this is done, I can return transaction details, dot get ID so you can see that the entire

implementation is completed now.

So what we can do, we can start the server and let's see that it is reflected in our service discovery.

So let me just start the payment service here.

And you can see that the payment services started.

It is connected as a payment service on Port 881.

So if I go to the browser, if I go to the Yureka and if I refresh, I can see that all my services

are there, I can fix our order service, payment service and the product service, everything is completely

working fine.

Now let's implement the fine client for this payment service and make the call from the order service

when we are doing the purchase order API.







Transcript - 

Now let's call it do payment API of payment service from the order service to complete our payment for

the entire place order API.

So what I will do, I will go to the idea and you can see that this is the API, right?

Do payment that is completely created.

Now what we have to do is we have to go to the order service.

And as we have added earlier for the product services as a fine line created right for the product service.

Similarly, we have to create the in line for the payment services so we can call the payment API.

So what I will do, I will just create a new Java class that is the payment service and this is going

to be the interface, right?

As similar to what we have created for the product services.

So payment service is created and then we need to annotate with fin client.

And you need to give the name of it.

So we will give the name.

So what name we have to give?

We have to go to the payment services application or the YAML file.

And this is the name of a service.

Right.

So this is something we will copy and we will give your slash.

We have to go to the payment controller.

And this is the.

Payment.

That is the request mapping.

So this is something that we have to add.

So we'll add this payment service slash payment here.

Now we will go to the payment controller.

So payment controller is this and this payment controller.

This is the API, right.

Post mapping.

So this is the declaration that we need to copy.

So this is the implementation part.

We need to declare in our order service.

So let's copy this.

Let's go to the payment service in the order service and let's declare the API.

Okay.

And this is the declaration.

And yeah, you can see that we need the payment request as well, Right.

So what we will do, we will go here in the payment service and in the payment service we have the payment

request.

Right.

So we'll copy this payment request from the payment service.

We will come to the order service in the order service that is the external.

And in the external we will create one package that is going to the request.

So let me just create a package.

That is request and within this request.

I will pay this payment request here.

And this payment more.

You can see that we can take from the order service model because here we already have the payment mode.

So that is something that we are going to use and the class is created.

So if I go back to my payment service now and if I import this payment service now, this particular

payment service is completely ready.

Now we can go ahead and use this payment service in our API.

So we'll go to the order service, MPL, This is where we have to call the payment service, right to

complete the entire payment.

And here in the product services we can have the object of the payment service.

So we'll define private payment service.

Sorry.

Payment service and we can do auto wire to inject it.

Now, once we call the product service, reduce quantity, once we reduce the quantity, and once we

have created the order with the created after that, once it is saved, we can call the payment service

to complete the payment.

So we'll just log that information and we'll just say calling payment service to complete the payment.

Let me just correct the spelling here.

Okay.

And now we can create the payment request object.

Payment request.

Equals to payment request dot builder dot build pattern.

Now we can pass all the values here, so let's do that.

Dot order ID, order add equals two order dot, get id dot payment mode order request dot get payment

mode dot amount order request dot get amount.

So these are the fields and the payment request is completed.

Now we can call the payment service.

And what I will do, I'll just check the information.

String order status equals to null and I will add the try catch here.

So what I will do in the try is yeah, we are going to call the payment services.

If everything is okay, then your order is success.

If there is any exception then we will handle in the catch.

So here I will just call payment service dot.

Sorry, not payment request payment service dot two payment.

And yeah, we need to pass the payment request.

Right.

So we'll just pass that information and if everything is okay then we will just add the information

log or info as payment.

Then.

Successfully changing the order status to placed.

And then I will just define order status equals to placed.

And if there is any exception log error and I'll just define error according payment changing order

status to payment field and then I'll just define order status equals to payment field.

Okay.

So I just try catch and handling the exception if there is anything in the do payment API, once that

is done, we will again change back the order status.

So we'll just define order dot set order status equals to order status and then again order repository,

save order, order and order place access successfully.

So you can see that we just added the DO payment API.

And in the DO payment API, we added the try catch block where the payment is successful at the time.

We will change the order from created to place and if there is any exception we will change from created

to payment field.

So we will get a proper information.

What happened with the order?

Now, once this is done, we can restart our application and test this.

So let me just restart our product payment and order service API.

These are the services, so let's restart and wait for it to complete and call from the postman.

So all the services are starting.

So all the services are completely started now.

Now let's go to the postman and let's do the order.

So the product 81.

Total amount 100, Quantity one.

And cash payment.

We will just hit on send and we got some errors.

So let's debug it.

So if you go to the order service.

So in the payment services, there is no API call, right?

So what I will do, I will just check in the server if everything is okay.

Yes, we have the payment service and everything, right?

If you go to the order service and there was no servers available for the product service itself.

So the error is in the product service itself.

So let me just recheck it now if it's working.

Because sometimes it will take some time to attach to the Eureka client, Eureka solver.

So I'll click on Submit Again.

And now you can see that I'm getting the response back.

That is 200 OC.

So that was just the issue of the timings.

So don't worry, sometimes this will happen.

So we'll wait for a couple of seconds to let services get attached to the Discovery Client.

Discovery Server.

Sorry.

So now let's go to the database and let's see everything.

So in my order, if I go to the tables, you can see my order is there.

And in one of the case we can see that the payment is failed.

We will go check the logs.

Payment service.

And there was some issue in the payment.

Right.

So what was the issue?

Let's check that as well.

And you can see that when there was issue, we were able to change the payment status to payment file

as well.

So this was a realistic scenario that it was handled, but there was some matter we need to understand

what is the error?

So you're calling the payment service to complete the payment was called and payment service slash payment

was also called and there was some error occurred in the payment service.

So if you go to the payment service in the payment controller, this is the post mapping.

Right.

And yeah, this is the error we just.

Jane, the information it should be.

PITTMAN Right.

So this was the typing mistake that we did.

Now, if I restart this application, it should work fine.

So let me just restart the application.

Is this still there is issue slash payment will come, right?

Okay, so now let me just restart again.

Okay.

Application is restarted.

Let me go to the browser.

Let's refresh the page and all the services are connected.

Right.

So now let's go to the postman.

Let's change the quantity to two and let's hit on send.

And you can see that the request is completed.

So if you go to the database and if I refresh the table, you can see that this is the fifth order for

quantity.

Two and the order is placed right.

So if I go to the PM and DB as well.

You can see that the payment is completed and the status is success.

So you can see that entire flow is completed if you go to the product.

DB as well.

Let's go to the product.

DB And the quantity should be.

Change.

So you can see that quantity is also changed to four accordingly.

Now, there might be a pie as well where there is some issue in the payment, then your quantity should

be reverted back, your quantity should be added back as well.

But these are the nitty gritty details that we can implement.

But you will get the gist out of it, like how to connect all the different services.

So you can see that from now you will be able to easily understand how to call the different micro services

based from the one API to the different API and all together and how we did everything using the client.

Now let's go ahead and implement another API to get the order details in which we will again call the

different services.

But this time we are going to implement using the rest template.

Earlier you saw we implemented everything using the phone client, but now let's take the example of

a rest template as well because that is also something API client which we can use to connect our microservices.

So let's go ahead and implement that.





Transcript - 

So let's go to the order service.

This is the order service.

And if you go to the order controller here, we can create one of the APIs to get the order information.

Whatever the order is placed, we can get the order information from there.

So we are going to create an API that is public response entity and here we are going to pass all the

information as a response.

So we need the object of it.

So let's create the response object which we are going to pass.

So in the model we can create the order response.

So let me just right click here, create new Java class and I'll just define order.

Response.

This class will have some fields, so let's define that private long.

Order ID private instant order date, private string order status and private loan amount.

These are the fields of order that I want.

What I will do is I will just create the order data annotation to get the gate setters or the rate all

args constructor to get a constructor with all the parameters added.

No args constructor to get the default constructor and add the date builder annotation to get the builder

design pattern.

So all these fields are added.

So I will go to the order controller now and I can define that.

I want the return as the order response.

And the name of the method I'll define as get order details.

And this is going to be the great mapping.

And I'm going to take the slash order slash ID as the parameter value.

Now, this is going to tick the path variable as we have.

Define the path variable.

So let me just define path variable.

And this is going to be long order ID.

Okay.

And here we need the object of the order response.

So order response equals to order service dot, get order details.

And we are going to pass the order ID here.

Once we get the order response, we can return new response entity.

And pass it.

The other response here.

And HTTP status codes.

Okay.

So this is going to be my written value.

Now, this order detail is something that we need to implement, right?

So let's go ahead and implement this.

I am going to create a method, get order details in the order service.

And I am going to implement this as well in the order service impl.

So let's go ahead and implement this methods.

So this is something that we are implementing.

So now let me just log the information here, log out info, get order.

Details for other ID.

And I'm going to have the curly braces here.

So this is the first parameter.

Now, I need to get the order information based on the order ID that I have passed from the order repository.

Right.

So what I'll do, I will just define the order object.

Equals to order repository dot find by ID.

And I'm going to pass the order here.

And if this is going to return the optional and if the order if the order is found, then fine, we

will get the order.

If not, then you are going to throw the exception.

So we'll just define or else.

True.

And here I'm going to define through the exception, new custom exception, because this is the custom

exception that we have defined for the order service.

And we can define order not found for the order ID, order ID, then we need to pass the error code

that is not found.

And I can parse the response that is four or four.

So this is the custom exception that are thrown from here.

Now, from here with this order object, I can create the order response object.

Right?

So I'll just define order response.

Order response equals to order response dot builder dot build.

Okay.

And what are the information that we need?

We need order ID so we'll get from the order.

That order ID.

Dot.

We need the order status so we'll get from order dot, get order status.

Dot We need the amount.

So we'll get the amount from order dot get amount dot order date So we'll get from order dot get order

date.

Anything else we need currently?

No.

So this is all okay and we are going to return the order response from your.

Okay.

So you can see that this is the simple implementation we have done here.

So let's test this out first.

So if I just restart my application, that is the order service application here.

So you can see that my service application is restarted.

Now let's test it out.

We'll go to the postman.

We'll create a new request as HTTP.

Local host call an 8082.

That's the port information.

Right?

Let's check all the service eight zero.

It's to right slash order slash fi.

So let's check if you're getting the order details for order number five.

And here you can see that I'm getting the order information.

That is the order identify order date is this and the order status is placed and the amount is 1200.

So you can see everything is completely working fine.

And if I give any other error that is six so I'll get the proper error as well.

That order id order not found for the order ID six there's not found and the status code is also 404.

Okay, so completely working fine.

But alongside this order details, I want the information about the product as well that has been placed

for this order.

Like suppose this is the order, right?

Which particular product that we placed for this order, that information also I need alongside what

payment?

I did that information also I did.

So that is something to that.

We will get the information from the two different services that we have, product service and payment

service.

So let's implement that to get the information of the product and the payment based on this order.




So let's go to the big idea.

And as we already know, that we are going to implement using the rest template this time.

So let's do that.

So if we go to the product service, right, if you go to the product service and if you go to the product

controller here, you can see that we have an API, right, with which we can get the information of

the product.

So in the order database as well, if you go and check in the order.

Heard in the order as well.

We have the product ID, so once we get the order, we have the product idea as available.

So we will pass this product ID to this, get product buy ID API and we can get the product information.

And for that you can see that as a passing value is just the ID, but the return is the complete product

response.

So what I'm going to do is I am going to go to this product response here and I am going to get this

complete information, product response, and then go to the order service.

I'll go to the order service.

This is my order service.

I will go to the model and go to the order response.

And here's within this order response, I'm going to create a static class OC.

So yeah, you can see that I've just created the inner class here, and this inner class will have the

private.

Okay.

And what I will do, I will, rather than having the product response, I will just define product details

here.

OC And this product details I will just define here product details, product details.

OC So there's just an inner class just for the simplicity, but ideally we'll be having the different

class and we can call accordingly.

OC So this product details we will get and I have just added the object of it.

So to get the product details from the rest template, we need to have the object of a template as well.

So first let's define the rest template object and we can call the product details API or product Service

API to get a product based on that template.

So what I will do, I will go to the order service application and here let me just create a bean.

So I'll just define a bean and I'll just define a method that will return the rest template.

And this is going to return new rest template object.

Okay?

And this is something that I will define as a bean.

And this being I will define as a load balanced.

So if there is multiple services, this list template can load balance those and can give the information

back.

Okay.

So you can see that we just created the rest template.

Now this rest template we can use in our service layer.

So in the order service layer, as we have defined all the different services right here, I can define

one last template here.

So I'll just define this template.

And as we have defined the bean, we can get that bean here as a load balanced.

So I'll just auto wire this information.

Now here in the order response after getting the order, we can get the product information.

So here what I will do, I will just add the logger here logger info and working product service to

fetch the product for ID order dot get product ID and here it is going to return the product response.

So let me just take the product response here.

Equals to now we are going to use the rest template here, right?

So that's template dot read for object.

So to get for object is the method.

And here we need to pass the API.

So we will do HTTP, colon forward, slash, forward slash and rather than giving the IP address of

the port information, we will give the name of the service.

So the name of the services product service, right.

And slash product is the API slash.

What we have to do is plus order dot get product ID, right?

Once we do this, what we need the data back as the product response dot class.

So you can see that we got the product response back based on the rest template and we are calling the

product service slash product here.

We are not calling based on the IP address and port number or any domain information.

We are just calling based on how it is connected to our services history.

So this is the other example with the rest template we are calling the services rather than the client.

Now let's set all this information in the order response Dot Product Details class.

Right.

Equals to order response dot product details dot builder dot build and your I need to pass dot product

name will get from the product response dot get product name dot product ID you will get from the product

response dot get product ID, dot other information, whatever it is you can set.

But let's keep it simple for now.

So these are the information.

Now this information I can set here as well.

So within this order response, I can set DOT product details and set product details here.

So here you can see now product details is also set.

If I restart this now order service.

So now my order service is restarted.

So let's go to the postman and let's hit the same API.

And now you can see that you are getting the product details as well.

We have just set the product name and the product ID the rest of the things we have not said.

So that's why we are getting zero.

But you get the gist that we are calling the other service from the order service using the rest template

to get the details and these requests are load balanced as well.

Now let's go ahead and call the payment details as well.




Now go to the payments service and go to the payments controller.

And here you can see that we have only one API that is for do payment.

We do not have any API that can get back the information for the payments.

So we need to implement that API and then we can call it from our order service.

So let's implement this first.

So within this payment controller we can create a new API.

So here we will define public.

Response entity and we need to parse the type.

So we'll just define payment response here.

Right?

So that payment response is not here.

Whatever the response we need to send, we have not created yet.

So we need to create it, Right?

So let's go to the model and let's go ahead and create a new class that says Payment Response.

And what will this will have.

So this can how private long payment ID, private string status.

Private payment mode.

Private loan, the amount for which the payment was done.

Payment date and for which order this payment was done.

These are the general information that we have stored in the database as well.

Now, to make this payment response, as in Podio, we need to have the data annotation from the Lombok.

To add the getter setters.

It allows constructor to get a constructor with all the arguments and the read no args constructor and

Eldoret Builder.

Nitty gritty details.

Now we can use this in our payment controller, so we'll go to the payment controller and we will use

the payment response here.

Now we can give the name that is get payment.

Details.

Buy order ID and this is going to be get mapping.

And yet you can pass the information as a path variable because this is a mandatory field.

And for the mandatory field we are going to pass as a path variable.

So here I can pass slash order ID and you can see that the entire area is smart and it can understand

that we need to have the path variable.

So let's add the path variable with string order ID.

So these are the advantages of using the good ID.

So now let's return the value and create a method to get the values as well.

So here we'll define return new response entity and the object that we will get from here is the payment

service dot Get payment details by order order ID and we are going to pass the order idea and HTTP status.

DOT is going to be the status code for me.

Now this method is something that I need to create, so let's create this method in the payment service

and this is going to return the payment response.

Now there is a spelling mistake, but you can correct it.

Okay, let's correct it itself.

Just refactor it, rename it and.

Payment details, buy order ID.

So once you do this, it will change here as well.

And now we need to implement.

So let's go to the implementation in the payment service impl and implement this method.

Okay?

Okay.

Now this is going to be implemented, so let's do the code for this.

So let me just add the log as your log in info getting payment details for the order ID.

Now here we need to get the transaction details equals to transaction details.

Repository dot find my order id now find my order ID method won't be there because that's a special

method right?

By default find my ID would be there.

So what we have to do is we have to go to the.

Transaction details repository and we need to define a method.

Now, here I'll define a method in the repository layer for a specific purpose, because we need to

get the database in the order that is mentioned.

So here I'll just define a declaration so it should return transaction details and find by Find by what?

By order ID because order ID is one of the fields in my entity.

So if you go to the entity, this is the order ID defined, right?

So I can copy this.

I can go to the transaction details report repository and I can pass this information.

Information and your all should be capital.

It should be in the case.

So find by order ID and here I can define long order i d what will be the parameter that I'm passing.

So just declaration you need to give the implementation is by default from the spring GPA.

So now if you go to the payment service, I MPL, I can call find, buy order ID and I can pass the

order ID.

I can change year long dot value of.

Not pretty, but by default you can pass the long as well.

So you get the gist.

So now we get the transaction detail information.

So now this transaction detail information has to be changed to payment response.

So let's change to the payment response here.

So let's create the object of the payment response equals to we'll use the builder pattern here and

let's add all the values dot payment ID we will get from the transaction details, dot get ID, dot

payment mode will get from the transaction details, dot get payment mode, dot OC and this is going

to be payment mode dot value of.

OC as in enum dot payment date we'll get from the transaction details dot get payment date dot order

id will get from the transaction details, not get order ID dot status transaction details not get payment

status dot amount transaction details not get amount.

So these are all the fields that we have got.

And at the end I can just pass on the payment response back here.

Okay.

So clearly all the information's are done now.

Now what I will do is this particular payment API, whatever I have created now in the payment controller,

this I can call from the order service to get the payment information.

So for that I will need to go to the order service.

In the order service, I will need to go to the order service MPL, and here I can call that information.

So once I get the product information, I can get the payment information.

So here I can define log info.

I'll just log the information getting payment information from the payment service.

And here I need payment response.

Payment response is not yet available.

Right.

So what I need to do is I will go to the payment service, I'll copy the payment response and I will

come back in the external in the response section.

I will.

PS This payment mode is from the order service model, so my payment response is created now.

Now let's go to the payment service.

I sorry, order service I MPL and I can use the payment response here equals to rest template.

Dot get for object.

And here I can define the URL.

So the URL before that the return type is payment response class.

Now you URL http column payment service.

Right?

This is going to be inside HTTP colon forward slash forward slash payment service slash payment.

This is the thing.

If I go back and check again, payment controller.

This is going to be the payment and then I need to pass the order ID rather than that.

What I can do is rather than that, let me just do slash order slash order ID This makes more sense

as the API.

So what I will do slash payment, slash order, slash ID.

So if I go back to my order service, I MPL slash payment slash order slash.

Plus other dot.

Get ID.

So I will get the payment response now once we get this payment information here.

We need to attach this payment information in my order response that I need to send back.

Similarly, how we added in the product product details, right?

Similarly, we will add the payment details.

So let's go to the order response and how we added the product details.

Similarly, we can add the payment details.

So I'm just adding the payment details directly here just to save time.

So as we added the product details here, we added some fields for the payment details as well.

We added a static class, we added some of the fields and we added the all the arguments here.

Similarly, how we added the product details here, we will add the payment details here, payment details.

Now we can attach all the information in the order response itself directly.

So if we go back to the order service, simple.

How have we got the payment details after the product details, I can do order response payment details.

Payment details equals to order response plot payment details.

Dot builder dot build.

Awesome.

Right.

Now let's paste all the information so payment ID we will get from the payment response dot get payment

ID dot payment status will get from the payment response dot get status dot payment date will get from

the payment response dot get payment date dot payment mode we will get from the payment response dot

get payment mode, dot the status mode and all those things are done.

So this is all I need for now.

So my payment details are also part of the payment response I did and I can attach all this information.

Dot payment details in the payment details.

So cool.

Right?

You can see that we got all the information, we got the order details, we got the product details,

we got the payment details.

Everything is attached to the order response.

That is something that we need to send back.

And all this information we got using the rest template and the rest template.

We have also added as a load balance.

So if there are multiple services we will get a load balance data.

So once this is done, what we can do is we can restart our application.

That is the product service payment service and order service.

Let's restart it and let's check the API.

So all the services are up Now let's go go to the postman and let's hit the same API if I hit send.

You can see that I'm getting all the information here, right?

So I'm getting all the details, I'm getting product details and I'm getting the payment details as

well.

So all the details I'm getting at one point and all these other different APIs called from the rest

template itself.

So this is how you can implement the different API calls using the front line as well and the rest template

as well.





Transcript - 

Now, what we have understood is we have few of the services, right?

So we had the order service, right?

We have the product service and we have the payment service.

Okay.

Till now, what we are doing is we are calling direct all the services.

So whenever we want to work with the order, we are directly calling the order service.

Whenever we want to work with the product, we are directly calling the product service and similarly

with the payment as well.

So we are directly calling all these services based on that information and based on that poor information.

Suppose now all the services are running on one of the servers here internally in your organization.

Now any public request comes right by any of the clients.

Suppose it might be your mobile application, might be web application or whatever it is, the requests

are coming.

So what generally would be all those requests should be protected right from the public network to your

private network.

So that means that whatever requests are coming from this, all these clients.

Right.

Cannot directly come to your order service or cannot directly come to your product service or cannot

directly come to your payment service.

It has to be protected.

It has to come by one of the services.

And that service should be responsible to handle all those checks that all these requests which are

coming are proper or not or are authenticated, authorized and all those information.

So there should be only one single point of contact from where all those services will interact to.

And after that, that particular service will interact with those internal services.

So for that reason we can have the API gateway.

So this is the API gateway, right?

So for that API gateway we can implement and this API gateway will be a single point of source for all

your internal services.

It will act as a gateway for all the requests coming from your client.

So all this clients, right, all these clients will do the request to your API gateway.

Only an API gateway is responsible to find.

Okay, we got the request.

This request should be for which of the services it will identify.

Okay.

This is for order.

Let's send to other service.

It's for product.

Let's send to product service.

It's for payment.

Let's send to payment service and accordingly we can get the data.

And internally, if order service needs some of the information, they can directly get the data from

the product service or the payment service internally, internally, because this is something internal

to your network.

We can handle it internally if we have some information from the API gateway.

So all the requests coming from outside of the network, public network, we can try out through everything

using the API Gateway and API Gateway will handle all those things for us.

So currently we had implemented this entire part of this.

Now let's go ahead and implement this API gateway as well.

So from API Gateway, all our traffics will be traversed.

So let's implement this.






Transcript - 
So now to create the API gateway, we will again use the Sprint Cloud components.

Cloud Gateway.

To implement this and to create a new project, let's go to the spring initialize.

So let's go to the browser and let's go to the spring initialized to create the project.

I will just remove everything here from here.

I don't need this information.

We'll add whatever we need.

So I will be using the Java students under three and the.

Cloud Gateway as the artifact name.

This is my cloud Gateway, and all the information should be the same packaging and the origin and everything.

And now let's add the dependencies.

So to make it as a cloud gateway, what we need is the one as a cloud dependency.

That is the spring cloud bootstrap.

And then we need the.

Gateway dependency.

You can see that it provides a simple, yet effective way to route to EPAs and provide cross-cutting

concerns to them, such as securities monitoring metrics and resiliency.

So these are the default things that we are going to implement in our API gateway.

So let's add the API gateway.

Then what we need is we need the web flux because spring reactive web, because your API gateway will

be your reactive application.

So it's always going to be monitoring with all the other applications.

So let's make it as a reactive application.

And then I'm also going to add the Zipkin here and sleuth here.

I'm going to add the Eureka client dependency as well.

I'm going to add the Lombok dependency as well.

And I'm also going to add the actuator dependency as well.

So these are the basic information that I need to create my API Gateway Cloud.

Spring React to Web Zipkin Sleuth.

Your record is called client Lombok and Actuators, so if there is anything we will add those information.

We added configure not.

Let me just check if he added config client.

Okay.

Yeah.

Config line was missing.

Let's add the conflict line as well.

So these are the things that we need.

Anything extra or whatever we implement later we will add those dependency.

So let's generate this project and open an idea.

I just copied the cloud gateway here.

Let's open an idea.

I will go to an idea, go to MAVEN and import our MAVEN project at MAVEN Projects and I will select

the Cloud Gateway here and I will just open the project.

So let it complete all the configuration for the Maven.

And once that is done, you can see that the Cloud Gateway is here, so we need to add the configuration

for that.

So first of all, let's go ahead and change the resources application, add properties to the YAML file,

so we'll refactor and rename it to the YAML here.

Once we go to the application of YAML, we will need to add a few of the details.

So that is server or port.

This is going to run on port 9090.

And we need to add the spring application name.

So spring application name, I can add API.

Get away.

And then we need to add the configuration for the config server so it can use those config servers.

So I will go to the order service and I'll copy those information.

So I just need this config and import.

So let me just copy this, go to the cloud gateway again, go to the application YAML and piece this

information.

So your configuration is done.

Now we need to do the actual configuration for our API gateway.

So all the traffic is coming to API Gateway and from the EPA Gateway, it can understand to which particular

services we need to send the traffic to.

So that is the configuration that we need to do.

So let's go ahead and do that.

So all the configuration for the different routes that we have, like for a particular route, which

particular services we need to send the data or send the request to all the configuration we will do

using the German configuration.

So let's do that.

Once we add the config information, we can add the cloud config.

So here I will just add the cloud information and in the cloud will add the gateway and the gateway.

We need to add the different routes.

So let's add the different routes.

Now there might be multiple routes.

So we need to define based on the different ideas that we have as we have three different applications,

right order service payment service product service for each and every services will create the different

IDs and we will map the routes to it.

So this is going to be our first ID and your I will define order service.

For the order service.

What I want to do and for this order service, I define the URI.

You or I should be load balanced.

Go to the order service.

So this is the name of the service that we have defined and then predicates.

I define the path path equals to slash order slash star star.

So that means if I go to the order service first.

Okay.

In the order service, I have only one controller with one part OC currently.

That is the slash order.

If I have multiple, I need to specify multiple.

So if I only one that is slash order.

So I will go to the again application of the YAML file of the cloud gateway and I defined that slash

order slash star star.

Everything that comes as a path that should be routed to the order service.

OC Simple thing that I defined here.

Now the same thing.

This all information I have to define for each and every service.

So this is for the product service.

And this is for the payment service.

So let's change the information.

So this order service then.

Payment service.

Payment service and the part should be payment, then this is for product service.

Product service and the part should be product will just check once the products and payments are correct

or not.

So let me just go to the payment service and controller.

It's payment only, okay?

It's not payments.

And if I go to the product service, this is also product.

So my all the configurations are correct now.

So these are the only configuration that I have to do now.

All my traffic that comes to this API gateway will traverse to the appropriate services.

So now what I will do, I will just save this and I will start my API gateway.

So let's go to the not started services Cloud Gateway and start the application.

Now you can see that my application has started on Port 1990.

If I go to the browser, if I go to the Yureka and if I refresh my page, you can see that my API gateway

is available in my service registry.

Now what I have to do is, as we have defined here, right, all my traffic should go to the API gate

rather than going to the order service.

Now how I will define that, what I will do is rather than calling the API or IP address call and port

number of the order service or the product service, I will just define the domain name or the local

or whatever it is with the port number of the API gateway.

So all my traffic should go to the API gateway and EPA Gateway will define where to traverse the traffic.

So for that, if I go to Postman currently you can see that I am passing http localhost 8082 slash order

slash FY 8082 is the order service.

Rather than that, I'm just changing to 1990.

So this is my API gateway.

So this request will go to the API Gateway.

And from API Gateway, my request will travel to the order service and I should get the result back

if I click on send.

You can see that I'm getting the same information.

So that means my all the traffic went from the API gateway now rather than going from the particular

services.

So now there is an API gateway in front of each and every services that we have.

Sounds cool, right?

So this is how you will implement the API gateway in your microservices architecture.






Transcript - 
Now let's implement circuit breaker in our application.

Before that, let's see what is circuit breaker.

So we might all know what is a circuit, right?

When when the circuit is closed, the current pauses and when the circuit is not closed, the current

doesn't pass in the electricity right in the electrical circuits.

That's how generally it works.

So it's a similar concept here as well.

So we need to understand why we need a circuit breaker and what is the circuit breaker altogether.

So if we take a normal example here and let's understand why we need a circuit breaker is suppose we

have one API gate, right?

We already have created one of the API gateway here and with the API gateway, what we're doing is we

are going to call the order service.

Spelling is incorrect.

This is the order service.

And from the order service we are going to call the product service.

And we are also going to call the payment service.

And as this is a microservice architecture, we are going to call one or more services from one of the

services.

And there might be chances like any of the services not working.

Suppose at one of the time there are multiple instances of order service and there might be chances

like this entire service is not working right and there might be chances as well.

Like the product service is not working.

And whenever you want to get the data, I suppose I want to get the data for the get orders.

So I'm just doing get orders call here.

Right.

So what we are going to do is we are going to call the API gateway, right?

Because from the client we are going to call the API Gateway.

And from the API gateway, it's going to call the order service.

And from the order service it will try to get the product information and it will try to get the payment

information from the payment services.

And at that particular point of time, the order service is down.

And when the order services is down and whenever all the requests will come to API gateway and it will

try to call the order service, all the requests will be end up in the error because order service is

down.

So what API gateway is doing is it is taking all the requests blindly and it is passing all the requests

to the order service because API gateway doesn't know that order service is going to run or not.

Either order service is going to return with the data or not, or the service is up and running is healthy

or it is down.

It doesn't know all those information.

So what API gateway will do is, okay, I got the request, I want the order details.

Let's pass the request to the order service and to get the data.

So what will happen is until this order services down, API gateway will pass all the requests to the

order service to get the data, and it's just a waste of resources and the waste of time as well.

Like we already know that the order service is down.

So why to send all those requests to the order service, Right?

So what we can do is we can have a mechanism where it will constantly check, like if the order service

is working or not.

And if it's failing, then let's not send all the requests to the order service.

Let's wait for some time to start that particular service.

And then when the service is up, let's send all those requests.

That is a very mindful thing to do, right as a human behavior.

If you see that's a very natural thing to do.

Like if something is down, then why to send the request or why to utilize all those resources as we

know that it's going to return in the error.

So let's wait for some time.

Let's wait for other service to get up, and then we will try to send all the requests, which makes

sense, right?

So in this scenario, circuit breaker will help us.

So this is a design pattern that we can use and there are a lot of different libraries allow us to do

that.

Earlier we used to use the history library, but that is something deprecated now.

So we are going to use the Resilience Forge Library.

So resilience for the library allow us to implement the circuit breaker in our architecture.

So what circuit breaker will do?

Circuit breaker will have different states and based on those different states, it will try to handle

all those scenarios.

So suppose I want to call the order service from the API gateway.

So what I will do is add this API gateway label.

I will add one circuit breaker and that circuit breaker will handle all those scenarios for the order

service.

If I want to handle the product service, then what is the calling service for the product service that

is the order service.

So at the order service level, I will handle the circuit breaker.

So you can see by adding the circuit breaker we will able to handle what type of information and what

how we want to control the rest APIs.

So let's take the simple example here.

What I want to do is within this API gateway, I want to call the order service.

So at this particular point I am trying to add the circuit breaker here.

And what I want is if the service is up, if this if the order service is up, gather detail so it will

try to get me the details.

Whatever I get from the order service.

If the order service is down, that means my order service is not working.

It's sending me the internal server error.

That service is not available.

So at that time what I should get is I should get a response back saying that the order service is down

so we can say to the client as well, okay, there's something is down, try to do the request after

some time.

So that's the thing that we have to do that we want to do.

So what I will do is I will add a circuit breaker at this point.

And now how circuit breaker works, Circuit Breaker works on the principle of the different status.

It will have different state, and based on that state, it will handle all the routes.

So let's consider that.

So what we will do is it will generally have to three states.

That is the close state, open state and half open state, and this will be all connected in a triangle.

Now, how everything will work is by default, when everything is working fine at that time, your state

will be closed.

So that means your entire circuit is closed, so all your requests can pass through and everything will

work completely fine.

That's the healthy scenario.

That's the closed circuit.

Now what will happen is suppose your order service went down.

I'm just writing here.

That order.

Service went down.

Once the order service is down.

There will be a request from the API gateway to the order service and that request feel.

Other requests come and that request also failed.

Third request.

That request also failed, so you can give the number of requests in the circuit breaker to understand

that vinyl circuit should be changed from close to open so you can block all the connections.

So suppose you have given that if consecutive ten requests are failing, that means something is wrong

in the service, right?

So at that time what it will do is circuit breaker.

It will change the status from close to open.

And when the status is open, the circuit is open.

So that means no request will be allowed through the circuit breaker.

So from the API gateway itself, your request will not go to the order service.

It will be blocked at that point itself.

Now your state is the open state in the circuit breaker.

Now, how much time the state will be in the open state?

So for this thing you would have given some time interval.

Suppose 10 seconds.

So what it will do is for 10 seconds, your state will be in the open state.

And after that, ten second it will change to half open.

So you can see that your state is not directly changing from close to open.

It is going to half open Now, whenever your state is in the half open, what it will do is half open

means half is closed and half is open.

So that means half of the request will be passed through and half of the request will be blocked.

So that means if ten request is coming here, five, the request will be allowed to pass to the other

service and five of the request will be blocked and.

Out of that five request, out of that five requests which are allowed, which are passed in that particular

thing, we can tell the circuit breaker that from those requests which are passed and in that particular

fire request, what are the what is the success rate?

So if we tell that whatever the requests are passed from that, if the 60% or 70% of the request is

passing, which are successful, then at that time chain the half open straight to the closed state,

which means most most of my requests which are passing, right, those are successful.

That means my order service is coming up.

So that means I can change my status to the closed status for my circuit breaker.

And all my request can be passed on can be successful.

But if whatever the requests are passed from here, all those requests success is below the threshold,

which which means whatever we have given here, that means if 60% or 70%, whatever the percentage that

we have given that threshold, if the success rate is below that threshold, below the threshold, that

means that my service is not up.

So at that time, what it will do is it will again change the status from half open to the open state.

And again it will go to the open state.

In the open state, it will wait for the given period of time.

And after that period of time it will again change back to the half open.

And in the half open again, it will do the same process.

It will be half open and half closed.

So it will allow the half of the request and for that half of the request it will check the threshold.

And if the success rate is below the threshold, it will again change to open.

If the success rate is above the threshold, it will change the closed and the circuit will be closed

and all the requests will be allowed.

So that's how generally the circuit breaker will work.

And at a particular point we can add the circuit breaker.

Now what we can do is at this particular point, when the status is in the open, we can add the fallback

method.

Whenever the circuit breaker is open, we can call a fallback method, which can send a response to

the calling service back to the client back as well.

Now, there might be multiple scenarios what fallback method needs to do and what fallback data we should

send.

Suppose when it's very critical thing to do, like some of the transactions, some of payment has to

be done or something like that, we can directly throw the message that this service is down and in

the other case is also what we can do is we can just identify, okay, what type of data.

Suppose I'm just sending the static data itself and what I would be doing is I'll be taking all the

data from the database and sending back from that service.

What I can do is rather than doing that in the fallback method, I can define, okay, that's fine.

I'm not able to get the data from the database, I can get the data from the caching if I have implemented

and send the data from the caching itself.

So these are the different scenarios which you can handle using the fallback methods, using the circuit

breaker.

So at the end all your requests won't be and of failing something or something should happen for your

request.

So with the circuit breaker, we will have a mechanism to understand what's happening with the request

and we will allow all the services also to handle all those scenarios.

Like suppose if a service is down, we will give a time to that particular service also to be in the

healthy state.

So this way the circuit breaker will help us and now we will implement the circuit breaker in one of

the four services.

So let's go and implement that.

I hope you got the proper understanding about what is a circuit breaker.

But if still, if you have not got do let me know in the discussion.

We can have a more discussion on that as well.




Transcript - 
So what we will do is we go to our daily idea and we will go to the Cloud Gateway, because in the Cloud

Gateway we are going to implement the circuit breaker.

Now, if you go to the Cloud Gateway and if you go to the XML file.

Here, we need to add the dependency.

So what we are going to use is we are going to use the resilience for dependency.

Resilience for gives us the ability to implement the circuit breaker in our application.

So if you go to the browser and if you search for resilience forge.

Okay.

You can see that it provides the circuit breaker.

And if you want to go through the entire documentation, you can see that it has the diagram, whatever

we discussed just now, and you can go through each and everything can see that there are different

options available like count based sliding window, time based sliding window.

This are the different mechanism for the circuit breaker and all the things available so you can go

through the entire documentation and you can learn each and everything about the circuit breaker and

how to implement in the different scenarios.

But this is something that we are going to implement today.

This is the dependency.

So you can go here, you can get the dependencies and everything from here.

But as we are using the springboard, we can leverage the springboard status.

So what I will do is I will go to the spring analyzer.

And I will add the dependency that is the resilience forge.

You can see that spring cloud circuit breaker with resilience forge as the underlying implementation.

So if we add this, we will get the spring cloud starter, circuit breaker resilience forge.

So all the dependencies which were mentioned, right, all the things we'll get in one starter so we

can copy this and you can go to the indulge idea and add this dependency.

So after this reactor plugin, I can add this and I will refresh my MAVEN project.

Okay.

Once we add this, what we will do is we will go to the configuration of the Cloud gateway.

We will go to the application YAML file.

And here we can add the configuration so you can see that we have already added the configuration for

our gateway that whatever the URL would be for that URL, which service should be called.

So if that particular service is down or anything, when we are not getting a proper response at that

time, we can call the circuit breaker.

So circuit breaker implementation we can add here, we can add the filters for that.

So what I will do is after this predicate of the order service here, here I can add the filters, So

I'll just add the filters here.

I'll give the name of the fielder.

There can be multiple filters.

So I'm just adding a list.

So I'll give the.

Circuit breaker as the filter.

And here I can parse the arguments and I can add the name of the argument.

So I'll give the name as order service and I can give the configuration, like when the circuit breaker

is called and when the circuit is open, which particular fallback Uri I should send the data to.

So for that, what I have to do is I have to define one of the API as well.

So when I'm not able to call the order service, it should go back to that particular fallback.

Uri.

So I can add the fallback URI here and I'm adding forward call and forward slash to order service fallback.

So that means I need to create this order service fallback as the API, so that if the order service

is down, I can forward all those requests to this fallback API.

So for that, what I will do is I will go to the Java, I'll go to the Cloud Gateway here.

I will create one controller package because I'm going to create a controller, right?

So within this controller I can create a fallback controller.

So let me just create a fallback controller here.

I'll create a Java class and I'll just say fallback controller.

This fallback controller is going to be a rest controller.

So I'll just define this a rest controller.

And I can here define one of the APIs, and here I can define a method, public string and the name

of the method order service fallback.

And I can return from your order service is down.

And this is going to be my gate.

Mapping is as it is an API.

And in this I can define.

This order service fallback.

So you can see that we just define one of the API here that is the order service fallback.

And this particular method is going to return all the services down.

Simple string message and this is going to be called from the application YAML file.

If you go here.

Here we have added a filter and that filter is a circuit breaker filter and the name of the circuit

breaker we have given is the order service and the fallback UI would be forward slash slash, order

service fallback.

Now the same thing, same filter.

I can go ahead and implement for the payment service as well and for the product service as well.

Right.

So what I do, I will go here for the payment service and I will add this filter.

I'll go to the product service and same I will add for the product service.

You can see that the indentation is very important here.

So now for the payment service, I'll just copy this payment service and add the payment service name

here.

And rather than the order service, I will call the payment service fallback.

Same here.

I will copy the product service here and I will add the product service.

And rather than the order service fallback, I will call the.

Product service fallback.

Okay.

Now these two products, service fallback and the payment service fallback, these are the two APIs

also I need to create.

So I'll go to the fallback controller.

I'll copy this one API and I can just duplicate this and I can copy this payment service fallback.

Add your payment service fallback, copy this product service, fallback and copy here so you can see

that two methods are there.

Here I can do payment service fallback.

Here I can go product service fallback.

This is going to be my payment services down and this is going to be my product services down.

So you can see that three methods are created.

These are the three fallback APIs that will be called when a particular service is down.

So this is very simple.

So we can see that in the Cloud Gateway, we implemented the circuit breaker for our different services.

So now what I will do is I will restart my Cloud Gateway application.

Okay.

This is giving error for Zipkin.

Zipkin is a sleuth.

Is not running.

I guess so.

Let's check this.

You can see that our open Zipkin is not running.

Let's start it.

Okay, let's just restart our application again.

Okay.

There we are getting is unable to find the gateway filter factory with the name circuit breaker.

Right.

Okay, so we have forgot to add one of the configuration.

So let's go ahead and add the configuration.

Sometimes when you're working with a large projects, these type of things will happen.

So don't worry.

So what we'll do, we'll go to the Cloud Gateway application and here we will add the configuration.

So spring will know about the circuit breaker that we have added, which means that whatever the configuration

that we have added here, this will be added to the context.

So here will go and we will add the configuration.

So we need to add a customized bean for our resilience for a circuit breaker.

So we'll create a method, public customizer, and you can see that this customizer should be from the

this one or this spring framework to customize, because here we can pass on the information what thing

we need to customize, right?

So here I need to customize resilience resilience for JSON kit breaker.

Factory.

So this is something that I want to customize.

And I can say this is going to be my default customizer.

And this is going to be been because after that only it will be in the spring spread out right once

we create the bean.

And yet what we will do, we will pass on the default configuration for now.

But whatever we want to configure, we can configure accordingly, but we will give all the default

configurations.

So I'll just define return factory.

Factory dot configure default ID new resilience project config Builder and we are going to pass the

ID year and year after id dot circuit breaker config and here you are within this circuit breaker config

dot of defaults, dot dot build.

So you can see that it's just a simple customizer we have added for the resilience for Circuit Breaker

Factory.

We have added the configure defaults for the resilience 4G circuit builder and we have added the default

configurations here.

Nothing else.

Now let's start our server.

So we'll go to the services again and let's restart our start our application.

Let me restart everything.

Sometimes restarting will solve all our issues.

Okay.

I was going through this, searching this for the error that we are getting.

And I found out that we might have used the wrong dependency as we are using the reactor dependency

right in our service.

And you can see that cloud gateway.

We are using the reactor.

The dependency that we have added should also be reactor.

So if I just add the reactor resilience four J Right, Reactor resilience four J.

This should work.

Let's try this out.

Okay.

This has been added now let's restart our service.

And hopefully you can see that our service is running.

So it was just a wrong dependency that we have added as we're using the reactor project.

This has to be a reactor dependency.

So now what we will do is we will go to the postman and we will try to call our API.

So this is something that we were calling, right?

So we'll hit on send.

And you can see that we got a request, right?

So everything is working completely fine.

Now, what I will do is and I will stop the order service application.

So that means now, whenever I am going to call my API gateway from API Gateway, it's going to call

the order service.

And my order service is down.

So if I hit send now, you can see that I'll be getting that order.

Service is down.

OC.

Cool, right?

So you can see that my circuit breaker is working.

It is opening the circuit, and I'm getting the fallback method that we have defined that order services

down.

Cool, right.




Transcript - 
Now let's implement this circuit breaker in the order service as well.

So I'll go to the order service and if you go to the permit XML file.

Here.

We need to add the dependencies.

Right.

And yet what we'll do, we'll go to the again browser.

We'll go to the spring initialized and we will copy this dependency.

Here in this project, we are not using reactors, so we'll just add the base default dependency.

We'll go to the order service here.

This is the XML file for the order service.

Right from the order service.

And here we will add the dependency of the circuit breaker that is resilience for now as we've added

the dependency.

What we will do is we will go to our fain clients because from the order service we are using the Fain

clients to call our external services.

So here what we can do is within this payment service and the product service here we have added the

client.

So here we can add our circuit breaker.

So whenever we are doing the API call outside, we can have the circuit breaker implemented for that.

So here I will just add the circuit breaker here, circuit breaker from the resilience forge as the

annotation.

And here I need to give the name of the circuit breaker.

So I'm just giving that external as the name.

And what I can do is I can also give the fallback method that needs to be called.

So let me just give the fallback method as well.

A fallback method and I can give the name.

So I'm just giving the fallback here as the name, as the fallback.

Now, this particular fallback method I can create, so let me just create this here.

So as this is the interface, we can use the Java AIDS feature for creating the default methods here.

So let me just create a default method and I'll give you the same name here.

And from here I will take the exception as the injection here and I will just throw new custom exception

from here.

This is unavailable.

This is going to payment service, right?

So I'll just define payment services not available and it's going to be 500 error code.

So we just define a fallback method for our circuit breaker.

Same thing we can implement for our product service.

And I will just copy from the payment service the default method here.

I'll go to the product service.

And I will just add the default method here and I'll just define product.

Service is not available spelling mistakes.

I'll just correct it.

Now, for this external circuit breaker we have added, we need to add the configuration like for this

action, what configuration should be.

So for that, in the earlier example, we went ahead with the Java based configuration where we added

the default configuration using the bean.

Now let's try to add the same configuration using the YAML file as well.

So here we will go to the application root YAML file.

This is our order service and here we can add the configuration.

So here we can add the resilience for configuration and this is for the circuit breaker.

So we'll have the circuit breaker and we're going to add the instances.

And the instances is external because we added the external, right?

So within this external, yeah, we can add the different configurations now.

So I will just add event consumer buffer size.

This is going to be ten.

So my buffer would be ten.

Then I'll just add a failure threshold.

This is going to be 50% and minimum number of calls allowed is five.

Then automatic transition from open to half open state, half on enable.

So I'll just make sure this is true.

So that means from open to half open it will go automatically wait duration in open set.

How much time I should be waiting.

So I'll just define this as 5 seconds then I'm just defining permit number of calls and half open.

How many call I should permit.

Suppose three calls.

Now what is my sliding window size?

Suppose it's ten.

And what type of sliding window?

I want to use that Also I can define I can say this is count based.

Okay.

So you can see that these are just the basic configuration that we have added, like what should be

the minimum number of call, what wait duration and all those information.

So this is going to be a count based window.

So it's going to be the ten size.

And from that ten size, it will try to calculate what is happening for that circuit breaker OC.



Transcript - 
Now, as we have implemented the circuit breaker in our API gateway, let's implement the rate limiter

as well, because rate limiter is also very important thing that we need to implement in our microservices,

in our API gateway because we need to avoid the DDoS attacks as well.

We don't want our APIs to be vulnerable.

So what we will do is we will implement a rate limiter in which we will implement how many number of

requests are allowed per user per second.

So that way we can have a proper API so that for each and every API, there has to be a maximum number

allowed in that number.

Only all the requests will be there.

So what we are going to do is we are going to implement this rate limiter using the Resilience four

J plus Radice.

So if you don't know what radius radius is, the in-memory database.

So we are going to add Redis in our functionality.

And with this radius and resilience, we are going to handle the rate limiting functionality for our

APIs.

Now for that, what we have to do is we have to add the radius in our application stack as well.

So to add the red is what we will do.

We will use the Docker as we use for the Open Zipkin, because Docker is the best and easiest way to

add any of the third party application.

We do not have to worry about deploying and all those stuffs with just a command we can start our application

easily.

So what we will do is we will add the radius.

So let me just go and search.

Okay.

What I'll do, I will just add this Docker image.

Okay.

We will just go to the official image here.

This is the official docker image for this.

And if you scroll down here, you will get a command to start.

You can see that this is the command, right?

So I'll just copy this command and I will go to the terminal.

Okay.

And I will pace this command.

Before that, I will just check my docker is running.

I just do Docker images and I'm getting the images.

So my Docker is running because see that I already have a this image, but that's fine.

We will use the new command.

So this is our command that is Docker run hyphen for name some name.

So let me just go, let us try this hyphen dx readies and after before that what I will do is I need

to give the port information by default.

Right.

It runs on port 6379, so I'll just expose that port.

So I'll just define hyphen p6379 colon 6379.

You can see that I have just exposed the port to my host machine using radius here.

Once I run this command, my red is started.

If I check with Docker command, you can see that my Redis is working and we have the access to the

6379 port information.

Now, once it's done, we need to add the dependency of Fridays in our project as well.

So let's do that now for this.

What we will do, we will go again to our IntelliJ Idea will go to the Cloud Gateway here and within

this cloud Gateway will go to the XML file.

And here we need to add the dependency of Redis and make sure that as we are using the reactor plug

in your reactor project, we need to add the red is reactor.

So I am just adding the dependency here that is springboard starter data is and reactive because we

our project is reactive project.

Once we are this refresh our project.

Okay.

And my project is refreshed.

Now we need to add one configuration sorry to configuration one to enable the rate limiter for all the

projects and one to add for which particular key it should store the data.

Currently, we do not have any information about a particular users that we are going to implement after

the spring security, so we'll keep that aside for that, but we will add the basic configuration.

So for each and every request there will be one key.

And for that key we are going to handle it.

So what I will do, I will go to the Cloud Gateway application.

And here I will add the key information.

So here I will add a key resolver.

And you can see that key resolver, user key resolver.

I'm just giving the name here, and this is my bin.

And I'm just going to do return exchange calls to mono dot just And yet I'm just giving the one data

information that is going to be user key.

So currently what will happen is it will try to store all the data in one key that is the user key.

So it will just create a key user key and against that it will store the information in their address.

But later what we will do is we will add the user information.

So for particular user ID, it will store the data.

So if I have 10,000 users, so for all the 10,000 users, it will maintain the number of requests.

Currently, it is only maintaining for one user key.

Once we add this information, we have to go to our application YAML, file in the resources and yeah,

we need to add the filter information as we have added for the circuit breaker.

So here in the order service after the circuit breaker, I can add one more name and here I can add

request rate limiter.

This is the new filter that I'm adding.

And for this I can add the arguments and here I can add this rate limiter dot.

Replenish rate.

I'm just giving one year and readies rate limiter burst capacity equals to one.

So here I'm giving two information for the request rate limiter the same thing I need to copy.

For all the things.

So this is going to be my.

For the payment service.

This is a request rate limiter, the same thing I need to do for the.

Product service as well.

So you can see that I've just added a simple.

Rate limiter and I've added the replenish rate as one and burst capacity as one.

So now what is replenished rate and burst capacity.

So replenish rate means how many requests should be allowed per second?

I've given one request.

That means one request should be allowed per second.

And what is burst capacity?

Burst capacity means how many requests should be allowed in that particular.

One second.

Now, to understand this better, let me just show you the example.

What I mean by replenish rate and burst capacity replenished rate means.

Suppose you how suppose one second duration year and you have one second duration again.

Then you have one second duration again, this is all one second duration and what with replenished

rate we have defined is within that one request.

Suppose 1000 requests are allowed.

So this is something that we have defined using the replenish rate like for each and every second 1000

requests is allowed.

So within this one second 1000 request is allowed.

Within this one second 1000 requests is allowed and within this second 1000 requests are allowed.

But what if.

Within that one second.

All the thousand requests come at this particular peak itself.

So suppose 0.8 second pass and only 0.2 seconds remaining.

So in that path you got a lot of requests.

That is 1000 requests.

So that means within that one second duration time, you didn't get all the thousand requests, but

in the last part itself you got 1000 requests.

So you can see within a very, very short period of time you got a huge amount of requests.

So with that, your application might not be able to handle all those requests.

So what you do is you give the burst capacity and within that burst capacity you define that, okay,

Within the burst capacity, you are just allowing only 200 requests within the burst or what you can

say this entire.

Worst capacity should be that within that particular second.

So rather than this particular interval, you will get this long one second interval.

And within that you can define okay, 1000 request time allowing within that time.

So that's a particular difference.

Like what is the replenished rate like means how many requests you are allowing per second and what

is the burst capacity like in a particular second duration, how many requests you are allowing.

So that particular two things that we have defined here now we will restart our application and we will

see how many requests that we are allowing.

So one request should be allowed and if I'm adding the other request, I'm doing another request.

It should not be allowed.

So let me just restart my Cloud Gateway application and check this.

So my application is started.

Now let's go to the postman.

Let's go to the gag orders here and if I hit send.

You can see that I'm getting the order here.

If I hit again, send.

I'm getting the order.

So within a second, I'm getting the order.

Now suppose within a second I'm hitting multiple times.

One time, second time.

And here you can see that I got the error with 429.

That is too many requests.

So that means you did a more request than allowed for a given interval of time.

So you can see that how easily we were able to handle the rate limiter.

We just added the configuration and the radius dependency and everything was handled for us automatically.

So this way you can implement rate limited to prevent the DDoS attack in your microservices.





Transcript - 

Let's talk about security.

So now in this project, we are going to implement the spring security with OCTA or two.

So first of all, before we implement security, we need to understand what we are going to implement.

Earlier, you have seen that we had created multiple microservices and with all those multiple microservices,

currently we are able to directly access it.

But there has to be way that all the microservices are secure behind a particular authentication gateway.

So all the requests which comes to the application are to be authenticated and authorized.

Then only those particular requests should be allowed.

So in this tutorial we are going to implement spring security plus or two authentication.

So let's first understand what is odd to previously, whenever we were developing the monolithic application,

we just used to add the spring security and we are just doing this session based authentication and

for that particular user session was created and within that session interval user was allowed to do

some of the operations.

Now when we are developing the microservices architecture to create a session for each and different

services is not possible.

So what we do is we go ahead towards the auth to authentication.

Now what ought to gives us all to gives us the authentication based on the authentication code, So

it gives the authentication code.

So now that has to be the authentication server which will take the request.

That request will have the credential details and this authentication server will return the odd code.

Now this authentication code will have a few of the information like, okay, the credentials are valid.

It will try to check if the credentials are valid and everything and it will have all the information

of the authorization as well.

Like this particular request is authenticated.

And now for the authorization, what it has supposed, if I have multiple roles available, then all

the roles information it will have and the different things that we will go through in detail, like

what All the things it will have.

But ideally there will be an authentication server and it will return the authentication code, and

that authentication code will be the JWT token.

What is JWT?

It is a JSON web token.

This JSON web token will consist of three things that will be the header information, the body and

the signature.

So this JWT token, whatever is created will be unique.

And the other thing is whatever the authentication code is created, write whatever the auth code is

created, that auth code will have the code itself and the duration as well, like for which particular

duration that code is valid.

After that we need to generate a new code and that code has to be passed.

Now when we are generating the authentication code, it is a client's responsibility to send this code

for each and every request.

Okay.

Now, whenever the client is sending the request with this odd code, these services will handle those

authentication code.

And and with that authentication code, it will try to get the particular information.

It will try to decode this JWT and it will get the information.

Okay.

Like this particular request is authenticated and authorize it has all this permission.

Suppose if I'm doing the other request, this particular request has the customer role.

So all the APIs with the customer role only should be allowed for this particular user.

So those type of validation we will do using the spring security in order to OC now as the authentication

server.

To give the authentication and everything, we are going to use the OCTA core.

OCTA is a third party authentication library.

So what octa will do is OCTA will give the library.

So we are going to use that library and it will give us all the order to implementation.

So we just have to add the library and we are just going to tell what is the issuer and everything.

So it will give the authentication server and the resource server.

So we just have to use those things.

Otherwise we have to implement by ourselves.

So always make sure that we are using whatever the libraries are available rather than reinventing the

wheel.

Now how this entire thing flows, right?

So whenever we are doing any request, what ideally it tells auth to is like Auth to tell us that I

am going to do a request on behalf of a user.

On behalf of the user, I'm going to do the request internally.

So that means whenever we are doing the request for the first time, what it will do is it will try

to take the consent from us.

You might have logged in with multiple third party application like login with Google, login with Facebook,

log in with GitHub and everything.

Whenever you do that, you will get a consent form that this particular application is trying to log

in with Google and it will use all this information.

There will be different scopes available and it will be displayed like it is trying to use the email

scope, it is trying to use your username and all those information.

So whatever you allow, those particular scopes will be allowed and based on those scope, it will try

to do the request.

So it's as simple as it is.

You don't have to worry about the nitty gritty, but this is the general idea that how it will work,

what we are going to do is we are going to implement one of the authentication APIs.

Let me just show you that as well.

So you can see that these are the microservices that we have created and we have the Okta authentication

server as well.

Now within this API gateway, we are going to create one API that is going to be the login API.

So within that login API, we are going to pass our credentials and with that credentials, this API

gateway will try to hit the Okta authentication and it will authenticate our request.

So whenever we give the credentials in return, Okta will authenticate that particular user.

Okay, this particular user is available within the Okta and this particular user is having this all

scopes, these all permissions and all those permissions wrapped together.

It will return the JWT authentication token back to the API Gateway.

So now API Gateway has that particular token.

So we got the token.

Now that token ideally will be sent back to the client.

Now, clients is responsible to send that token as the authentication bearer token with all the requests.

Now it's going to do so now whenever we are going to send another request to do the order or to place

the order or to get the order information, we are going to send that request.

Within that request, in the header information, we are going to send that token which was generated

by the Okta at the time of login.

So we are going to send the request within the API gateway.

It will get the token and it will send.

Okay, we have the token available and this token is having this request and that token will be passed

as a header information to the calling service.

So from the API gateway, suppose it's calling the order service, so that token alongside the request

will pass through the order service as well.

Now within this order service it will decode the token and it will have all those information.

So when the token is decoded here, it will have the information.

Okay, so this authentication code, whatever we return, is authenticated and authorized.

And this has all this permission.

Suppose it has a customer permission.

Suppose it has an admin permission.

Now, based on that permission, it will also have the different scopes available like this particular

token can get information.

Like what information can it get?

So based on all this information, it will try to get the relevant information from the different service.

And whenever we are doing that, the order service will send those token as well along the other services.

That's what it will do.

So every time, whenever we are doing the request, it will send the information.

Now one thing also we are going to implement is whenever I am calling the product service or the payment

service, suppose I'm trying to do the payment service.

So this payment service will only be called when we do the order directly.

I cannot do the payment.

So this payment service will have a limitation like this.

Payment service can only be call with the internal scope.

It cannot be called with the other scopes.

So whenever the request will come from the order service to the payment service as the scope mentioned

as internal, then only payment service will allow that request rest.

All the requests will not be allowed the same way whenever the request come here in the order service

and from the order service the request is for the admin and whenever we are doing the request for the

product service in that product service it will identify okay, this request has what scope that had

been our customer.

So based on that it will identify the scope and it will send the result.

If scope is matching, then it will send the response otherwise if scope is not matching.

Then it will send the request back.

It is the unauthorized request.

We cannot give you information.

So you can see that all this information are being handled using the authentication code and no session

is maintained anywhere.

So the whole point of understanding each and everything is to understand how authentication token works,

how the authentication code and how the all to mechanism works.

So we are just going to do one login API.

That login API will take the credentials, it will give the authentication token refresh token and what

time that token is valid.

And for a particular user, what is the permission that it has?

Once we get this information, we will send all those requests with that authentication code internally

and internally, we are going to implement different APIs with the different roles.

So this is going to be a really awesome tutorial.

When we are implementing everything, you will understand a lot about security and Okta authentication,

like how everything works together.

So now, without wasting any time, let's go ahead and implement our spring security.






Transcript - 
Now as we are going to use the octet as our authentication provider, we have to register our application

in Okta.

So for that, what we have to do is we have to go to the Okta developers and create the account.

So go to the developers, search for the developers in your browser and you will be going to the Okta

developer home your.

Once you go there.

Sign up here.

You can use any of the already existing provided to login.

I'll just use the continue with Google to login.

Once you come here, select the region.

I'm going to select India as my region.

And I'm going to click on continue.

Once you log in, you will get your domain information.

This is going to be your workspace.

That is day.

Whatever it is, this is for mine.

For you, it might be a different number altogether, but you will get your account ready and you will

be able to traverse through each and every information.

Once you do that, once you are on this page, this is going to be your home dashboard page.

Once you are here, what you can do is you have to register your application.

So for that, what we have to do, we have to go to our application here.

And within this application we have to go to the applications tab and here we are going to create the

application, we are going to create on the Create app integration here and we are going to select the

Odyssey Open Connect and we are going to use the application type as web application.

Once we do this, click on the next.

And yet you will get the new Web app integration page here and here you have to give the name of your

application.

I'm giving microservices as the name.

You can give any name here.

You can add your logo, information and everything.

After that.

You need to give the grant type.

In the grant type, I am going to select the client credentials and I am going to take the refresh token

as well.

Once that is done, you need to give the sign in redirect URL.

So if you just search your.

Oh.

To sign in.

Redirect URL.

You will get this earlier.

In the documentation itself, you will get all this information.

So if you scroll down here, you can see that this is going to be your you are also just copy this URL.

I will make sure that I will add this URL in the discussion.

So copy this URL and go here and add this earlier.

Now everything is fine for now, but my application is running on port 1990 because if you go here,

right, my Cloud Gateway is running on Port 99.

So I need to give port 9090 rather than added ock and slash login slash auth to slash code slash OCTA.

This is the default URL based URL and for the sign out redirect url as well.

You need to give the same.

That is 1990 because my port is 9090.

Once I add all this information I just need to save.

I will just keep this for now.

That is the control access.

And click on sale.

Once you are done, your application will be created and you will get your client ID and the client

secret.

Make sure that you copy all this information and don't share it with anyone.

Because with this credentials you are going to connect with your Okta accounts.

You are able to see this information from my account because after this tutorial I'm going to delete

this.

So this is going to be your client ID and your client secrets.

And if you scroll down, all this information that we have added are coming here, you can see that

sign in URL, sign out URL and all those information.

Now, once this is done, what you have to do is you have to go to the securities.

In this securities, you have to go to the API.

Within this API, you will see this information that is the authorization server because all your request

is going to be authorized against the authorization server.

So this is our authorization server and the this is the default authorization server, this is the audience

and this is the URL.

So this all information we are going to use.

So earlier we got the client decline secret and the audience and the URL.

This for information we are going to use in our application to connect to the Okta OC.

Once this is done, click on the default here.

Once you are here, you can see you are getting all those information, right, your audience or you.

Here you are.

And everything after that, you have to go to your scopes.

And within this scopes, you can see there is a lot of scopes available.

Based on all these scopes, you will get information.

Suppose if you want to signal that this is the open ended request, that means you also have to do the

authentication.

We will give the open ed scope.

If you want the profile information, we will give the profile scope.

If you want the email right, you will give the email scope.

So based on all these different scopes, we will get the information so you can see that all these informations

are there.

So if any of the scopes that we want to use that are defined here, we can use the scopes.

And if you want to define any extra scopes, we can define here as well.

So one scope I will define here, that is the internal scope, because what I want to do is I want to

do the internal communication between the different micro services.

So from the order services to the product service from the order services to the payment service, these

are the internal calls.

So from one micro service to the another, micro services.

So to make that internal call, I will define that internal scope and I will also make sure that internal

scope is defined within the authorization server.

So whenever I'm doing any request, authorization server will know that okay, this is the internal

scope and it is defined with us as well.

So I'll click here and I will name the scope as the internal here.

Okay, so internal scope is defined here.

Rest all the information I'm just keeping as it is.

So you can see that new scope is added.

Now if you go to claims.

Yet within the access claim, you can see that you are getting the claim type what you are going to

claim.

So what I'm trying to do is I am trying to claim the group information because groups are something

rules.

So before we go ahead and do any information, what I'm trying to do is I'm trying to show you the groups

as well.

So to show the groups like what are the different roles that you are allowing in your application,

you need to define those groups also first.

So I will go to the directory and groups here.

So I'm just opening the new tab here.

Okay, I'll go to that tab.

And here you can see that I have defined these two roles.

That is customer group and the admin group.

Everyone group will be by default available.

But these two groups are defined.

Now how to create that group.

Just add the group here.

You just need to add the name of the group and the description that you want to give.

That's it.

So you can see two groups I would define customer and admin.

So in my application that can be an admin as well.

There can be a customer as well.

So whoever are doing the orders and and all those things, those are going to be the customer and admin

will be like admin of the application who can add the product information and everything.

If you want multiple, you can add multiple as well.

Once you do that, you need to have the users as well.

Now considering in our application that users are going to be added from the user services to the Okta

or by the default registration as well, because whenever you are going to do the call by default,

the login page will be there and you can sign up there as well.

So in the people section, if you come here.

You can see that there are different peoples available.

This is my default account and this is the account that I've created.

And this is also Nicole Gupta, another account I've created.

Now, to create the that is a simple thing.

You can just add a person here.

You can add all the information, what type of user it is, the first name, last name, username,

primary email.

The group's information, like this particular user belongs to which group?

Suppose I want this user to be in the customer group.

Then I can select this customer and I can add this customer.

I also want this as the admin, so I can add admin as well, so a particular user can be in the multiple

groups.

So once I add this, this particular user will have the access to all the activities which admin can

do and all the activities which customer can also do.

So once you add all this information, you can do what you want to activate now that user or you want

to activate later.

So once you add all this information, email will be triggered to that particular user with their credentials.

And once they login, they have to reset their password and everything.

That's a standard process.

Once that is done, once the users are created, once the groups are created, you want your application

to understand those groups as well.

So for that we need to add the groups in the claims that whenever I'm getting my data right, whenever

I'm getting the authentication code, I need those groups information also added.

So what I will do, I will create a new claim and within this claim I will name this as groups.

Okay.

And this group should be included in the access token.

And the value of that group should be groups.

And the filter should be regex.

And I'm doing dot star.

That means all the groups I want.

So whatever the groups are created that groups I will get.

So let me just create this claim and this claim is added.

So once this is added, your configuration for the Okta is done.

Now you need to add the dependencies in your project and configure.

So now let's go ahead and do that.





Transcript - 
So what I will do, I will go to the IntelliJ Idea, and within our Cloud Gateway we will add the configuration.

Now, before adding any configuration, we need to add the dependencies as well for the Spring security

and Okta.

So if I go to the browser and if I go to the spring initial user.

And here I can add this spring security.

So I'll just search for the security and I will add the spring security and I will add the occupier.

So you can see two dependencies I'm adding here.

So if I just explore here.

So I can copy this to securities.

That is the springboard starter Security and Okta springboard starter.

So let me just copy this to dependencies and add in my application.

So I'll go to the IntelliJ idea.

I'll go to Cloud Gateway and form XML file.

And after the reddish dependency that we had added, I can add this to dependency.

That is the spring start of security and the OCTA and I will load my project again.

Now, once you've added this and if you restart your project.

Everything will freak out.

Okay.

If you go now, your application is started.

If you go now to Postman and if you do the same request.

You can see that nothing is happening.

It is just trying to authenticate a request.

And you can see that you are getting 401 unauthorized, because whatever the request we did, it is

not authorized.

So currently, whatever we did currently, it's not able to understand where to go for the authorization

and everything.

We have to add all those information.

So let's add those information.

So we will go to the application of the YAML file and here we will add the configuration.

For the octet.

So we'll just add the octa.

Within the octet we will add or two, and yet we'll add the issuer.

What is the issuer?

Then we'll add the default.

Which is audience.

Then we are going to add the client ID.

Then we are going to add the client secret.

Then we are going to add the scopes.

This is all information we need.

So we will go to our browser, will go here, and we will add the information so audience will copy.

This is going to be your.

Audience.

Okay.

And your issuer.

So issuer is this one.

So I'll just copy this information.

And this is your issuer.

Okay.

Then client I.D. and secret.

So for the client ID and client secrets, you will go to the.

Security is the application and in the.

Application, you will go to the applications and this is your microservice.

So just click on that and here you will get your client in secret.

So copy your client ID and add your.

And copy the client secret and adhere.

Don't share this information with anyone.

Now for the scoops you need to add which particular scopes you need.

So here's what I need is I need the open IDE scope.

I need the pro file scope, I need the email scope, and I need the offline access scope, all the scope

information you will get from your scope.

So if you go to your API.

Sorry, security in the security API.

And if you click on default here, if you click on scopes here, you will get all the scopes.

So what we define, we define the open IDE scope.

Then we define the profile scope, view your profile information open and E for open ID signals that

a request is an open ended request profile means to extract data that is based on what profile information

you have provided, email ID for your email address, and then we have added the offline access keep

you sign in to the app.

This we are adding to get the refresh token.

With that refresh token, we can get a new token when the particular access token is expired.

So this information we need.

So we have provided all this information.

So now my application knows that where I need to go to get all this information to authorize and authenticate

my users.

Now I also need to tell my spring application that all the request has to be authenticated, right?

Whatever comes here.

So for that we need to add the configuration.

So let's add it.

What I will do, I will just create a new package, will name security, and here I will just add a

new Java class and I'll just say Okta or oath to.

Web security.

This is just a class that I'm declaring here OC Okta or to Web security, and that is a spelling mistake.

So let me just refactor this.

EU Web Security OC Oct two.

This class is going to be configuration class, right?

So I'll just define this is the configuration and I'm going to enable web security.

Ideally, this particular class is the reactive application, right?

This particular cloud gateway.

So I need to enable web flux security.

Now, once this is done, once we have declared this configuration, we need to override the configuration

using the filters for this HTTP, we can use the security filter chain and to pass the information.

So what I will do, I will just create a method to override those information.

So let's declare public which will return security Filter chain.

And this is going to be the bean.

And what I'm going to override so that I can inject here, so I can override server HTTP security.

And this is going to be HTTP as the name.

Look here.

I can override this server state security.

So let's define what we need to override.

So I'll just define http dot authorized exchange.

Dot.

Any exchange should be authenticated, dot and.

OAuth two login.

Okay or to login, so login page should be added.

So this is going to be at the default login page of the octet, dot and dot or two resource server or

resource server, and that should be giving the JWT for us.

So this is just a basic configuration that we have added that all of our requests should be authenticated

and if not, there should be a default login page.

And for our resource servers we should be accessing the JWT.

And after that we can just return HTTP dot build and it is throwing the error that we should be using

security web filter chain and yeah, correct.

So we are going to add security web filter chain.

Let me just clear the imports here and you can see that your class is created.

Now, what we'll do is our authentication web security filter is done.

Now what we are going to do is we are going to implement one API.

With that, we are going to call our OCTA.

So with that octa, we can authorize and authenticate our user.

So for that, let's go ahead and create the API and we are going to implement what we need.

So let's do that.






Transcript - 
So now let's create a rest API endpoint to allow the authentication for a user.

So when we are going to hit that particular URL that we're going to create that is slash, authenticate,

slash login, we are going to be redirected to the Okta authentication to enter the credentials and

it will return the authentication code for us.

So let me just create a new controller here.

In the controller package itself.

Let me just create a class and I'll just say authentication.

Controller and this authentication controller will integrate with added rest controller.

And we're going to map a particular request here.

So with the request mapping annotation, we are going to map to slash authenticate.

And within this, we'll create one method to handle one login API.

So we're going to add a gate mapping here and this gate mapping will be slash login.

This is going to handle the slash logging for us.

After that, let's create a method that is public response entity and what type of object we need to

send.

So we're going to create a model in which we are going to take some of the fields that we want and we

are going to send back.

So for that, what I'm going to do is I am going to create a model here.

So let me just create a package model.

And within this package model, let's create one class and let me just create a class as.

Authentication response.

And this is going to how private string user ID private string access token.

Private string refresh token.

Private long.

Expires at at what time?

It's the token is going to be expired on that thing and list of collection of string as the auth list.

So I'm just adding the authority list here.

So this will have all the authority information, like what type of authority that particular user is

having.

It is a customer, it is admin or all the other, whatever it is that all information that I want and

I will make this class as at the rate data annotation, I will add the other data annotation here so

I will get the getter setters, I will add the all args constructor to get argument with all the parameters

and the rate.

No args constructor to get a default constructor and add the builder annotation to get me a builder

design pattern for this class.

So you can see that my entire class is created.

Now.

I can use this class as a response object in my controller, so I'll go to the authentication controller

and here I will be using this as a response and I'm going to use the login method.

Now, within this login method, I'm going to inject a few of the things, so I'm going to inject a

DC user.

Okay.

And I'm going to insert the model here.

And then I'm going to add auth to authorization client.

These are the information that I need.

Now I will add the annotation so that I will get a particular data out of it.

Now this is a user is my principal object.

So I will add the annotation authentication principle.

And this particular client, this is going to be my.

Registered or to authorisation client which client it is?

Octet Client.

This is the information that I'm giving you.

Hope everything makes sense now from here that I'm just injecting the user, the model, and the oath

to authorization client.

Now here I can use our code.

I can create the code here.

So what information I need here, I need the information as authentication response that I need to send

back.

Right.

So authentication response, authentication response equals to authentication response, dot builder

dot build pattern because we are going to build using the builder pattern and we will get all the information

from the DC user and the client itself.

So we are going to build this object from that.

So we'll get user ID from the DC user ID, so I'll just do DC not get email because the email would

be its user ID, OC dot access token.

We'll get the access token from the client.

So client get access token get token value.

Then refresh token refresh token will also get from the client.

So client dot get refresh token dot get token value that expires.

Add that also will get from the client client dot get access token dot get expires at dot get epoch

seconds.

Then we are going to get the authority list.

So for that dot authority list, we'll get this from the DC user.

So it's a user dot get auth list authorities.

Dot stream dot map.

Ground authorities dot get authorities dot collectors dot to list.

So you can see that I have taken up all the authorities as well from here.

So here you can see that my entire object is created.

Now I can send this response back, so from here I can return.

New response entity.

And I'm just going to pass on the response comma HTTP status dot.

Okay.

Okay, so this is going to be my written value.

So you can see that we are taking the user as an input here and then the model and the client information.

And with this, with the help of this, I am building my object that is authentication response and

I'm sending back all this information we will get once we are logging in.

Now let's restart our application and test this out.

So let me just restart the application here.

Okay, My application has started.

Now what I will do, I need to log in here.

That is localhost 9090 slash, authenticate, slash login.

So let me go to the browser and let me just open the incognito mode and I will just call localhost 1990

slash, authenticate, slash login.

And here you can see that you are redirected back to the Okta sign in page.

Now here you can add the login credentials.

So if I go to my Okta here and if I go here, my user ID is this one shell one two 300 millimeter dot

com.

So I'll be using that and I will add my credentials.

One side, click on, sign in.

Okay.

Seems like my credentials are invalid, so let me just retry again by setting up the credentials.

Let me just close the browser and try it again.

I'll go to local host.

Calling 9090 slash authenticate, slash login and you're getting this page.

I just copy this primary email ID.

Let me just use this email ID and password.

And you can see that it is signing in.

And you can see that we got our authentication token, we got our user ID, we got the access token

and all those information, whatever we needed.

So you can see that we were able to implement the Okta authentication like a login page where whenever

we will try to hit our login page, we will redirect it back to the authentication from the OCTA.

When we add our credentials, it will be authenticated, authorized, and we will get the authentication

to token, refresh, token and our all the scopes information.

So we are getting our user ID, we are getting access token, we are getting a refresh token, we are

getting all our roles as well, Scopes, information and everything on authority list we are getting

and the scope information.

So all the information we are getting now, we can use this information and we can call the different

APIs.






Transcript - 
Now we have to implement in the order service, product service and the payment service as well so we

can leverage this token information.

So let's go ahead and implement the security in our order service.

So we'll go to the IntelliJ Idea and let's go to the order service.

And here we need to add our dependencies.

So in the order service, let's go to the XML file.

So for this, what we will do is we will go to again browser, we'll go to spring initialized and we

had added to dependencies that was spring security and Okta.

But here we will need one more extra dependency that is octet client or client.

So these three dependencies will need.

So let me just explore the project and copy those dependencies.

So that is auth to client spring security and the OCTA.

So this all dependencies I'm copying and let me go to the initial idea.

This is the order service pamphlet XML file, and let's add the dependencies.

Once you add the dependencies, rebuild your project.

And the project is rebuild now.

Now we need to add the configurations.

Now, once we do this, we need to add the basic configuration in our config server as well.

So the conflict server is using our GitHub, right?

So in that GitHub we need to add a basic configuration.

So let me just show you that what we need to add.

So if you go to the browser and go to the GitHub and go to your spring app config and go to the application

YAML file, this is the extra thing that you need to add that is the OCTA or two.

And in that you need to add your issuer URL and the audience because this will be the basic information

that you need to add in all your services.

Okay, so this three line of code you need to add in your application YAML file and we will not add

again in the all the other services.

We will just add the other configuration that has been remaining.

So just come here in your application yaml file and add this information.

Don't use directly this project.

You can fork this project and you can change the information according to the issuer URL.

So you need to use your issuer URL.

If you're using my issuer URL, this will not work because I will be deleting this project.

So once you add this information, go back to the IntelliJ idea.

And here we need to add the configuration in our application or the YAML file.

So let's go to the application file of the order service and yes, we need to add the configuration

so that order service will understand what we have to do.

So here in this order service, we need to give the information about the client registration as well.

So this is going to be registered as a client which will behave like this, will behave as a client,

which will call the other services.

Right.

Because from the other service we are going to call the other services like product service and the

payment service.

So with that particular client information and with a particular scope, it can be able to get the data

from the other service.

So it will have that permission to do that.

So we are going to register this as a client and will give all those permissions to do that.

So we'll come here in the spring security.

We are going to add all those configurations.

So here after the config, we'll just add the security.

So spring security all to resource server in the resource server will give the JWT.

And in this JWT, we are going to add the issuer URI and this issuer URI will be the same as what we

added earlier.

So if we go again to the cloud Gateway in the Cloud Gateway, whatever we had given the issuer, you

are right, the same thing we need to add here.

Once you are this sure you are right, let's add the other information.

Now let's register the client and the client will add the registration details and we will give the

name here.

What is the name of the client?

So I'll give the name as an internal client.

You can give any name here as I'm going to do the internal request.

I'm just registering this as an internal client and here you can give all the other information.

So here we'll give what is the provider?

Provider is Okta.

And what is the authorization grant type.

So here we will give its a client credentials.

Look, there are different types of authorization, but in that we are using the client credentials

for this, registering as a client and using the client credentials to do the request to get the access

code and do the request to the other clients.

And we'll define the scope what scope it will use.

So let me just define that.

It is going to use the internal scope.

And for this as well, we are going to add the client ID and the client secret.

Okay, So these two things I'm going to copy again from here.

This is going to my client ID.

And this is going to be my client secret.

And for this client who is the provider.

So that also information also we will add is going to be Okta provider.

And what is the issue where you are?

I so assure you or I would be the again same issue where you are.

Right.

So let me just copy this.

An idea.

So all the information is completely added now.

Now, once this configuration is added, we need to add the configuration for the web security as well,

like how we did Web security and we implemented security web change filter right in our config server.

Right?

We have added the security package here and in that we have added the OCTA to Web security and in that

we have added the security web filter chain.

Same way we need to add in our order service as well.

So what we will do is we will create a package here, the security.

And we will add all those information here as well.

So within this security, let me just add a class here.

I'll just add the class.

Web security config.

And this class is going to be annotated with a great configuration, and I'm going to enable Web security.

Enable Web security.

One more thing I'm going to enable is method security.

So enable global method security, which means that I can add most of the configurations on the method

level itself.

And here I am going to enable pre post enable equals to two.

Now here we need to override our security web filter chain.

So let me just define that public.

Security web filter chain.

And this is going to be my been and this is going to take HTTP security as HTTP.

And I can add the HTTP configuration.

So I'll just add HTTP dot authorize requests.

So what I'm going to do here is I'm going to do authorize, request, authorize request dot.

Any request should be authenticated oc dot auth to resource server.

I'm just going to part all what two resource over configure.

Colin, Colin, JWT, JWT.

We are going to get the JWT and it is asking to add the exception.

So let me just add the exception to the method signature.

So this is something that we have done.

It's very easy.

So you can see that what we are doing is we are just doing the authorized request for all the HTTP and

all the request.

All the requests are going to be authenticated and for the oath to resource server, we are going to

have the JWT.

Now, after that, we have to return HTTP dot build.

Okay, That's a simple thing.

So this is going to be only normal security filter gene.

So there are a lot of classes.

Sometimes you will get overwhelmed like which one to use.

But don't worry, all these configurations you will get from the documentation itself, it's nothing

complex.

You just have to grab those information and override it.

So we have simply just override the information here.

Now, you can see that we have added the configuration, that all the requests coming to the order is

going to be authenticated.

But from the order, we are again doing the call to the payment service and the product service using

the fin client and the rest template.

Right.

So if for that, if you see you will see the configuration here that in the external client we have

the configuration that we are doing, the fin client to the payment service and the product service

and we also have the configuration here.

So for that, what we have to do is we have to override those configurations as well.

Like whenever we are doing the request, right, whenever we are doing a request to payment service

and product service, we need to pass our tokens, right?

So we will override the configuration.

So with that, we will be allowed to pass on the information, whatever the token we are getting.

So let's go ahead and override those configurations as well.






Transcript - 

For that reason, we are going to add the interceptors and and those interceptors will handle all those

requests.

Like whenever I'm doing that request to the payment service and product service, it will add the whatever

the authentication token would be in that particular request itself.

We don't have to manually do it.

So for that, within the external package, we are going to create one intercept package.

So let me just add the intercept package and within this intercept package I am going to create a couple

of classes here.

So let me just create the first Java class that is the auth request interceptor.

This is to override the configuration for our fain client.

So first we'll complete the fin client and then we are going to complete our rest template as well.

So auth request interceptor and this is going to implement request interceptor.

You can see that this request interceptor is of the fin client.

Okay.

So once we do this, we need to override a method, sorry, implement a method.

So we are going to implement the supply method and this is going to be my configuration class.

So I am going to annotate with that configuration.

Now here's what we are going to apply that in the template itself.

We need to get the authentication token.

That authentication token we will get from the auth to client manager.

So we need to have the object of the client manager.

From that client manager.

We will get the token and we will pass that token as the authentication header as the authentication

barrier token.

So for that first we need the object of it.

So let me just define the object of oath to authorization authorized client manager and I'm going to

auto wire that.

Now with this template, I'm going to add in the header information.

Like what I am going to add.

The name would be authorization.

Okay.

And after that, I'm going to add as a bearer token be a IDR space.

And then I'm going to add plus or to authorize client manager, authorize or to authorize request dot

OC with client registration ID.

What is your client registration ID?

So if you go.

To the application that YAML file, right?

If you go to the application, the YAML file, this is the idea that you already find that is internal

client.

Right.

So this is something that you need to use here.

So let me just remove this extra information.

Dot We need to add the principle here.

Principle is nothing but your scope.

So what is going to be your scope?

Your scope is internal, right?

So I'll just copy this.

Internal.

Yea.

Not build.

And from here, I'm going to get the dot, get access token dot get token value.

So you can see that I have parsed the token, whatever token I got from your OC that I've added as a

better token in the authorization header, you can see that we added the configuration.

Now we are getting one of the error here.

You can see that the error is could not auto where no being of oath to authorize client manager phone.

So we need to handle this as well.

We need to define this bean as well because from the client manager only we will get this information.

So let's go ahead and define that as well.

So to define this or to authorize client manager, I will go to the order service application class

and here I'm going to define.

So let me just define your public or to authorize client manager.

Client manager.

Okay And this is going to be my been.

Now this client manager is going to inject two things.

That means it is going to inject the repositories which repositories or client registration repositories.

So let's define that client.

Registration repository.

And.

OT or to authorized client repository of these two things is going to inject and we are going to use

these two things here.

Now, yeah, we'll need the object of the order to authorize client provider.

This is the one or two authorized client provider.

We'll create the object of it and using the builder oath to authorized client provider, builder, dot

builder, dot build method.

And here we can pass some of the information's dot client credentials.

Now, from here we are going to return the default auth to authorized client manager.

So we'll just define the object that is default or to authorize client manager equals to new default

auth to authorize client manager.

And this is going to take the argument that is the client repository that is that is you can see that

it's going to take the client registration repository and the authorized or to authorize client repository.

We have both the information, right.

So we'll just pass this.

I'm going to apply pass client registration repository and or to authorized client repository, both

the information I passed here.

Once this is done, I'm going to pass out to authorized client manager, dot set, authorized client

provider.

So I'm going to set the client provider as ought to authorize client provider, whatever we have defined

earlier.

So to authorize client provider, we are going to add here.

So you can see that we just added the client provider and the client repository in order to authorize

repository everything to our default or to authorization client manager.

So your default client authorization manager is ready.

Now this is something that we have to return, so I'm just going to return.

Plant manager here.

So this client manager will have all the information and this client manager will be used.

Your interceptor, whatever the interceptor you have created here, you can see that there is gone.

Now, this interceptor will use this and we are going to get the access tokens from you.

Now, similarly, we need to implement for the rust template as well.

So let's implement that.

So here within the interceptor package, I'm going to create a new interceptor and I'm going to create

a class rust.

Template interceptor.

There's a spelling mistake.

Let's just refactor this.

MP ll a t.

And this is going to implement.

Client http.

Request interceptor.

And this We need to implement this method.

So let's implement this method.

And yet as well, we will need the object of a or to authorize client manager.

So let's just define that or to authorized client manager.

And I will create the constructor to bind this.

So let me just define public.

It has template interceptor which is going to take or to authorize client manager.

And I'm just passing the value.

This dot.

Or two authorized client manager equals to whatever the object that we have passed.

You can see that it's simple, nothing fancy.

So you can see that it's just a constructor which we have defined now in this method as well.

We need to override this request and we need to pass the access token as the authorization better header.

So let's do that.

So what I will do, I'll just request dot get headers, dot set headers.

Sorry, I need to add a new header.

Right.

So I'll just use the add header method in this.

I'll just add the information authorization and I'm going to add a banner space and then I need to add

the token token I'll get from the auth to authorize client manager dot authorize or to authorize request.

Not with client registration ID.

Client registration ID is closed to internal client.

OC.

Dot principal principal is going to be internal because that's the scope, internal, not build.

Dot.

Get access.

Token dot.

Get token value.

Look, you can see that we are getting the token value and now we are selling is a better token in our

headers.

And yet I'm going to return execution, not execute.

We are going to send the request and the body.

So you can see that your request for the rest template will send this information.

Now, this interceptor.

Right.

This interceptor, we need to register with our estimate as well.

So if you go to our.

Order through the application here.

We have defined the rest template.

Here it is.

Right.

So let me just create the object here.

This template template equals to new rest template and this is going to return this template here.

Yet we need to register that interceptor.

So rest template dot set interceptors arrays because this is ArrayList, right?

So I'm just going to use arrays dot as list new rest template interceptor.

Right.

This is the one that we've created and this is going to take authorization and this is going to take

this method on whatever method we have created, right?

Client manager And this client manager is going to take two things.

That is the client registration repository and or to authorize client repository.

These two things I don't have currently.

So let me just create the beans out of it.

So let me just.

Copy these two.

And let me just create the object here.

Right.

And I'm going to.

Or let me just make it a private.

This as well.

Private.

And this is auto wire.

And this is, well, auto wired.

These two objects are there now.

I can pass these two objects here.

Client registration repository and.

Was to authorize client repository.

So now you can see these two objects are created.

So let me just make it clean.

Cool.

You can see that it's very simple.

We just added the rest template and in that last template we have just added the interceptors.

So both interceptors are added.

This interceptor is added as a part of this template, whatever we created and the other interceptor

was added as a part of the FIN client.

We just added the auto configuration at the right configuration here, so both the interceptors will

be added now.

Okay, so now what are the request we are going to pass from here?

From the order service, it will send those requests with those particular headers to the relevant services

that is going to be your product service and the payment service.






Transcript - 

Now with the payment service and product service as well.

We need to implement this.

We need to implement the spring security.

So let's go ahead and do that as well.

Now, if you go to the payment service here in our project, this is in the payment service, go to

the XML file of the payment service and let's have the dependencies in here.

So in the dependencies, let's scroll down here.

After this, I'm going to add two dependencies.

That is the springboard Security and Okta springboard starter.

Okay.

This is the same dependency we added earlier.

I'm just copy pasting this to dependencies.

Now.

Once this two are added, we need to add the configuration for the security.

So as we have done earlier, we created the package and the web security config.

Let's do the same thing here.

Let's create a package that is security.

And within this security package, let's create the class that is.

Web security config.

And here we will add the configuration annotation and we will add the enable web security.

These two configuration that we have added here now within this web security, we need to add the configuration.

So that means we need to override our security filter chain.

So let's do that.

I will just create a method that is going to return security filter chain.

And this is going to be our bin.

And here it is going to take HTTP security.

And I'm just defining with HTTP.

So http dot authorize request, authorize requests.

We are going to use the lambda expression here and.

Authorized request, dot and measures.

And yet I'm defining that any request with slash payment.

Which has authority internal.

And whenever we are defining it authorities here, we need to mention this word scope underscore internal.

So all the request going to slash payment, which has scope internal, which is going to be any request

that has to be authenticated or to resource server or to resource server, configure call and call an

JWT.

And this is going to throw the error.

So let's add the exception to the signature.

And this is going to return HTTP dot build.

So you can see that if we added this core spelling of the scope is wrong, scope underscore internal.

Now you can see that this is quite a little bit different than what we have added in our order service.

So if we compare it with the order service, let me go to the order service.

And within the order service, if I open the.

Web security config.

Here you can see that we just directly define that any request should be authenticated.

We are not defined that which path needs to be authenticated because here we have added the enable global

method security.

Because here within this order service we are going to be authenticated as per the method level.

So we are going to define in the method level.

We will do it in the later part, but for each and every method, for each and every API, we are going

to define like which particular APIs needs to be allowed for which particular roles.

So that's why we have defined this way and we have enabled the global method security, but in the payments.

We have not done that in the payments.

We you are specifically define that for this particular path payment slash star.

If the authority is internal, scope is internal, then those requests should be authenticated.

So that's the difference that we have added here.

I added this to way differently so you can understand that how multiple ways we can do it.

So whenever you are searching for any of the implementations to do in your project, you might get differently.

Like some places it might have done with the global method security.

At some places with the end measures it would be given.

But you need to understand that you can do either of the things, but these are just the implementation

details.

The core logic should be the same.

So this way you just define all this information here.

So now the implementation in the payments is done.

Now let's go and add in the product service.




Transcript - 
So let's go to the product service.

This is my product service now.

And in the product service I need to go to the XML file.

And I need to add the configuration.

So let me add this configuration.

I will go to the browser.

I will go to the spring initialized and I will copy these to.

I just need to add the spring security and the OCTA.

So let me just copy this to.

And as a dependency here.

So once the dependency is added, we need to have our security filter chain implemented.

So for that we need to create the package and in that package we need to add a class.

So let me just create a package that is security.

And within the security package.

Let me create web security config.

Web security config.

Okay, This is going to be my configuration and I'm going to enable Web security and I'm also going

to enable global method security and pre post enable equals to true.

Now you can see that this is similar to what we did in the order service.

So if I go to the order service, right, I'll scroll up, I'll go to the order service.

And within the order service we have the similar file, right.

If I scroll down within the security web, security config is there.

So let me open this and ideally I can copy this bin.

So let me just copy this bin, go back to the product service and I can add this information.

So you can see that same thing we have added.

It's the same thing.

Now what we have to do is your payment service is done right because we did with the end matches there

and that we already defined that for this particular path and this particular role we are allowing here,

we are allowing everything.

So here we need to define as the method level.

So if you go here at a controller level, so you are I am at the product service.

So in the product service, if I go to the product controller here for each method, I can define what

role it has.

So at that time we need to allow.

So what I'm doing is here you can see that here it has a add product method.

So this ADD product is only done by the admins.

It should not be done by the customers ideally.

Right.

So I will add the configuration here and I will define as pre authorized.

So within this pre authorized, I can define that it should have has authority as admin.

So you can see that within this method I simply define that this method should only be allowed when

the authority is admin, so only admin can access this customer won't be able to access this.

Now get product by ID that also we can define now this get product by ID.

This should be allowed by the admin because admin can also see the product information right by the

id.

Then it should be also allowed by the customer because customer can also see this information and alongside

this should also be allowed by the internal scope.

Because there is internal scope that we have created.

Right?

That internal scope will be called by the other order service.

So when the internal services are been invoked right from service to services, that's going to be the

internal scope.

So for that internal scope also when there is a client registration, this should be allowed, right?

So this particular method should be allowed by all the three roles.

So let's define that.

Pre authorize.

And they should have the authority has authority as admin.

And I can copy this and I can say or they should have.

Customer.

Or they should have internal.

And whenever you are writing internal, it is a scope.

So you need to define a scope, underscore internal.

So these are the authorities that we have defined.

Like this method should be allowed by all the three.

They should be only allowed by admin.

So that configuration we have added here.

Similarly, we have to go to the order service and do the same configuration there as well.

So let's go to the order service.

Let's scroll up in the order service.

Let's go to the order controller and yeah, we can do the same configuration.

Now this place order.

Who can place the order?

Only customer can place the order.

Admins cannot place the order.

So here we can define pre authorized at the root pre authorized and has authority.

As customer.

Okay, so this is only by the customer.

Now the other is get order details.

Now this get order detail should be done by both admins and customers.

Right.

And we're not adding internal because there is no API currently which can access directly internal because

this is the calling API.

This API will be calling the different services.

So that's why I'm just adding pre authorized here, which has the authority as admin.

Or.

As a customer.

Okay.

These two information.

Now, you can always go ahead and mix and match all the information.

The only thing you need to consider is that the case.

Sensitiveness So whatever the value that you have defined in your octet, the same thing we have to

use here.

So this all information, this all configuration that we have added here, now what we can do is ideally

all the configurations are done.

We can restart all our application and we can test everything.

Okay.




Transcript - 
Let me just restart the application.

I will just restart cloud orders.

Cloud Gateway or the Service Payment Service and the product service.

So let me just restart everything here.

Let's wait for a second to complete all the process.

Okay.

All your applications are up and running now.

What we will do is we will check in our service registry that everything is up and running.

Within our service registry, we can see that we have API, gateway config, server order service payment

service and product service.

Everything is done.

Now we need to call our API.

So what we will do is we will go to the.

Safari, and we will refresh this page to get the latest refresh tokens and access tokens.

Okay.

Okay.

You can see that you got the token here.

Now, what we have to do is we have to use the access token here.

So copy this.

Access token.

The access token is TILIA.

So let's copy the entire token here.

Now, this token, we need to sign, as I explained to you earlier as well, it's the client responsibility

always to send the token to the services.

So I will go to the client.

My client is postman right now and this is my API.

That is the local host, 1990 slash order slash phi.

This order I want now what I need to do is I need to send this token as well.

Without that, it will not work.

So I will need to go to the authorization and here I need to select the type as auto.

You can see there are multiple types, right?

So I will select the or two here and here you can see that add authorization data to where here you

can see it's added in the request header.

And then here you can see that available tokens.

So I will just scroll up here and I will just add the token here.

This is my token.

Now, once it's done, you can submit the request.

And here you can see that you are getting the data right.

So you can see that internally it is passing the access token to each and every places.

Now, if I just change one information right in the bearer token, if I change one of the values and

if I hit send, you can see that I'm getting the 401 unauthorized error because my token is not correct.

So you need to make sure that your token is correct and then only will be able to get the information.

So this way we can implement the spring security with Okta authentication in our microservices.

So you can see that how easy it was if you go to the code.

It might seem a little bit complicated, but it's just a default configuration.

When you are searching anywhere, you will get a standard code format and you can easily be able to

implement this.

Just follow these steps and just follow the concept like where we have to use what things where you

are using the global method security or you are going with the URL parameter approach or whatever it

is.

So we just implemented the spring security with authorization token with auth code and also using the

different roles.

So with all the different roles, different functionalities that have been implemented.

So similarly you will be easily able to do it as well in your projects.






Transcript - 

Now let's implement unit testing in our project.

So in the unit testing, what we are going to do is we are going to use the J unit as the dependency

unit is the dependency that we are going to use.

This J unit will come by default when we are generating the Springboard project and this j unit is a

j unit fi that is the J unit Jupyter dependency.

So this is what we are going to use to create our J units.

We should always create the J units for all the functionalities that we have created.

If we have created the different layers for all those different layers, we should create the test cases.

Now, within this J unit, we are going to use two different things.

Within this J unit.

We are also going to use the Marketo framework.

Marketo is one of the libraries and we are also going to use the wire MOC.

These are the two things that we are going to use in our application to create the test cases.

So Marketo is something that we are going to use to mock the methods.

Now, before we go ahead, we need to understand a concept called Mock.

So what is a mock?

Whenever we are doing the unit testing, we always heard that we need to mock this or we need to mock

this.

But what is this?

Mock what mock means?

Suppose if you are writing a test case for one of the functionalities, suppose you are writing a test

case for getting the orders.

Och, I'm just taking the example that we are writing the test case for getting the orders and within

this getting the orders functionality, we have multiple calls to the database or multiple calls to

the other service as well.

But here what we are doing is we are just trying to test this functionality.

We are not trying to test the other calls that it's been doing from that particular method.

So for that, what we do is either they are getting called from the DB or from the other rest endpoints

or whatever it is.

What we do is we mimic those scenarios programmatically.

So what we do is rather than directly calling the database queries or database methods, what we tell

them is whenever you are calling this particular method, return this object, return this mock objects,

We will define those when we are doing the coding.

But this is something that we define okay, Rather than calling that this is something that you can

return, we will create the object and we will return that.

Similarly, we will define for the rest endpoints as well.

Like whenever you are doing the rest endpoint, call, don't do the rest endpoint call, just return

me the values from here.

So that means we are just mimicking that particular behavior and we are just passing the data.

So we are marking that particular object rather than not calling everything.

We are just more concentrated on the particular tasks that we want to run rather than calling all the

different APIs and everything.

So this is a concept where we will be using a lot when we are writing our test cases.

So we'll mock a lot of objects and we will define like whenever you're calling this return with this

particular things.

So this way, whenever the test cases are run and whenever a particular matter is been called, rather

than calling an actual method, it will mimic the behavior of that.

The other thing we talked about was via mock.

Now, when we're mock, we will use, we will use via mock, when we want to mimic the or mock the rest

endpoints here you can see in the Marketo we were mocking the methods, but in the via mock we will

mock the rest endpoint.

So we will actually mock what particular URL that we are going to call.

So suppose from the order service, we are going to call the product service and the payment service,

right?

So we will mock those rest endpoints.

So we will get a particular response back.

We will define that.

Okay, this particular wire mocks is going to return the JSON data, so we'll mock that JSON data in

a particular JSON file and we will return that file.

So this is something that we are going to mock using the mock and from the marketer we are going to

use the methods.

So what we'll do, we will create two type of test cases.

One will be our unit test that we will do for our order service.

So whatever the methods are there in the order service that we will do the test cases for that we will

write test cases for that and what we'll do, we will also write the integration testing.

So whatever the controller we have created, right for the order service, that entire controller,

we will write the test cases for that.

So for the order controller, we will write the test cases and we will mimic all those API endpoints

using this via mock OC.

So these are the two things that we are going to do.

So once you do this, you will get the complete understanding about how the unit test case should be

written in your applications.

So without wasting any time, let's go ahead and implement the order service test cases.







Transcript - 
So now you can see that we are in our in Algeria and we are in our order service project.

So we are going to implement in the order service.

Similarly, you can go ahead and implement in the product service and the payment service is very easy.

We are just picking the hardest part.

So once we do this hardest part, the other parts will be very easy for us, right?

So that's why I have picked up this order service which will cover all those particular scenarios.

Now to add the taste cases, we need to have some of the dependencies as well.

First, we will need the dependency of the G Unit Library and Marketo library.

That to library will be default added.

When we are creating the Springboard project, the other library we will need is the one we just discussed.

You are right that we are going to use the wire as well, so we will be needing this library to be added

in our project and one more library we will need is the Spring security test, because we have added

the spring security in our project.

So we will need to mimic those actions as well.

Like what are the JWT tokens and what are the things that we are going to pass in the actual APIs that

things we need to mimic in our test cases as well?

So for that, we are going to use the Spring Security Test library as well.

So these two libraries that we need to add.

So let's go ahead and add that.

So what I will do, I will go to the XML file of the order service.

Okay.

This is the order of this XML file.

And here I need to add the dependencies.

So here I have added the dependency that is via g eight and this is the latest version.

2.3.2 and I have added another dependency that is the spring security test.

Now, alongside these two dependencies, I'm going to add one more dependency that is the edge to dependency.

That is the edge to end memory dependency because when we will write test cases, we are going to store

those data in a particular database.

So we will be able to understand, okay, all those particular things that we are doing are able to

save the data and we are able to get the data from the database as well.

And that particular database is going to be the in-memory database.

So it will be only used in our test cases in the actual we are using the MySQL.

So let me just add that dependency as well and I am adding that dependency as well.

That is the actual database.

Now don't worry if you are still feeling that from where I got this dependencies then as we have got

from the earlier from the spring initialized, you can get all this dependency from this spring initialize

or you can just add those dependency and you can copy those dependency here.

If you Google it as well, you will get the dependency from the MAVEN repository as well.

Now once we have added this dependency, we need to configure our application dot YAML file as well

because if you see the springboard project we can do the configuration for our actual application and

for the test cases differently.

So within our test folder we can create the resources folder and in that resources folder, if we add

the application, the HTML file, that particular application YAML file that configuration will be only

be limited to our test scope, not our actual application scope.

So that also we will be needing.

So we will create that as well.

So let me just.

Create a new directory and tell it resources.

Okay, So within this resources, we can create the application or HTML file.

So I'll just create the file and I'll just say application dot yaml file.

Within this application or the HTML file, we can add the same configuration that we have added here.

What you can do is you can copy all this configuration and chain the URLs and everything to point to

edge to in-memory database rather than the actual database.

I have already created this YAML file, so let me just copy it.

I'm just copying this entire file here.

So now you can see that it's a simple configuration that we have added.

We added the spring config import like whatever the config server that we were using.

We are that only thing you can see that we added as an optional configuration.

That means if this configuration, if this config service is available, that's fine.

Get those configuration from there.

Otherwise, if it's not available, we added the optional so that means it won't throw the error.

Because if you see our application, whenever this entire application is running, it is not necessary

that our config server is working right because this is just the order service that we want to test.

So that's why we added optional.

So this springboard application will not freak out that I need the config server.

So we added the optional configuration and then we added the data source.

You are equals to edge two ma'am order database and the username and password.

We gave the edge to driver.

We give what driver we are using.

Then the database platform we give that is the true dialect.

And in this security we added security auto resource server and the client registration and the provider.

These are the same information that is there in our main application YAML file.

If you see here, these are the same configuration we just copy pasted here.

We don't need any resilience, forge dependencies or anything and we just disable the Eureka client

for testing purposes.

So that's the only configuration that we have added.

So you can directly go ahead and copy this file and add in your configuration.

So I just explain you like what all the things that we have added and why we have added.







Transcript - 
Now, once the configuration is done, we can add the test case files for our service layer.

So what I will do is I will go to the order service impl file.

Now, for this particular file, I need to add the test cases.

So what I will do is I will right click here and I will go to generate and I will generate the test.

Once you click here, you can see that it is asking that I just want to use which library I want to

use the unit file library and my class name would be orders of simple test.

Is there any superclass or anything which package you want to add?

All this information it will ask you.

Okay, you can add everything here, whatever you want.

You can select the checkboxes and click on.

Okay.

As we are doing everything manually, I'm not adding anything.

I will just click on OC here.

But if you don't want to do this way, then you can go ahead to your test package here and create the

service package.

And in that service package you can create the order service, simple test class, whatever you do,

that's fine.

It's the same thing.

Now, once I click okay, you can see that within the test folder, within the service package, the

order service I am build test is created.

Let me just remove this.

I don't need this and I will just make this as a public class.

So your test class is created.

Now we will add our test cases here.

Now, first thing to make this particular order, for example, test as a test class, I need to annotate

with springboard test.

So now this is going to be my test class.

So spring will know that now what we have to do is we have to mock the services that we are using in

our order service.

So if you go to the order service, MPL, and here you can see that we are using the order repository,

product repository payment, service, rest template.

These are the things that we are using.

So rather than using actually this thing, what we will do is we will mock all the services.

So when we mock all the services, we can call all these methods by mocking them directly, we do not

have to call all the services directly.

We will mock everything and we will just focus on this order service.

I MPL So I will just copy all this thing here and I will go to the order service test and I will paste

everything.

Now, rather than writing Auto Wire, which will directly inject the actual bean, I just need to change

with Mock You can see that this mock annotation is from the Object Market Library.

So once you add mock here all this particular.

Classes will be marked.

Okay, so the mocking is done.

Now we need to inject more where all this particular mocks needs to be injected.

So all this mocks needs to be injected for our order service MPL because for the order example, we

are going to create the test cases right now.

So for that what I will do is I will write the object order service.

This is the interface Order service equals to new order service.

I'm Bill.

This is the implementation that I'm using for my order service.

And now all the marks should be injected here.

So I will just write inject mocks.

Right.

So this is the class which is going to inject all the mocks available hope everything is clear till

now, like where we are using mock and where we are using inject mocks.

And what is the use of the springboard test?

Now what we have to do is we have to test all of our scenarios.

So if you go to the orders of example, we have different methods.

Now all these different methods do some different operations and all those different operations are

done based on the different conditions all together.

So we need to handle all those conditions in our test cases.

That's the primary goal.

So if I take this particular example of the get order details, so let's try to write the test cases

for this, get order details.

So what we are going to do is we are going to call this get order details and this has few scenarios,

right?

Like the first is a success scenario and the other one would be failure scenarios.

These two things is by default that you have to add in all your test cases.

So success scenario would be like when we pass your order ID it will get the order details, then it

will get the product responses from the product service and then it will get the payment response from

the payment service.

Then it will create the entire object for us and it will return the object.

That's the success scenario.

So that scenario we will handle.

And one more test cases would be like, I will pass the order ID and there might be some issue that

this particular order is not found or we might got some error from the product service or we might got

some error from the payment service.

Anything can happen.

So that can be an error scenario or failure scenario.

So all the scenario we can handle.

So let's first try to handle the success scenario where we will call this get order details, where

we will pass this order ID and we will get the entire object back.

And one more thing.

You need to understand that whenever we are passing the order idea, there are different operations

being done here, right?

So we will see how we can handle this.

Different scenarios as well.

So now let's go to our order service test.

Simple.

And here let's create a method to handle one test scenarios.

So what I will do is start with void and I will just give the name of the method test.

When order is success, you can see that by the name of the method, I will be able to understand what

this method does.

So try to form a habit that from the name of the method itself, from the name of the test methods,

you will get to understand that what this method is doing, what this particular test case is doing.

And this is your method.

Now, to make this as a test case, what you have to do is you have to annotate with a direct test.

So now you can see that we got a play button so we can individually play this test as well.

And whenever we are building this application at the time, also, whatever the test methods are, they're

annotated.

With other test, those methods will be executed.

Now, ideally, whenever the test cases runs, we generally tend to generate the reports.

So what particular name should come when the report is generated?

So for that we can use at the rate.

Display name annotation from the J Unit Jupyter API.

To add the name here, I can define get order.

Success scenario.

So from here, you can see that we'll be able to easily understand from the report as well.

Now, within the test case, we need to call our services.

So what we ideally do is we call the actual methods here, like which particular method we need to call

that particular method, that particular call we will do here.

And for that particular method, within that particular method, whatever we are going to call for that,

what are the internal different calls available?

So for those internal calls, what we will do is we will call the mocking.

Okay.

So for whatever the actual method, we are going to call for that whatever the internal calls would

be, we are going to mock those calls.

And once we actually call the method, we need to verify that whatever happened, that is something

correct or not.

So for that, what we will do is we will assert the operations, like whatever the result we wanted

to get, we got the result or not, those type of things.

And we also can do is verification, which means that whenever we are calling this actual method, there

might be some internal calls as well.

So how many times those internal calls happen because some of the calls may be within the conditions,

right?

So sometimes some of the calls may be skipped, sometimes some of the calls may be added.

So all those particular scenarios we can handle, like from this example, from the test order, if

we can see that order, for example.

So from here we can see that okay, it is calling the order repository find by ID method so we can add

in the verification that does it.

Call this particular method or not.

If it call, it should only call one time after that.

We can also add the another thing, like it should call the rest template get for object for the product

service.

Was it able to call or not?

And if so, it should only be calling one time yet.

Also it should be calling one time.

Okay.

This way we can add the verification like how many times it has been calling.

So that way our test case would be full proof that whatever we have implemented within our actual method,

everything is properly executing and we are getting the proper response.

So once we add all these steps, your test cases will be foolproof and your application will also be

foolproof.

So you will get very few bugs.

So with those basics aside, we need to call this get order details from the order service MPL.

So let's call this.

So what I will do is I will go to the order service test in the actual call, I need to call that,

right?

So what I will do, I will just use order service.

Dot get order details and the order ID I'm going to pass this one.

That's the random I.D..

Now, whenever I will call this method internally, there are multiple calls, right?

Those multiple calls will be using the rest template payment service, product, service order repository,

whatever would be.

And we are mocking all these things.

So we need to mock those as well here.

So let's try to do that as well.





Transcript - 

Well, guess what we are going to do here is if we go to the others, for example, the first thing

when we call this method is this log info.

That is fine.

It's just a logger.

After that, what it's trying to do is it is trying to get the order details.

That is the order object from the order repository find by find by ID.

So that means you have already marked this order repository.

And from that it is trying to call defined by ID method.

So this is something that we can mock, right?

Because we are not allowing this as an actual call because we have mock this repository, we are only

concentrated on our service layer.

So this is something we can mock.

So what we will do is whenever we are calling order repository defined by ID, create the order object

and return that order object.

That is something that we are doing.

So if I go back here in the order service test, I can define.

Marking here so I can define mosquito dot when.

Order repository dot find by ID.

Now this find by ID.

I do not want that.

It should only be passed one.

What I want is this other ID is long.

So for any long.

Value.

You can see that it is from the argument matches not any long.

So for any longer value.

What I will do is I will just mark this as a static import.

Okay.

Add an on demand static import so you can see that now it's a little bit cleaner as well.

What I will do is for Marketo as well, I will add the static input.

Add a static import.

So now we can see it's very clear when I'm calling order repository defined by ID parsing any long value,

I need something in return.

Right.

So here I can define then return me something.

So what?

I should return back.

So if you go to the orders of example, what this is returning is find by ID, it is returning the optional

of order.

Right?

Optional of order.

So I need to do the same thing.

So if I go back to the order of example test, I need to return optional optional off.

And here I need to pass the order object.

Now I do not have order object, right?

So I can create the order object here.

So let me just create the order object so I can define order.

Order equals to what I can do is I can get the order from a different method altogether.

So I can define get mock order.

OC this order.

I will pass here.

Let me just import the order here.

This is from the Condor Daily Code buffer order entity.

And let me create this method as well.

Get mock order.

So I will just show context, menu, create method, get more order in the order service MPL test.

So you can see that this method is created, which is a private method.

So here I can return that object.

So here I can return order, dot, builder, dot build, and here I can pass all the values.

So I can say order status equals to placed.

Dot order did instant dot Now dot order ID has one because we are going to pass one write dot amount

as 100 dot quantity as 200.

Dot product IDs two dot bill.

So you can see that we just created a dummy order object and that we are passing here.

So here at this point you can see that we got the order and whenever we are calling order repository

dot find by ID with any long then it should return optional dot of order, which is this one.

So one mock is done.

Now if you go to the order service, simple again.

The other thing after that it is calling is here you got the object so that means it won't go for or

else throw because we are passing the object back then.

After that, what it's doing is it is doing the rest template dot get for object.

So we have added this template as well for the marking.

So we will mock this as well.

So whenever we will call this template dot get for object with this URL, we should return the product

response.

Similarly, after that it is calling payment response as well.

So for this also we will do the same thing whenever I'm calling this template dot get for object for

the payment service call, I should return the payment response.

Okay.

Once that is done, we should return the order response and that should be validated against a particular

object.

So let's go back to our test case and let's try to mock these two services as well.

So I will go back to the order service example and let's mock here.

So whenever now whenever I'm doing rest template dot get for object, which is this thing, right,

which is product service slash product plus order dot, get product ID, whatever it is.

So if I go to service MPL, that's the same thing, right?

You can just copy this.

I should be product service slash product plus order dot, get product ID.

This is the thing that I copied comma, product response class.

So after that.

Comma.

Product response class.

Whenever you're calling this.

Not then return.

What we need to return at this point, we need to return the product response back.

So here I can create a different method to return the product response back so I can define get mock

product response.

And here, let me just create this method.

And this is going to return the product response.

So here I can do the same thing what we did here.

Return Product response dot Builder dot build dot Product ID as to because that's a tool that we have

used here, right?

Dot product name iPhone dot price 100 dot Quantity 200.

So this is the thing that I have defined.

Now I will be getting the product response also here.

Now, similarly, I need to add for the payment service.

So let's do that.

So I'll just define when rest template dot get for object.

And here's what we have to do is I will go to the service MPL and here I will copy these two things.

Okay.

That is payment service slash, payment, slash, order, order, dot, get ID and it should return

payment response class.

If we are calling this, then it should return get mock payment response.

I will create this method, show context action and create mock payment response in order.

Example test.

Now we need to implement this method.

Return Payment response dot Builder dot build dot Payment ID one dot Payment date as instant dot now

dot Payment mode as Payment mode dot cash dot amount 200 dot Order ID is one status is accepted.

So this is a payment response that I've created.

Now this payment response, we will get back here.

So now you can see that within the test when order success, we created the order and whenever the order

repository defined by ID we are doing for any long value, we are returning the order of optional option

of order.

And whenever we are doing this template dot get for object for a product service, we are returning

the product response and whenever we are doing the payment service, we are returning the payment response

object.

So you can see that we did the marking and we did the actual call as well.

Now we can go ahead and do the verification and assertion.







Transcript - 
So in the verification, what I can do is I can do Marketo verify.

This is also from marketers.

I can just remove this and I can directly to verify what I need to verify and to verify order repository

and how many times it was called.

So one time and from this order repository, what was one time called, which was find by ID.

Find by ID passing any long value.

I hope this makes sense because if you see here we have added like whenever we are calling order repository

dot find by ID for any long.

That's the one call in the order repository for the find by ID method.

That's the same thing that we defined here that verify that in the order repository we call the find

by ID with any long value one time.

So it will verify this that particular call happened or not.

Same thing.

We need to make sure that we do for this also that does this happen or not?

This is something that we have just defined that whenever you're calling return this, but this will

actually be verifying that.

Does this call happen or not?

So here we will again add verify rest template times one dot get for object this value.

Okay, so whenever from the last template I call get for object for the product service that will return

the product response, that should be one call.

Similarly, if I just copy this.

Okay.

And if I just pass on the payment thing rather than the product thing, you can see that payment should

also be called once.

So we added the verification that order repository find by I.D. should be called once from the rest

template.

Get for object Product service would be called once from the rest template.

Get for object payment service should be called once.

So all the verifications are done now.

Now assertion like what are the object that we are getting back?

What is that?

Now, after finding all these values, what it is doing is getting the order service object back, right?

So I can I can take all those details from the order response here that it is passing the order response

back.

So once I call this, the entire value should be stored in the order response here, right?

The same way what it was doing in the order service.

MPL Right.

Once we get all this value, it is creating the order response object and it is just setting the value.

So this is something that we can ignore mocking because this is just something adding the values after

getting all the values from here.

So we just need to define that whatever the order response object that we are getting should not be

null, right?

So if I go here and I can assert that.

Using the assertions you can see assertions is from the j unit Jupyter API.

Assertions dot assert not null.

Yet I can pass order response and this assertions.

I will just do a static input.

Okay, so I'm just doing assert nominal order response.

And one more thing I'm doing is assert equals.

So we are just matching the expected and the actual value.

So what I will check is like whatever the order that we pass, right order, not get ID that should

match with the order response, not get order ID right.

Because whatever the order idea that we pass, that should be equal to the same order idea that we got

back here.

So now you can see that we entirely implemented our test case for the success scenario.

So now when we run this, it should work.

So let me just run this.

And you can see that the entire test case is completed and we are seeing green over here.

So that means your test case is completely working fine, right?

So this way you will be able to implement a test case.

Now whenever you are not implementing any one of the things here, right.

Suppose if you have not implemented this thing like whenever you're calling this, return this.

If you just do this and run this again, it will throw you error.

Okay.

You can see that you got the null pointer exception here.

It was not able to get some of the data.

So you will get all the information here and you will see that something error is there at the orders.

Resample Java 118 line number.

So here you can see that at line number 118 order response.

So it's probably highly like this order response is null, right?

So from that null object, we cannot get the payment ID, So what this is null payment response.

So that means whatever this call is right?

Payment risk payment response, this last template get for object, this is returning null.

So that means this call didn't happen, which means that for this particular call, we didn't do any

mocking.

That's why.

Right.

So this way, whenever you are implementing your unit test cases, you will be able to understand based

on the errors that what is missing and how you can solve those errors.

So let me just go back and uncomment it out and your test cases is done.








Transcript - 
Now let's implement this failure scenario for the getting the order.

So let's go back.

And what I will do is I will will copy this entire test case.

Let's not copy it.

Let's create a new so it will be easier to follow along.

Right.

So let me just create a new test case and I will just define word test when get order not found.

Then not found.

Which means that whenever we are not able to find the order, we should throw the not found error.

This is going to be again test write and we should add the display name.

Get orders.

Failure scenario.

Now what we will do is we will do the similar thing, what we did earlier.

So here's what I'm trying to do is whenever I am calling this actual.

Thing, right?

Whenever I'm calling Order response.

Okay, this will go ahead and call the order repository dot find by ID.

So at that time, what I'm trying to do is whenever I'm calling order repository DOT find by ID for

any long value.

Okay, then return.

And I'm doing optional dot off label.

You can see that I'm passing the label value of null, so I'm just passing null here, which means that

whenever I'm getting null here, it will throw the exception from the actual method itself.

Right?

So if you go here from the actual method, you can see that if I'm not finding the value here, it will

throw the custom exception here at that order not found.

So now we will be getting this custom exception.

So we need to handle this custom exception rather than going through all the different logic that does.

We got it or we got this or not.

So let's go to the test class now and we just define that or the response we should get.

And whenever we are calling the order repository dot by ID, it will throw the error.

It will be returning the null here.

Now let's do assertion.

So what I will do so as we're going to do a session, so we'll just do assert throws.

What is going to throw and whatever it's going to throw, we can get that as well.

So I'm just doing taking that value.

Your custom exception.

Okay, So here we can define what is the expected type.

That is custom exception class.

And what is the actual actual would be whenever we call this.

Okay, so now I don't need this.

I'm just passing the value here directly.

So whenever I'm going to call this order, get order details for one, it is going to throw the custom

exception which will be matched here, and I will get this value here.

Now, again, let me do some more handling.

So I will just define assert equals.

Now what particular exception code error code is going to return that also I will match.

So it's going to return order not found, right?

If I go and check order.

So simple.

Okay.

And within the custom exception you can see that it's going to return not found.

Right.

So let me just use this one.

So if I go back to the impl and this is something that is going to return as the value, right?

So here I can define from the exception dot get error code this should match and what is going to be

status.

So assert equals status would be for zero four and exception dot get status.

So status we are going to get 404.

Now we will do verify as well once so verify that order repository.

It should have called one times at least not find by ID for any long.

So you can see that it's a simple matter where we have defined that whenever I'm calling the order repository

defined by ID with any long value, it should return me null optional of null.

And whenever we're calling actual order services, that order details it will go in the method and it's

going to call this particular method and it's going to null.

And whenever it's null, it's going to throw the exception.

So we check there when the assert throws that it will give me the custom exception, it will match with

that and it will see with that particular exception here in the custom exception and within this exception

class, we are matching that what particular error code that we are getting and what particular status

also we are getting.

So if those also match and if we verify that order repository should be called at least once because

we are calling it once and the rest of the thing is fine, we are not calling it because before that

only we are throwing the exception.

So that is something this particular method will identify each and everything which will happen when

there is no order available.

Similarly, we can define other methods when an order is available, but when payment is not available,

when order is available, product is not available.

So similarly, you can see these are the different scenarios that you can handle.

So this is one of the scenarios.

So let's run this and see what happens.

So you can see that your entire order get orders fail.

A scenario is also successful.

So ideally at the end you're all the test cases should be successful.

So everything is successful now.







Transcript - 
Now let's go ahead and implement the place, order API as well.

Let's implement the test for place Order API.

So here let's create another method.

And here will define void test.

When place order success.

Okay.

And this is going to be the test annotation will add the add the display test display name as well.

Sorry.

Place order.

Success scenario.

Now, within this test, now within this place, what a success scenario.

What happens?

So if I go here within the orders of example, if I go up, scroll up.

And this is the place.

All right, So within the place.

Order.

What we are passing is we are passing the order requests.

So that is some object that we need to create.

We need to mark this order request.

So that is the one thing that we need to do after that.

That is the longer after that, there is a product service call to reduce the quantity.

So that is something that we need to mock.

After that, we are getting the order object as well, right?

So we need to work with the order object to save.

So we are going to create the order object as well here.

After that we are going to do the payment requests as well.

So we are going to mimic the or mock the payment service as well.

And after that we are doing the payment and we are saving another data to the order service and we are

getting the order back.

So these are the things that we are doing.

So let's handle the success scenario where everything is a positive flow.

So we need the object of the order and we need the object of the order request to pass on the details.

So let's handle that.

If you go here, what we are going to call is long.

Order it equals to order repository dot place.

Sorry not order repository order service not place order.

And this is going to take the order request.

This object is not there so we need to create that object.

So let's do that.

So we are going to define order request order request equals to get mock order request.

So this method is not there.

So we need to create this method.

So let's create a method here.

And this order request we are going to pass here a simple thing.

Now let's define this return order request dot builder, dot build.

So here I'll define dot product ID one dot quantity ten dot Payment mode Payment mode dot cash dot Total

amount 100.

This is the order request that I'm doing now alongside this order request here, whatever we got, I

will need the order object as well.

So I'll just define order.

Order equals to get more order.

These two things are done now.

Now there is one more concept here.

That is the before all and after all as well before a particular method has been called.

So I can define a method and annotate that method with before all.

So that means that particular method will be called whenever the every test case has been called.

So whenever I'm going to call this method and whatever the ideal mocking I need, I can define all those

mocking there.

Currently I need all the different markings here.

So that's why I have defined this way.

But ideally you can add all those particular markings which are common to a particular method, to a

method, and that can be called for each and every test cases that also can be done easily.

You will get such kind of scenarios as well when you are working with the real world test cases.

Now, the first thing that is been done here is if you go to the order service in the place, order

first thing is saving the order, right?

And calling the payment service.

So let me just save the order and call the product service to reduce the quantity.

So let me just go here and I can define like whenever I'm doing order repository, dot SEO, what I'm

going to save anything of class, order anything of class order at that particular thing I'm trying

to go and save then return order.

Cool, right.

You can see that how easily we can mock everything.

Right?

So this is Mock.

Now, there was a product service as well, so let's do that.

When?

Product service.

Dot reduce quantity and this reduce quantity.

It has two parameters, right?

You can see there are two parameters.

One is product ID and one is quantity.

Both are long, so I can define any long value comma, any long value.

Then return.

What is returning?

So if I go to the reduced quantity here it is returning the response entity void.

So I need to define then return new response entity of void and HTTP status.

Not.

Okay.

This is something that we are returning now.

Now, after that, the other thing was if you go to the other service, we just mimic this reduce quantity

ten orders that is also we mimic.

After that, there is a payment request.

Right.

So if I go to the.

Do pavement.

And this do pavement is also returning the response entity long value.

So what I can do is I can go to the order service I test and here I can define the same thing like when

I'm doing payment service, dot do payment, and yet it's taking the payment request.

Right?

So I'll just define any class of payment request.

We just get a payment response.

It should be payment request, any class of payment request or then return New response.

Entity which is of type void with HTTP status dot OC.

Right this is the thing.

Cannot resolve matter then return.

If I go to do payment here this is returning long value.

Right.

Sorry.

Sorry.

My mistake is returning the wrong value.

So if I go to the do payment here, I should expect a long value rather than.

So if I go to the service, I am built as it should be long, any long value.

And now after that, once we call this, we can do the verify call, right?

So for the verify call, we can define verify order repository, how many times it should be called.

It should be called two times.

Because if you go to the.

Or as example.

You can see that order repository has been called twice.

First one is at this particular time or the repository dot save.

And after that order repository don't save again.

So it should be called twice for that same method.

It should be ordered repository dot save method and this save should be for anything.

Then again, we'll do verify product service how many times it should be called.

It should be one time for what?

Dot Reduce quantity for any long value.

Any long value.

Then verify payment service.

It should be called once.

Not do payment for any payment request class.

Okay.

You can see that we did verify call as well.

We can do assertion as well.

So we'll define assert equals whatever the order that get order ID is there the same order ID we should

have returned the same order we should have written.

So now let's try to run this and check if it's working or not.

So we found some error.

So let's try to resolve this.

What the error is.

Invalid use of argument matches to match as expected and one coded so at the daily code Buffer Order

Service Service Order Service MPL test.

Headline number one 117.

At this point, the request may, if matters, are combined with raw values in correctly some method

any with raw string.

When using matches, all arguments have to be provided by matches.

So what I will do is rather than parsing any longer, let me just give any value.

So ideally what is going to return for a payment?

So let me just scroll down here.

Payment, response payment.

That is going to be one, right?

So let me pass one year rather than.

Okay, let me try this.

Cool, right?

You can see that they are a result.

So sometimes you might get this type of issue.

So you need to understand what the issue is trying to tell based on the error response that you are

getting.

And accordingly you can change the code and it will work.

So you can see it was asking that you cannot pass any value here.

You need to pass particularly what it's trying to return, or it should match with the sum of the equals

operator.

So that's why we pass that.

It's going to return one because our order response, sorry, because of a payment response, you can

see that it's going to return a payment 81 So that's why I defined that it will return one.

So you can see that now this method is working.

Now let's go ahead and implement the failure scenario also for this.






Transcript - 

So let's define failure scenario wide test of an.

Place.

Order.

Payment fails, but still the order would be placed right.

It won't be successful, but the order will be placed.

So this is a success.

For that.

Let me just change to then fails then.

Order is placed.

This is going to be test case.

So let me just define the test and I can define display name as well.

Okay.

Please.

Order!

Please.

Order.

And payment is field scenario.

So your what things we need is we'll copy from the earlier as well to make it quicker.

So we need the order.

We need the order request whenever we try to save.

This is something that we need.

This is something that we need now whenever we are calling the payment.

And that is something that we need to handle rather than passing the success, we need to pass the error

response.

So let's copy everything.

Now.

This is going to return error.

So then throw is going to be new runtime exception.

The other thing is that this is all going to be running, right?

We are going to call each and everything, but at the end it is going to fail.

So let me just copy all this value here as well.

And we are going to get the order ID as well at least.

Right.

But the only order status would be different.

Okay, so now let's test this scenario.

And you can see that it is successful, right?

The scenario is successful.

You can see that we have one error which is throwing the payment level.

Right.

A recurring payment.

Changing order status to payment failed.

So you can see that this scenario is also working when there is a payment failure for your order.

But your order is successful or your order is placed.

But it's not completed yet.

So this way you can see that we just completed the all the test scenarios for our order service.





Transcript - 

Now, if you want to check the coverage, like how much code that we have covered that also you can

do.

So what we can do is so using the IntelliJ idea, it provides us how to check the coverage.

So let me just correct the imports here.

Okay.

So if I right click here in our order service test and there is a run order service test with coverage,

similar thing will be there in Eclipse and SDS as well to check the coverage.

So this is a good idea that before we commit our code, we check the coverage.

So whatever the coverage is defined in your organization, like what particular line of code that you

need to cover accordingly, you can check the coverage and you can add all your implementation.

So let's check the coverage here and pretty sure it's going to be less.

But still, let's see what it will show.

So you can see that all the test cases are run now and you can see that there is a coverage OC and you

can see that this particular package is there and there is one class, 100% methods are covered and

line of code is also 100% covered.

OC So if I open this and yeah, you can see that whatever the green line you can see, right, whatever

the green line you can see, that means those areas are covered.

So you can see that pretty much we have covered each and every details here.

So this way you can see what what is your coverage and accordingly you can improve on your test cases.

Now, this was all about the unit test cases for your order service.

Now, let's go ahead and implement the integration test for your controller.







Transcript - 
Now for the controller layer, we are going to implement an integration test, which means that whatever

the API is that we have created, we are going to test all those APIs and we are going to mock those

APIs.

So ideally what we have done is we have created the order service, right?

And this other service is going to call the product service.

And it is going to call the payment service as well.

This is what it's going to do.

Now, all these things, all this thing are connected to the service registry as well, right?

All these things are connected to service registry and for each and everything as well.

Here for the order service, there is the Okta authentication as well.

Right.

We are going to connect with everything with the OCT as well.

So you can see that there are a lot of components involved.

When we want to test the order service, we need to take care of the security as well.

Like whatever the tokens that we are passing that also has to be passed and from the order service we

are going to call the product service in the payment service.

So that means whenever I'm trying to call this product service in the payment services, those are connected

to your service registry, right?

And we are going to call them using this service registry, using the instance name itself, rather

than calling the IP address or the port number.

So to handle the scenario, we need to do some kind of configuration.

We need to use the wire mark first of all to call the rest APIs.

And we also need to configuration for our service registry because whenever we are doing the testing

for the order service at that time, there is no guarantee that you will have the service registry up

and running.

And whenever you are writing the test cases for this order services, you should write in such a way

that there is no dependency on the service registry, right?

Because you are just testing this component itself.

Our goal is to just test this component, not the service registry.

So we have to mock all these things as well.

So we need to do the configuration for the service registry.

From that service registry configuration, we should return this particular payment and product services.

So we need to have those configurations also in place and we need to also have the configuration for

the Okta.

That we have already added.

Right.

But we need to have the configuration for OCTA as well, and this should return me JWT data.

So with all this configuration, we can use the Wire mock to do the integration test for our order service.

So you can see that these are all components that we need to configure to do the entire integration

test for our order service.

Right?

So let's go ahead and implement our test cases.

We will create our test case for our order controller and we will do all such kind of configuration

as well for the services, history and our security.

So let's go ahead and implement that.





Transcript - 
So as we did earlier, we need the class test class for our order controller.

So what we will do is we will go to the order controller here and we will do right click and generate

the test case.

So we'll go to generate and we will click on test.

This is going to generate order controller test in the controller package of the test folder.

Simple, right?

So you can see that this is created and if you see here the structure, you can see that within the

test folder there is a new controller package created and within that there is a order controller test

created.

I don't need this, so I'll just remove this and I will add a public class here.

Now, to make this a springboard test class, I need to annotate with the rate springboard test.

Now this is going to be your integration test, right?

So what I will do is this is going to create your entire context for you.

So I'll just define that.

This context should not be dependent on any port, so I'll just define that configuration.

That server dot port equals to zero, which means it will run on any port.

Now we have also added the configuration properties in our application or the YAML file, right to be

used in our test cases.

So within this test case I will need to use those.

So I will just enable the annotation using enable configuration properties.

So all those configuration properties will be enabled here.

Now within this order controller test, we are going to use the integration test.

That means we are going to call the API itself.

We are not going to call the order controller method that is this place order or anything.

We are going to actually call the API that is the post mapping API, right?

So to enable that right, we are going to use the mock MVC.

So mock MVC is a class that is going to mock the entire MVC for us.

So to enable that we need to enable the configuration for that.

Right?

So for that I'm going to enable that.

So if I go to the order controller test here and here, I'm going to define the configuration.

So here I'm going to define auto configure or mock MVC.

So all the auto configuration will be added for the mock MVC that we are going to use to call our entire

rest API itself.

Now, once we have added this, we also need one configuration.

We discussed this right in the diagram that we will need the service registry as well.

So we need to have some configuration of the service registry which will return me.

What are the different instances attached to it?

So that configuration also we need and that configuration should be available within our order controller

class.

So this particular class will know that okay, there is some configuration and that configuration I

can get from this particular classes.

So for that, let me just create one class that will have the configuration for the service registry.

So let me just create a class here.

And I'm defining this as a test service instance.

List supplier.

I'm just adding this class name because this is going to again implement the service instance list supplier

class, which is going to give the supplier all the list of the instances available, test instances.

So let's create this class and this is going to implement service instance list supplier.

So once it is implementing this, we need to implement the methods as well.

So let's implement the method, which is a good method and the service ID method.

And you can see that it is returning the list of service instance.

So this service instance we need to create this service instance is nothing but the services or the

instance of the service that is registered in your service registry.

So we need to create that.

So let's do that configuration here.

So I'll just define that service list of service instance result equals to new error list.

And here I will just add result dot add new default service instance.

This will have the instance ID so I can define payment.

Service and it's going to be having the service ID, So that's going to be payment service again is

going to have the host information.

So let me just add.

Local host It's going to have the port information.

So I'm just adding it as a port and it is a secure service or not secure.

So currently it's not on HTTPS, so it's not secure.

So you can see that payment service payment service localhost 88 and file.

So this is one service that we have added.

Similarly, I can add another service.

There is going to be a product service.

Okay that is also on local host board adad and Falls.

That means that your service instance is running on Port 88, right?

Your service registry and everything is connected to it.

And at the end we need to return.

Flux.

Flux of service.

Right.

So flux dot just off result.

So you can see that we just created multiple services and attached to our service instance list.

Now this particular configuration should be available throughout our test instance test context.

Right?

So we need to add the bean for that.

So let me just create a class that will contain the bean of it.

So let me just create a new Java class.

And I let me just define order service config.

That's a simple class that I'm defining here, order service config.

And as whatever the confusion that we define as a direct configuration in our actual application for

the text context, we have to define with a test configuration.

And here let me just create a method public.

Service instance list supplier.

This is going to return service instance list supplier.

This is the same thing that we implemented here, right?

Service list instance supplier.

For this we are adding the configuration supplier and this is going to return new object of.

Test service instance list supplier rather than default.

And this is going to be my been.

Look, this is a simple configuration that we generally add, but rather than normal configuration,

we added the test configuration.

Now, this particular class needs to be injected here in our order controller, so it can use that configuration

here.

So here I can define that with the right context configuration.

What is my configuration for the context?

And yet I can define with glasses equal to whatever the name of the class is that we defined.

That is.

Order service config dot class.

Okay, So now you can see that we added all the required annotations and configurations for running

this our order controller context in our test environment OC.






Transcript - 

Now we will need some of the references that we were using in the order controller.

So if you go to the order controller, yes, some of the things that we are using is the order service,

right?

So we'll need the object of the order service.

And after the order service we will need the object of the order repository as well because we are going

to check the data in the order repository using the order repository from the table, because this is

going to be a complete integration test.

We are going to check the entries in the database as well because we are using the edge to database.

So that particular data will be available in the memory until the entire context is completed.

So within that time we can do any operations on the edge to database and we will need the object of

the mock MVC as we have order configured the mock MVC.

So let me just add all those objects, private order service and I will auto wire this and then I will

need the object of the order repository.

And I'm going to wire that as well.

And I will need the object of the mock MVC as well.

Mock emphasis from the test web.

And I have a lot of this as well.

Now, we need to add the configuration of the wire mock.

Right.

We have already added the dependency of the wire.

Mark, if you go to the XML file here, we have added the dependency of the wire mock.

So here now we need to register that extension via a mock extension that we're using in our class here

and we need to configure that.

So let me just add that I'm just adding static wire mock extension via mock server.

Equals to wire mock ext dot new instance dot build.

And we can add some of the configurations here.

So I will add dot options and here I'll add more configuration, dot wire, more config, dot port.

And I'm going to add 88 support OC.

I'm adding the added support because the same thing that we have configured in our test service instance

list supplier, right?

So we can get the details from those services when we need.

And this is the extension.

So I will use register extension.

And addition register extension.

So this will be registered in our test cases.

Now, we are also going to work with the JSON data, right?

Means converting our objects to JSON.

So for that we will need the object of the object member.

With that, we will be able to convert our classes to JSON and from JSON to classes.

So for that, let me just configure that as well.

So I need the private object mapper.

Okay.

This is from the Foster XML Jackson Data Library.

Equals to new object mapper dot register modules.

Rather than this, I will just define that.

Find and register modules.

What are the modules are there and then dot configure.

And here I can add some of the configurations.

So what I need to do the configuration here is I want to write my dates as a timestamp format.

So that configuration I'm doing here, So I'm doing serialization feature dot right dates, right date

says timestamps to false.

And then one more thing I need to configure the serialization dot fail on unknown properties.

Fail on on properties as false.

So some of the properties are not that it won't fail.

So this is just the basic information I have added here.

Whatever the configuration you need, what are the properties that you need to define?

You can define it from here easily, and most of the times you may not require this configuration as

well.

You can directly create the object of the object member and you can directly use it.

But the idea is to have the object mapper and whatever the configuration you need, you can do this

conversion.

So for those reasons I am adding those configuration for you.

Now all the basic configurations are done and what are the things that we need in our test cases to

write our test cases that things that we have defined Now we can go ahead and write our test cases.






Transcript - 
Now let's write a test case to place the order when there is a successful scenario.

So we'll write it as a case that will place the order.

That is the entire integration test.

So from the place order, it will try to store the data in the database.

From that it will call the product service to reduce the quantity, and then it will call the payment

service to complete the payment as well.

So all those will be API calls marked by wire marks.

So let me just write the method here.

So I'll just define public.

Wide.

Test when place order and do payment is success.

Okay.

And this is going to be my test.

So I'm just annotating with the test here.

So here's what we're going to do.

First, we are going to place the order.

Then after that, we will check, get order, buy order ID from DB and check if that order is correct

or not.

So we're going to use the database as well and check the output of it.

So these are the basic things that we are going to do here.

Now, before we implement this, we need to call some of the services every time when we are doing a

particular process, right?

So whenever we are doing the place order, so for that place order, what are the APIs that we're going

to call?

We are going to call the Reduce Quantity API, we are going to do the payment API and all those things,

right?

So those are the APIs that we can use multiple times within the different test cases.

So what I can do is I can call all those APIs using the common method and I can call that particular

method whenever a particular test case has been called.

So that configuration we can do here.

So let me just do that.

So for that I will just define what setup method.

So this will be called whenever a particular test method has been called and how we will define that.

This particular method should be called for each and every test cases we can annotate with before each.

So this way it will be called for each and every test cases.

And yet I can do some of the steps.

So here I need some of the details so I can define those methods and I can call those methods.

So I need a method for getting the product response right so I can define get product response.

I need a particular detail, so I can define product details, response.

I can create a method and I can call this method.

Here.

I want to define a due payment as well.

Like whenever there's a payment we can define write, do payment.

We can also define get payment details.

Like, what are the payments that we did?

If you want those details, we can get those details and we can define reduced quantity that whenever

I'm doing the order, I want to reduce the quantity.

These are just the default methods I've created.

I've just defined these methods.

Now I need to implement this method.

So all these methods will be called whenever a setup method is called for each and every test cases.

Because these are the general details that I will need every time.

So first let me get the get product details response.

So let me just go ahead, show context and create a method that means get product details response similar

way I can define this as well.

Create method, get payment details, also create method and reduce quantity.

Also create a method.

Okay.

So you can see that I've just defined those methods.

And these methods will be called whenever I'm calling a setup.

So let's define first get product details, response.

Now how I will get the product details.

So whenever I am doing the API request, that is slash.

Product and slash idea of that particular product, Right?

Whenever I'm doing that particular API call, right at this time, I'll be getting this product details

for a particular product, right?

So this particular API call I have to do so I can do this API call using the wire server, whichever

I have defined here, right.

Via mock server.

So using this I can call this APIs.

So I can define here.

Why am OC server dot stub for OC?

So you can see that we are doing stubbing here for what's stubbing that we need to do.

We need to define so I can define why your mock dot get and yet you need to parse the URL.

This watermark is static import.

So let me just import here on demand static import in the get.

I can define the API so I can define the API slash product slash one.

Whenever I'm calling this, I want some data will return what this is going to return will return.

This will return something in response.

This is going to return a response.

This is also part of via mock dot with status http status dot oc not value dot width header.

This is going to have the headers as well and the header will be content type will be media type, dot

application, json value, dot width, body.

So this is something error that I have defined.

This is going to be here.

Make sure that you are adding at the correct place.

Okay.

So after this.

I need to add that with body.

Now within this body what we need to return.

So whenever we are doing slash product slash one, it generally returns the product response, right?

So that means it will be having a decent format.

So that decent format we are mimicking because we are not actually calling the slash product slash one,

we are just mimicking it.

We are just using the watermark and stubbing those values.

So for that, what I have to do is I have to put that JSON somewhere and that particular JSON I can

return.

So what I will do is I will put that JSON here.

So within the resources I will create one directory as a mock OC, and within this mock directory I

can place those JSON files and yeah, within this mock I can create a file.

Get product dot, JSON and this will have that JSON format.

So let me just grab the JSON format.

You can see that it's nothing complex.

It's just that it's a JSON format and which is going to return product ideas one product name as iPhone

and the quantity as 370 and the price is 200.

Whatever it is, it is a normal thing here.

So this particular product I can use here to call this, so let's just define that.

So yeah, we need to get that particular file and convert those data.

So I'm just defining stream utils dot copy to string.

This string utilizes a static class.

Let me just import statically on demand static.

So copy to string.

Order controller test dot class dot get class loader get resources at stream OC.

And here I can define from where I need to get that stream.

So it's in the mock slash get product, not JSON, OC comma care set dot default care set OC.

And this is going to ask to add the exception.

So let's add that exception.

OC So you can see that it's very simple.

What we did is we did we use the mock server to stop this.

That is the slash product slash one I'm going to call this get request.

And in that get request I've defined like what it will return.

So it's going to return a response with status code http status dot.

Okay.

And within the header it's going to add that content type should be JSON value and within the body it's

going to return a JSON format.

This JSON format I would define in my mock folder, get product JSON, right?

So this JSON is going to return and this JSON is going to return.

So that's why I define that copy copy to string from the order controller test class dot class loader.

So within that class, get all the resources that stream from this file.

It's nothing complex, It's very simple.

And here also it's going to add the exception.

So let's define that so you can see that one method is done, which is going to return me my mock product.

So now let's go ahead and implement others as well.

So now let's define do payment.

Do payment is going to be the post request, right?

So if you go to the payment service and you can see that due payment is there, Right.

So do women is a post mapping and we need to parse the payment request.

So let's define that as well.

And it's going to return the long value.

If you go to the order controller test and go to the do payment method.

So yeah, we can define via mock server dot stub for via mock dot post.

Okay.

I don't need to use this post.

Now, I can actually call this post and I can define you are equals to that is also part of the wire.

And yet I can pass a URL and the URL is slash payment.

Right.

Slash payment.

I want to call this is going to return a response dot with status.

HTTP status.

Dot oc.

Dot value.

Dot width.

Headers and year with headers.

I'm going to copy this thing.

It's going to pass content type as JSON values.

So that's it.

It's going to return white.

So this is all fine now.

Now payment is also done.

Now get payment details now for the payment details.

The same thing.

We need to pass the slash payment and order ID.

So you're also will do via mock server dot stub for and we're going to do the get request and we are

we are going to do the URL matching here.

So you can see that I'm just giving you the different options here.

So how different way we can add the URLs here and here within this you need to pass the URL regex.

So I'm just defining slash payment slash dot star.

So anything after that which will return?

With a response, not with status.

HTTP status.

Dot dot value.

Dot.

With headers and the header is going to be the same.

So let me just copy this right dot with body and within this body it's going to return copy to string

or the test or the controller test dot class, dot class loader, dot get resources that stream.

And here we need to parse the file name from where we need to get so from mock slash.

Get payment, dot JSON.

This is a file that we have not created, but we will create comma default care set.

And this carousel we can statically import and this is going to throw the exception.

So get payment.

JSON is something that we have to create.

So let's go ahead and create the get payment or JSON.

So whenever I'm calling slash payment, the spelling is wrong, so let me correct it.

So whenever I am doing slash payment, slash ID, whatever I'm passing, it will return me this response

back within that response I need to.

See that response as well.

So I'll just create a mock object, create new file.

That is get payment JSON, and I need to add the payment JSON.

And this is my payment and that is payment ID equal to one payment models.

Cash status a success amount is 500.

Payment date is whatever the date I have given and order.

It is one simple thing.

You will get this JSON whenever you are doing a request from the postman.

The same thing that you can add here.

So now if I go to the order controller test back, you can see that this method is created now, which

is going to return me stub for a slash payment slash star.

Now one more method is there for reducing quantity, so let's implement this as well.

So you also will define via mock server dot stub for is going to be the put request and is going to

be URL.

Matching.

And this is going to be slash.

Product.

Slash.

Reduce quantity.

Slash star dot will return a response dot with status.

HTTP status dot.

Okay dot value dot.

With headers, the header is again the same.

Let me just copy content type media type application JSON value and that's it because this is going

to return void.

So you can see that we just implemented for methods that is get product details, response, do payment,

reduce quantity and get payment details.

And all this for methods are implemented.

And you can see these all are doing the wire call and we are stubbing some of the values.

Either if we are using JSON, then we are defining that JSON.

Otherwise we are defining that what body it is going to have in the JSON format.

So you can see all those four methods are then now all these four methods will be called for each and

every test cases whenever is needed.

Now we need to implement this test place order when you are doing a payment.

So let's implement this now.






Transcript - 

So now first we are going to do the place order and whenever we want to place order, what we have to

do is we have to pass on the order request object, right?

So we need to create the order request object.

So let's define order.

Request equals to get mock order request.

This is something that we have to mock.

So that's a particular method.

Let's create it.

Okay, let me add your above itself and let's implement this.

So it's going to return order request that builder build again.

Yeah.

We need to add all the details.

So it's going to how DOT product ID is one dot payment mode as payment mode, dot cash, dot quantity

as ten dot amount as 200.

So this is the default order request.

Now this order request we can pass on when we're doing a request.

So let's do the request now.

So we are going to do the mock MVC request.

That means mock MVC perform.

This is the request that we are going to do and this is going to return mock MVC result.

Sorry.

MVC result.

Yeah.

Okay.

And we are going to do the post request.

Right.

So you can see that this is the part here, Mock MVC request Builder post.

So we need to use this and mock MVC builders.

This is something that I am going to add as a static import.

So we are going to do the post request here for slash order.

Slash.

Place.

Order.

Dot width.

Okay.

Sorry.

We just imported the wrong class here for the post, so let me just correct it.

Okay, So let me just optimize the imports here.

So this post should be from mock MVC request builders.

Okay.

Because we are mocking this dot post and after that dot width we are going to pass some of the values

here.

So here we need to use the JWT.

So you can see that this JWT is from the security mock MVC request post processors.

So this is what something that we are going to use.

And this is something that we can add as the context.

Add on demand, static import, and here we can pass the authorities which authorities that we are going

to use.

And within this we will add new simple granted authority.

And this is going to be the customer because customer can call this right in the actual code.

Also, we have seen that.

Customer will be able to do this content type, media type.

Dot application, JSON value, dot content object mapper dot write value as string from the order request

dot and expect and expect mock MVC result matches.

OC dot status.

Dot is oc dot end.

Return.

So this is the formatting that we have done here.

You can see that it was simple.

It was just a simple call that we did.

We did order request.

Order request equals to get an MVC order.

So this is the method that we have defined here which will get us the order request.

And then with mock MVC result, we are doing the mock MVC perform and here we are doing the post request

on slash order slash place order with JWT authentication token.

So this is the mock JWT.

And within this JWT, we are at the authorities that this JWT will have the authorities of a customer

and with content type as application JSON value and the content that we are passing as the request body,

that is the order request.

So we use the object mapper to convert our object to the string value that is a JSON value.

And in return, what we are expecting after this call is we are returning that the status should be

OC.

OC.

That is a simple thing that we have defined here.

So once this entire thing is done and our request is done, the result will be stored here in the MVC

result.

So at the end of this, what we store is we store the, we pass the data back so that data I can get

from your so I can define this way string order ID because order is something that we are going to get

back so I can define mock sorry MVC result right.

Not get response that get contenders string.

So we are going to get the order here, whatever the order it has been passed.

So that means you can see that we place the order.

So that first part is completed, done here.

So now what we are expecting here is whenever I'm doing slash order, slash place order at the time

with the JWT tokens, we are passing all the requests.

That request will come up with some more requests because once we do this request, it will go towards

the request to reduce the quantity.

So at that time this reduced quantity will come into whichever.

Okay, so this reduced quantity will be called and it will return and it will reduce the quantity and

it will also come in the place that is the do payments or do payment will also be called from the same

thing.

And at the end data will be stored in the database and the entire request will be completed and we will

get the order ID back.

Now, whatever the order ID we get back here, if the same order ID is available in the database, if

we check, that means our whatever the request we did was successful.

Right?

So at this particular time, that data has been stored in the edge to in-memory database as we've added

the edge to in memory.

So now we can call the repository layer with the same order ID and I can get the value like what is

the value of that order?

So let's do that.

So as this order is going to return the option of order, I can define optional of order.

And this order is from the common daily code for order service utility.

So our entity order equals to.

Or the repository.

Find my ID and I'm going to pass the order ID here.

And this order is long, so I'll just pass long dot value off.

Already.

Okay, So at this point, I'll get the order object from the database.

Now, here, I can do the assertions I can define.

Assertions dot assert.

Yeah.

I'll check.

Order.

Dot.

Is present and these positions I can statically import.

So here I've done this session as well.

So whenever I'm getting this order, I'm checking if this order is present or not.

And at this point I will get okay, that order is present.

So I will just define that as a true order is present and from here I can get the order object, order

or equals to order, not get.

So I got the order and I can check the values of the order so I can define assert equals long dot pass.

Long order id should be equal to the order dot get id sorry order get ID.

Right.

So whatever the order it is, that ID and the order ID pass should be same again assert equals.

The status should be placed with the order.

Get status oc assert equals order request dot get total amount should be equal to the order that is

or dot get.

Amount assert equals or the request get quantity should be equal to the order quantity.

So you can see that these are the assertions that I added, like whatever the data have passed, right?

Like whatever the data I got from the order that should be equal to the what order requests that I have

placed.

So all those things should be matched then.

Only I can say that whatever the request I did is successful.

Now you can see that most of the things are done.

I can test this out now, so if I test this out, it will create a context.

It will perform all those operations and it will save the data, get the data, and once everything

is done, data will be wiped out.

So now let's run this.

And you can see that we have some errors.

So let's check what what are the errors?

The error we are getting is media type is not supported.

So let's see what we can do here.

The error we are getting is at or the controller line.

146 So this has to do something with the object mapper data, which I'm trying to understand from here

is is like at line number 146, which means it is not returning the 200 status.

So that means whenever I'm calling this place order, it should create ideally it should call all these

methods as well, right.

Slash product slash one and all those.

So that is something that we need to check.

So if you go ahead and call the reduce quantity, let's check all the services once.

So if we call the.

Be product service, reduced quantity.

We are passing the ID and that is going to be the void request and we are passing ID.

Okay.

That's all fine.

If I go to the payment service.

We are going to pass the payment request.

To do the payment.

Let's check again.

Reduce quantity.

We are doing the wire moc stub.

Stub for put.

Is it a put mapping?

Yes.

Is a put mapping for the reduced quantity.

Let me just copy.

The spellings are correct.

This product service is correct or not.

That also will check.

In our test service instance list supplier.

Okay.

All these are correct.

What else we can do is we can again, go ahead and check here.

So here you can see that I did a mistake because error is shouting at me that the HTTP media type is

not supported.

And here you can see that I did a mistake rather than content type.

I define the content.

Right?

So now let's check this again.

So let me just run this.

And still it feels so now let's see, because this is going to be the different error now.

So here you can see the error we are getting is no such exception.

And if we see that it's from this spring framework response entity and it is a fallback method, right.

Which is expecting longer.

And the fallback method which is expecting here with this circuit breaker here is for the product service

here, right?

Because product service does reduce quantity.

It is calling.

So let's go ahead and check that.

So what I will do, we already have a couple of services, external services that we have created,

right?

Product service and the payment service.

So let's go to the product service and check and.

Here, you can see that this is the method for us.

This is something that we are calling, right, is quantity.

And for that the fallback method defined.

Right?

Following method defined is this one.

And you can see that we define the fallback method for a particular entire class.

But ideally the following matter would be for a particular method that we have defined.

This particular interface has only one method, so that's why we have defined here.

But ideally, if you have multiple methods, you can define the same annotation at a method level,

and for each method you can define the different fallback method that we are getting is because you

can see that the return type is response entity wide for the reduced quantity and for the fallback method.

For this it should have the same return type.

So fallback method is this and you can see that it is having the return type is void, but it should

be response and it is void.

So what I will do, I will just change to response and it is void and we will see the test cases again.

So let me just change this to response entity and let me change to void type.

Okay.

And you can see that after this, right?

If you go to the order service in Biel, after the product service, we are going to call the payment

service as well.

Right?

We are going to do the payment request and we are going to call the payment service payment.

So if you go to the payment service, there is also the same thing, right?

This is the do payment method, which is response entity long as the return type.

And the following method is defined at the class level, not the method level, because we only have

one method available and the fallback method type is void.

So ideally this particular void should also be changed to response entity of type long.

So let's just change this and test our test cases.

So let's go to the test case.

We'll go to the order controller test and let's rerun the test case.

And you can see that test case is completed and it is successful.

So you can see that you need to identify the errors based on the error logs itself.

You need to get the understanding from the logs every time, whatever the error would be from the error

log, you will get the understanding, you will get the information, what is missing and what needs

to be changed.

Once you try to Google it with that, you will get the solution very easily.

So now you can see that we implemented the test cases when we place the order and they do payment is

successful at the time we completed the entire test cases.

Now let's implement the other test case.






Transcript - 
Now let's implement the test case where we are trying to place the order, but with the different authority.

Currently you can see that the payment can only be done by the customer.

Now let's try to do the payment when we are doing the payment when the authorities admin.

So let's just try to create a new method here.

So I'll just create a new method and I'll just define public test when place order.

With.

Wrong axis, then throw 403.

So you can see that based on the method name itself, I'll get to know each and everything what this

particular method is going to do.

And I will annotate with a direct test here.

And this is going to be wide.

Okay, now let's implement this method.

So you are also will do the same thing.

We are going to do the same MVC request to the place order.

But instead of customer we are going to pass the admin.

So what I will do, I will take this entire code from here so we can see all the time because we are

going to do the same request here, right?

This is going to the same request where we are going to pass the order request here and rather than

customer, we are going to pass the admin and we have to add the exception to the method signature.

So let's add that as well.

So now you can see that we added the exception, so the compilation error is gone.

So we are going to do the mock MVC dot perform slash order slash place order with the admin authority

OC earlier we did with the customer order authority that was successful.

And here what changed we have to do is currently you can see that earlier we checked when the status

is okay, but now when we do, status should not be okay.

OC status should be for zero three.

That is forbidden.

So this is the only thing that we have defined that we are expecting that this should be forbidden.

Now, if we run this.

We should be getting the forward.

And as the response and this particular task should be successful.

So let's run this.

And you can see that our test case is successful, right?

So you can see that we were able to implement our test cases.

That is the success scenario as well, where we are doing the complete place order with all the required

details and with the failure scenario as well.

When we are passing the wrong access type, that also is successful.






Transcript - 

Now let's add the another scenario where we will implement the GATT order.

So we'll pass the API to get the order with slash order slash order ID, and based on that, we should

return the or we should get the data back for that particular order.

So let's implement that as well.

So let's do public.

Test.

Sorry.

Public wide test.

Get order.

Ideally, we will be writing when get order.

Right.

So when get order is success.

And we are going to annotate with other test here.

And here we are going to implement.

So here also we have to do this mock MVC request and we have to get the data.

So let's do that.

So here we need mock MVC.

Sorry, not mock MVC, MVC result write MVC result MVC result equals to mock MVC dot perform and here

this is a get API so we will pass get here that is from the.

More can we see request builders dot get and yeah, we need to pass the API so let's pass the earlier.

That is going to be slash order slash order ID so I'm passing one as the order ID here dot.

With.

And we need to pass the JWT token as well here.

JWT authorities.

New.

Simple.

Granted, authority and the authority that we are going to pass is admin because this API can be performed

by admin as well and customer as well.

So whatever we want, we can pass it here.

Dot content.

Dot content type here will define this is media type dot application, JSON value dot.

And expect what we are expecting in return.

So we are expecting we are expecting here that is mock MVC result matches dot status.

Dot is okay and dot and return.

So this is the call that we did here.

We will add the exception in the method signature.

So you can see that we just did the call here to the slash order slash one with the admin authority.

And here we are expecting that this request should be okay and we should get the result.

We also pass the JWT token with the authorities.

Now let's take the actual response.

So we'll just define string.

Actual response equals to MVC, result, dot get response, dot get content as string, and we'll just

do the order.

Order equals to.

And you can see this particular order I am taking from the repository, from the database itself.

So whenever we will do this operation, right, whenever we will do this particular operation, at that

time we are inserting the order, right?

So this test case is inserting the order, and this test case will take that order and check.

So you can see that both are interconnected.

So this particular test case will not run independently, but it will run with the entire context itself.

So whenever we are going to run the entire context, entire class at that time, this is going to run.

Otherwise it is not going to get that object.

Ideally, whenever we are doing or whenever we are running the test cases, we are going to run the

entire context all together.

Not a single, single one.

So here I'm just doing order repository, dot find by ID, dot get.

Okay, So you can see that I am getting the order with order 81.

And let's get the expected response.

Expected response equals to get order response based on the order we pass.

This method is not there.

So we'll create a method and based on the order, we will create the order response and will send back

here.

So let me just create a method here, create method, get order response.

This method is created and we are passing the order.

So here let me just implement this method and this method you can see get order response whenever we

will be doing slash order slash one.

If you go to the service MPL, if you see your get order details.

So whenever you're doing get order details, you are getting the details from the order that is the

order repository.

Then you are getting the details from the product as well.

Then you are getting the details from the payment as well.

So you can see that you have to do those calls as well to get the details back.

Okay.

So this method, whatever I have created.

OC get the response.

We'll try to get the details for the order, plus try to gather details from the product and payment

as well.

And then it will return the entire expected response.

And we will check both of these two things.

So you can see that we are doing the entire integration test with all the details.

We are getting, all the details from all the different microservices that we have created.

Right.

Isn't it cool?

So now let's implement this.

So first what we will need is we will need the object of the payment response.

So if you go to the orders of example, what we are doing is we are getting the payment response and

that payment response we are adding in the payment details as well.

So that is something that we will need from the payment slash order.

So we'll define order response payment details.

Payment details equals to object mapper, dot read value copy to string and this is going to be from

the order.

Controller test dot.

Class dot.

Get class loader.

Get resource stream.

And yes, we need to parse the JSON file that is the mock file.

So if you go here, we have already created the mock file that will return me to get payment.

So this is something that we are going to call.

So I will just define slash mock.

Slash.

Get payment, json, comma, default care set, comma order response, dot payment details, dot class.

Okay.

This error we are getting because this is something that I will need outside, right?

Okay.

So you can see that this entire method we have implemented, that whatever is available in the get payment

or JSON, that is something that we are loading here.

So that particular detail will get here.

Now, the same thing we have to do for the product as well.

So let's do that.

We will do.

Or the response dot product details.

Product details equals to object mapper dot read value.

Copy to string.

And within this we need to do order controller test.

Dot class.

Dot.

Get class loader dot get resources stream.

And.

Yeah, we need to pass slash mock.

Sorry.

I think we need to pass directly mock here because we are getting from the root right class loader itself

so it is directly mock folder slash get product dot json oc comma default care set comma order response

dot product details dot class and we did some mistake here, so let's try to fix it.

So here with read value copy to string order controller test class dot get class loader get resource

stream.

It should start here as well.

We will do the same thing.

Let's fix closing parenthesis and we added the exception so you can see both of them are now implemented.

So with both of these payment details and the product details, we will get the data from the mock that

we have created.

Now we need to create the object of the order response.

So let's do that order response or the response equals to or the response dot builder dot build dot

payment details with payment details, dot product details with the product details that we have added

dot order status we'll get from the order.

Harder not to get our status.

What order date we'll get from the order dot get order date dot amount we will get from the order,

dot get amount and the order id dot order ID we'll get from the order dot get ID and all the details

are there.

Now I can just return object mapper dot write value a string of order response so you can see that entirely.

We converted into string and we are sending back.

So now actual response and expected response we are getting here.

Now we can do the assertion here.

So we'll do assert equals and we need to check with the expected response, with the actual response.

So you can see that the entire method is implemented when get order is successful.

So let's run this.

And it failed miserably.

So now let's check why it failed.

And you can see that whatever are expecting, right.

And in the body we are getting is null.

And you can see that the status is for zero four, actually.

So we are not getting the data order not found for the order ID one.

So this is the response that we are getting, but actually the order is available.

So what we will do is if we run the entire test class, right, rather than individual method, let

me just remove the.

Imports that optimize the imports.

Now, if I run my entire controller class, it should work because my entire class will run in one context

where first this method will run, where we will store the data.

We will create the payment.

So this is the particular thing that's going to run where we are going to create the order.

And here we will try to do the another order which will fail with the admin.

And the third one is it will try to get the order from the database.

At that time we will get that order because it's in the same context.

We are using the two in memory to do all this operation.

And this edge to in memory will be only available within that context itself.

So let's try to run our entire controller test.

And yet you can see that it failed, but it is a different error.

So let's see, The only issue is we got the order, but the expected and actual is different.

So let's see why it is different.

Product.

Ready Quantity.

Okay, so the issue is with the payment status here, right?

So what I will do is I will change the payment status here.

So that is something that we have to do, right.

Because whenever we are sending the data that should be payment status should be successful that we

have to mention that we forgot.

So let's do that.

So what I will do, I will scroll here and whenever we are getting the payment details here, I will

mention this.

That payment details dot set payment status equals to success.

Okay.

Now, if I run this entirely again, let's see.

It should work.

Still, it is failing.

Let's see, what is the difference here?

Now, again, you can see that this is correct.

The only difference I can see here is now for the quantities.

Ideally, if we pass the correct quantity from the object, it should work.

So the only issue we are seeing is that is regarding the data that we are getting.

Right.

So this two data should be ideally equal.

So this is I'm giving you as a homework.

So try to understand this scenario and make sure that whatever the expected and whatever the actual

is that you are getting the correct.

But there is no issue in the code.

The only thing is that the matching is not equal.

So try to fix this from your end.

If you're not able to do, then we will discuss in the discussion forum.



Transcript - 
Now let's add one more test cases where we will add the test cases where we don't have the product itself.

So let's go to the order controller test.

And what I will do is we added when the order is successful, right?

So we will add one more where we don't have the order itself.

So let me just create another method.

Public word test.

When.

Get.

Order!

Order!

Not found.

Okay, So we'll do the test where we are going to pass the order ID that already is not present, so

we should get order not found Exception.

I will annotate this with a test here.

And what we have to do is we have to do the same thing.

We have to call the same API.

Right?

Same order slash one.

So I'm going to copy this just to save over time.

Right.

Because it's the same code.

So this is what we need.

We will add the exception in the method signature.

And what we have.

What we will change here is we already know that we are going to get the order selection because we

are saving the order slash one in the first particular request, right?

First particular test cases.

When do payment is successful, this is where the first order will be added.

And this particular method will try to get that particular order.

And this method should not get the order because we are testing order not found.

So at that point I will pass on another order ID to suppose and what I will expect is rather than is

okay.

Is not found.

Okay.

This is what I'm expecting.

So they should run successful as well.

So let me just run this.

And we should not be getting that order too, because it's not in the database.

So I should be getting success.

So you can see that I'm getting success over here, so you can see that how easy it is to implement

the test cases and how easy we are able to cover all the scenarios that we have added in our actual

application.

So make sure that you go through all these things and understand what particular code that we need to

write and how we can actually do the assertions, how we can do the verifications, how we can verify

a particular matter has been called, how to identify which particular framework to use, like when

to use, when to use market or when to use wire.

Mark all those stuffs and what particular unit testing you should write.

You should write unit testing for all your service layers and business layers and everything, all your

repository layers where you will be mocking all those service layers and repository layers whenever

you're calling the controller layer that is actually getting the request.

And that particular request should be traversed through your different businesses and everything.

So for that actual wire mark, we should write, we should actually write the integration test.

You can see that in the order service, we write the actual integration test, which is actually calling

the different services, which is actually inserting the data in the database.

But all those are within the particular context itself.

This will not impact any of your actual database.

Everything is within the in memory and all those calls are also being marked.

We also marked our service registry itself.

So you can see that we did a lot of things and we understood a lot of different concept as well.

So whenever you are writing your actual units and integration testing in your organization, this is

how you should be writing it.

So this is all about the units Marketo and environment for unit testings and integration testings for

the microservices.





Transcript - 

Now, in this course, what we are going to learn is we are going to learn, first of all, what is

darker.

So we will understand what darker is and what container is.

We are going to understand about each and everything, about the darker and each and everything about

the container.

The next thing we are going to learn is what is the difference between the darker and virtual machine?

Both gives us the similar type of functionality.

Both gives us the functionality to create a virtual machine on top of a physical machine.

So we are going to understand what is the difference between the Docker and Virtual machine and how

they differ in the architecture level and how Docker helps us With respect to the virtual machines,

we are also going to see the Docker installation, how we can install Docker in our system, in Linux,

in Windows and Mac.

So we are going to go through all those particular steps, how we can install Docker in our system and

then we are going to go towards learning all the different commands that are there in the Docker.

So Docker gives us the CLI as well.

With that CLI, we can do a lot of operations with the Docker so we can create the images, we can deploy

the images, we can do all the operations on the containers.

So there are a lot of commands available.

So we will be going through all those main commands and we are going to understand what those commands

does for us and how we can utilize those commands.

We are also going to see how we can debug our containers.

So what are the containers that we create using our images in that particular containers of our application

will be deployed.

So we will see how we can debug those containers.

We will see what happens within that containers and how we can utilize the logs and all those informations

within those containers.

So we are going to go through those as well.

We are also going to see how we can create the Docker files.

There are different steps available when we have to create the Docker files that will allow us to create

the different images.

So we are going to see that as well.

And we are also going to see how we can create our own Docker images and how we can push those images

to the Docker registry.

So all those steps as well, we are going to see, we are also going to see how we can bundle out all

the different microservices to Docker images and how we can deploy all those images in a Docker environment

and how those images will interact with each other, how those images will interact with the database

and the service registry that also we are going to see in this course.

We are also going to see how we can package all those applications using the Git plugin.

So JB is a plugin from the Google which will allow us to combine all these steps, whatever we do as

manual steps included in the MAVEN lifecycle.

So with one command we will be able to deploy, build and push all the images to the registry.

Those also we are going to do within this course.

So you can see that it's going to be very comprehensive course where we will include each and everything

to learn about the Docker and you will be good to go to start working with the Docker.

So whatever would be useful in your day to day activities, day to day job, we will.

We are going to learn all those things.

So at the end of this course you will be able to build your own Docker images for your application and

you should be able to deploy your Docker images as a container in the Docker.

So now let's start with the course.



Transcript - 
So what is Docker?

So Docker is a way to package all our application and all the related libraries and whatever the things

that we can use within our application into a single bundle.

So suppose in any of the application, if you are using Java and with Java you are using MAVEN as well

and all the different libraries and your application, keeping all those things differently, what you

can do is you can create the package of each and everything.

That means your package is created and this will be your Docker image.

So you can see that you are able to package all those application and application dependencies into

a single package, and that is something that you can share along with each and every one or our others

other developers, rather than giving all the things differently.

So that's the basic definition of Docker that it allows us to package all the different aspects of the

application into a single bundle.

That means at the end it's called a container.

Everything will be inside a container.

A container if you know the example of a ship, right ship containers in that one container, you'll

be having a lot of things.

So all these things, okay, all these different different applications, all the different properties

are the different libraries.

All will be bundled together in a container and this container will be running in your Docker.

This will be your entity that will run.

So whenever you create this image, you can see that this image is become portable, which means you

can pick this image and you can go to any machine and you can go ahead and deploy this application that

much portable.

It will be you do not have to worry about all the different configurations and all the different libraries

that will need to be installed in that particular machine.

Everything will be contained in that particular container, in that particular image, and you just

run that particular image.

Everything will be defined there.

Everything will be configured very easily.

Now, where this image will live, right?

This image will live in a container registry.

Now, I will give you the simple example.

We have different maven plug in.

So all this MAVEN plug ins and MAVEN libraries lives where it lives in the MAVEN repository, right?

So everything will be there in the MAVEN repository and whoever wants, they can take those data or

take those libraries from the MAVEN repository.

Similar is with the Docker images.

So all those Docker images, all these Docker images will be there in the Docker registry.

And this Docker registry can be public as well and can be private as well.

What type of registry are there?

So by default, Docker gives the Docker Hub registry that's a public registry where all our images will

live.

But if you want, you can create the private registry as well and within your organization you can create

private and all your Docker images can live there.

And whenever you want to use those images, you can get those images from the Docker registry and you

can build your applications on that.

Now, why this Docker is so much important that also we need to understand.

So let's first understand that.

So before Docker, what was happening is suppose you are working on the two applications, application

one and application two, you are working on two application.

These first application needs Java eight this needs me 13.2 This is just the example and it needs the

Kafka.

Suppose the other application needs Java 11.

It needs me 13.6.

Suppose it needs Python two as well.

And this needs python.

Python three as well.

So it's just the example where you can see that one application is been deployed with this type of tech

stack and the application two is been deployed with this type of tech stack.

So if you're working with both this application, you have to maintain all these versions in your machine.

OC.

You have to configure each and everything.

You have to install Java eight as well.

You have to install Java 11 as well.

And whenever you're working with application one, you have to configure that it will use Java eight

and you have to maintain the Java 11 for the application too.

Same way with me when Python and all those dependencies that you are using in your different application.

But when you are using Docker, what you can do is you can easily create the images of it.

You can create and download the images for Java eight that can be easily configured.

You can have the images for Java 11 that also can be easily configured and these all are within the

Docker environment itself and you can easily access all those data, all those things from your host

machine, that is your system machine and everything, whatever the application that you need will be

there in the Docker environment.

So whenever we'll go through the implementation, you will see how easy it is to do all those steps

using the Docker.

But if you are doing manually, you know that how difficult it is to maintain multiple versions of the

same application, multiple versions of the same library, right?

You always have to go out and change the different environment variables and all those stuffs will happen.

But with Docker, everything will be contained in that particular environment itself and we can use

those Docker images and Docker containers whenever we want.

So it will be very easy when you're working with multiple environments to use Docker.

So let's take one more example in your team there is a new joining game.

Okay, So for New Journey to do all those local setups, to download all the different types of libraries

and different type of SDGs and everything, it will take time, right?

Because you need to configure all these things for the different environments.

But using Docker, what we can do is we can combine or we can package each and everything which is required

for a particular application.

And with the same image, with the same package, everything can be configured easily.

We can create a package, we can add the configurations and each and everything and that we can keep

in one of the repositories, one of the registries, and everyone can take that and the local setup

and everything will be very easy for the developers.

So you can see there are a lot of advantages while working with the Docker.

Now other than development, we do a lot of deployments as well.

We need to move our application to the different environments.

So for that as well, Docker is really helpful.

So let's take the example before Docker.

So before Docker, what you would be doing is whenever you have deployed the application, this application

is deployed and what you will do is you as a developer, as your application is develop, you need to

go ahead and deploy to any of the environment.

What you will do, you will connect with the infrastructure team or whoever is working to provide you

the infrastructure to deploy application, the DevOps team or whatever, where you will tell that,

okay, this is my application that I need to deploy, where I will need the Java version as 11 one,

I will need the Maven as 3.6, I need Python as well as two.

I need react as well, React 16 and all these requirements you will give and you will give the configurations

as well that these are the configurations for Java.

These are the configurations for me and one Python and whatever all these things you will go alongside,

You will also give that this is the application on top that you have to deploy.

So you can see that you are giving a lot of instructions as well, write a lot of instruction to the

infra or the configuration team which is helping you to deploy application and with a lot of instructions

there might be chance like some of the instruction may be misinterpreted or there might be a chance

like we need to go to and from multiple times to make sure that whatever we want, everything is deployed

correctly and everything is configured correctly.

And the same way you can see the same thing can be deployed to multiple environments as well, right?

So for multiple environments, the same thing, if we have to do, we have to make sure that proper

documentation or proper SOP is maintained for this, right?

So if each and everything we have to do in any other environment, we can do the same step.

We can install Java 11 3.1, 3.6, Python two, React 16 all those configuration we can add and then

we can add the applications on top of it.

So you can see that there is a lot of hassle that we have to go through whenever we are deploying everything

manually or we are deploying everything separately.

But what if we are using Docker on top of it?

So whenever we are using Docker, what we can do is we can package each and everything, whatever is

there in a particular Docker image.

So what we can do, we can create the Docker images and in that Docker image we can create a Docker

image.

So in that Docker image will tell, okay, this Docker image will contain Java 11.

With this configuration, we will tell me one, it will how with 3.2 version it will have Python two,

it will have React 16, whatever it is.

On top of that, we will add our application as well.

So you can see that you define each and everything.

So this will be like your configuration file itself.

You will be writing the code for that.

Okay.

So you can see this configuration, entire thing you wrote for your image.

Everything is there and you can work with the infrastructure team or the content DevOps team to make

sure that whatever the Docker image you are creating is according to standards and all the stuff.

And you can see you just created a docker file for it or a darker or a configuration with which we can

create the darker images.

And this darker image is created.

We can keep this darker image in our registry and this particular Docker image wherever we want.

We can directly go ahead, take this image and deploy easily so we do not have to do any configuration,

anything.

By default.

Docker will do everything for us.

It will do all the operations, whatever is defined in this file and everything would be smoothly.

We do not have to worry about what is the documentation or what is the SOP and what steps we need to

follow in everything.

Everything is defined in this file, whatever we want accordingly, and whoever wants, if you want

in your local system, if you want any production, everything can be directly easily be done with one

particular file.

So you can see that there are a lot of hassle has been removed, a lot of burden has been removed whenever

we are working with the Docker.

So you can see that how much pain it will remove from the developer's head while using Docker.

So this was all about what is Docker, why we should use Docker and what benefit it will give us while

using Docker.

Now in the next video we will understand what is actually a Docker container.


Transcript - 

So what is Docker container as we discuss earlier?

Docker container is a package itself, which includes each and everything with all the libraries, with

configuration, with the application.

Everything combined together forms a Docker container and this Docker container will be running in your

Docker environment.

Now how this Docker container will be created that we need to see here.

So Docker container is created on base of a Docker image.

So first of all, there will be always a Docker image.

So we'll be creating a Docker image.

Docker image is something that will consist of each and every configuration in each and everything.

So Docker image will consist of, okay, this particular image will have the different libraries, this

will have different libraries, this will have all the different configurations.

This will have the application containing.

It will have the operating system on top live in the Docker registry.

So this Docker image will live in the Docker registry.

And from the Docker registry, what we can do is whichever environment they want, whichever environment

we want this application to be running in that environment, we can take this Docker image from the

Docker registry and we can have this Docker image here and this Docker image we can run.

And whenever we will run this Docker image that is called container, that will be the container which

will have each and everything, which is a running entity in the Docker environment.

What this will consist of actually you can see that we added everything library configuration, application,

operating system and everything, but what actually it will be created off.

So every docker image is consisted of different layers.

So whatever the things or whatever the application, libraries, configuration, whatever you add in

the Docker image, it creates a different layers.

So suppose let's take the example that you want the Java application to be packaged in your Docker image.

What you will do is I want my application to be deployed in the Linux environment.

Suppose Ubuntu so.

You will tell that.

Okay, I want my application to be on top of Ubuntu, so this is going to be your one layer.

And on top of that you will define on top of that install Java.

This is going to be your second layer on top of that.

After that you will define that.

Okay.

On top of Java, add some of the environment variables plus do some of the configurations.

These are the different steps you will be defining.

So for all those steps there will be different layers.

So this also will be one layer.

On top of that, you mentioned that, okay, this is the application, deploy that application in the

Docker.

So this is going to be your different layer.

Layer three layers for.

So this way you can see this Docker image will consist of different layers.

So whatever you will add on top of it, every time there will be a different layers created and at the

end you will be getting this layer intermediately.

Whenever you will be getting the Docker images, you will get the first layer on top of that.

This operation will happen.

On top of that, this operation will happen accordingly.

All the operations will happen and at the end you will get this layer and you will get the layer ID

as well.

So every layer will have the ID consisting for it.

And whenever you will see all the information for this particular image, you can see that how much,

how many layers it consists of and what are the different ideas and everything.

So we will see that as well, how we can see all those information.

So at the end, Docker image will consist of all the different layers, and those layers combined together

will make a single Docker image and that Docker image we can run and that is going to be called as my

Docker container.

That is a single entity which you which will be running in your Docker environment.

So from here we got to understand that Docker image and container are two different things and how Docker

image will consist the different layers and based on the Docker image, it will create a container.

Now in the next video, let's install Docker in our machine.



Transcript - 
Now to install Docker in your machine, go to your browser and search for Docker installation.

Okay.

And just go to install locker decks.

Stop here if you see if you go to the deck, stop here.

Okay.

This is something that we have to install.

Now, there are two versions of Docker available.

That is the Docker Community Edition and Docker Enterprise edition.

For us, Community Edition is more than enough.

So we will go ahead with the community edition only now if you come here in the Docker Desktop Overview

in the installation part, you can see that you can install in your Mac as well, you can install in

your windows and you can install in your Linux as well for each and everything.

That is a different way to install.

First, let's see how we can install on the Mac.

So if you go here, install on your Mac, these are the options that you will get.

Install Mac with the intel chip and install Mac with the apple chip.

So whatever machine that you are using, you can go ahead and download the package from here.

I'm using the apple chip so I will be downloading here.

So once that is downloaded, it's just a normal installation, just double click on it and it will get

installed in your machine.

Very simple steps.

You have to do nothing.

So you can see that it's just an interactive installation like all the other application.

So double click on it and it will be installed in your machine and you can search with Docker and that

Docker will be open for you.

That's how simple it is to install locker in your Mac.

So you can see that your docker shop is open and it is starting the Docker engine for you.

Now let's see how to install in the windows as well.

So you can see that there is an option installed on Windows.

So let's go there.

So from your install Docker desktop on Windows, you're also you can see there will be a executable

file, just download that executable file and you can easily install.

But with this installation you need to make sure that some of the features of the windows are enabled.

So with Windows, make sure you have to make sure that OCL two backend or Hyper-V backend and Windows

container, either of the one is enabled in your operating system.

And you also need to make sure that virtualization is enabled for your operating system, for your machine.

So if you don't know how to see if you go to virtualization tab here, you will get all the information.

So you need to enable the subsystem for Linux on your Windows machine.

So if you go to the turn Windows features on and off, you can see Virtual machine platform and Windows

subsystem for Linux.

So once you enable these two things, there will be a default update operation will be happening in

your windows and after that update you will need to restart your machine and then you need to install

Docker on top of it.

If you want to add the Hyper-V the same way, you have to go to turn Windows features on or off search

for Hyper-V and you need to enable Hyper-V.

So for Hyper-V also is the same thing.

Once you enable it, it will start installing in your machine and then you need to restart and after

that you need to install Docker.

Once you are done, you have to go to the task manager and check that virtualization is enabled or not.

And from your So if it's enabled, you should be easily be able to install Docker in your machine.

So these are the steps that you need to follow to install Docker in your Windows environment.

Now to install Linux in your Docker environment, there are different steps involved based on the different

distributions that you are using.

So if you come here installation per Linux distro, you can see that there are different distributions

available for all those distributions.

There are different steps available because all the distros use the different repository and the different

commands to do all the operations.

So if you want, if you are working on the Ubuntu and if you want to install docker on Ubuntu, just

go here and you need to follow the steps so you can see that prerequisite is make sure that Docker desktop

is uninstalled first.

So you can see that it's removing the Docker desktop and it is removing all the directories and file

structures and everything created whenever you have installed Docker next.

So once that is done you have to come here and you need to come to this install Docker desktop and you

need to add the repository.

Just check this commands, just follow these commands, just copy paste these commands and run in your

machine and it will be very easily installed in your machine as well.

Okay.

So these are the commands that you need to follow for the Ubuntu.

Similarly, you can go for the federal Debian or Arch Linux so you can see that all the steps are given.

Everything would be very easy to install once the docker is installed in your machine.

Either it would be Mac Windows or Linux, whatever the steps that you have followed.

Once that is done, you can go to your terminal and if you just check the Docker version, Docker hyphen

hyphen version, you should be able to get the Docker version.

Once you get this, then Docker is successfully installed and Docker is correctly installed in your

machine.

OC We are going to use the Docker CLI a lot in this course, so make sure that you are able to access

Docker from the CLA and you should also be able to open your Docker desktop and that also you should

be able to see each and everything correctly alongside Docker.

You can also create the account in Docker Hub.

Docker is a default registry provided by the Docker itself and here you can see a lot of different images.

So if you see here, this is my account daily code buffer and these are my repository, my images OC

which I have created and uploaded in the Docker hub.

But if you go to explore here, you can see the different official Docker images as well and verified

publisher sponsored Oasis.

There are different types of images.

So suppose if you want to work with Alpine from here, you can get those information.

If you want to work with Ubuntu, Python, Redis, whatever it is.

So if you just go with red is here.

So here if you go you can see you will get the complete documentation, different versions available

and how to use this docker image as well.

Everything will be given here.

And with this you should be able to add or get those images in your local environment and you can work

with the Redis as well using the docker.

So so you can see that a simple Docker command is given and all those information like different versions

and everything.

So you can see that all the informations are given here.

We just have to go here.

Whatever we need, we can get those commands.

Everything will be mentioned here.

So this was all about how to install Docker in your machine and what is Docker to make sure that you

create your own account as well.

So whenever we are doing any operations, we can connect to your Docker hub as well.

So that's all in the installation video.

In the next video we are going to see what is the difference between Docker and Virtual Machine.







Transcript - 
Now, what is the difference between Docker and Virtual Machine?

Because both Docker and Virtual Machine provides us the virtualization to use for our applications.

So first, let's understand the different layers that we get for the operating system.

So we have one layer that is the hardware layer, right?

We will be having our hardware.

This is your computer, PC, anything server laptop, this is your hardware layer on top of the hardware

layer.

What it will be, it will be having the operating system kernel your kernel layer.

Now based on the kernel, your operating system would be different.

Operating system will be having the different OS kernels and on top of the kernel and operating system,

you will be having your applications.

These are the different layers, level one, level two and level three.

So these are all the layers.

Now let's understand what different layers are being used by Docker and Virtual machine.

So if we scroll down here, let's create the diagram I just mentioned, hardware OS kernel and application.

Okay.

So I have just created a three layers application with kernel hardware and this is for Docker and this

is for virtual machine.

So what virtual machine will do is virtual Machine will create a layer of application plus OS kernel

on top of your host machine.

So suppose if you have installed windows in your machine, so Windows is your host operating system

installed in your hardware.

So with virtual machine, what you can do is there are different applications that allow us to create

the virtual machine like Oracle Virtual Box.

There is an option from the VMware as well.

So there are different options.

So what it will allow us, it will allow us to install the different operating system altogether.

So suppose if I want to install Linux right on top of the Windows, we can allow if you want to install

Mac on top of the windows, there are options available to install Mac on Windows.

So my point is using VirtualBox, we can do that because it will consist of the OS kernel level as well.

Plus the application level as well.

So this will have the different kernel and operating system.

This will have the different kernel and operating system.

And on top of that it will have the applications as well on top and on top of that it will have its

own applications as well.

So with this two layers, you can see that this is what we can achieve.

We can install any operating system and any applications on top of it.

But what Docker will do is Docker will just use one application layer only.

It will use the same OS kernel level.

And on top of that it will allow us to create the different virtual environment for the different applications.

So suppose if my OS and kernel is Linux, so then we can only use the Docker images.

Those are based on the Linux environment.

So if I want to install Java.

I want to install WordPress.

All these applications.

We can add on top of the OS kernel layer.

It will not contain any operating system and any kernel layer.

It will just run as the application layer on top of the existing OS and kernel based on whatever the

existing kernel would be.

So with this, the size of this images or size of this containers will be very low.

And on the other side, virtual machine will contain the OS kernel and the application everything.

So the size will be huge.

So ideally on the architecture level, this would be the difference, like on which layer our docker

will work and which layer virtual machine will work.

Both works very separately, both of them.

The use case of both of them are also very different as we only need the application layer, we need

only application to have the different versions and everything.

Docker will help us a lot here.

It will also have very less file sizes.

All the image sizes will be very less as well.

So those are very portable and we can move around all those in any of the environments as well.

Now with this difference, I also need to explain to you one thing that is the how different it would

be like suppose this if you take the virtual machine example itself, right?

So your host machine that is that is your main host operating system, right?

Like on top of your hardware, whatever the operating system which you have added, that is your host.

And on top of that, whatever you add using the virtual machine that will be your VM, that VM will

be like, Suppose this is Linux, right?

So this environment is completely isolated with this environment because this will have its own operating

system kernel and all those layers.

This will have its own layer, but we can access this using our host and we can access this using our

virtual machine environment with Docker.

That's the same thing we have the host environment.

This is our host environment.

And within the host environment we can have different containers, right?

These are the different Docker containers we can deploy right now with this.

One thing I need to make you understand is because this is the thing that we are going to use a lot

in the Docker is this container image will have its own IP address, has its own port number like application

will be working on port number.

Suppose this within this container we have the application that is running on Port 88 and within this

container there is an application running on Port 88.

This is possible because this will be isolated to this particular container and this will be isolated

to this particular container.

So both this container can run on port 8080, but for your host machine to access this, it can have

only 188 port number.

Right.

We cannot have multiple port numbers.

So here comes the concept about host port and the Docker port, internal Docker port.

So where we can always go ahead and access the port information, I can define the configuration such

a way that with this container, which port is 8080, I want to access this port 88 from my host port

that is 88.

So you can see that these two ports are the same, but for the container two, it also has port 88 and

I cannot access with port 88 because I have already assigned here.

So here I can define with port 8081 as a host port I can define that with 8081.

I want to use the port 8080 of the container two.

So you can see that both as internal are working on Port 88.

But on my host environment, I have configured in the different port information so I can access both

the application at the same time with the different port information.

So this type of configuration will be doing a lot in our Docker cause as well in our different applications.

So whenever we are doing that, we are going to define this host port and the actual port that application

is using.

So make sure that you understand this host port information and the actual Docker port information.

So this was all about the differences of the Docker and the virtual machine, how at the architecture

level, which particular layer it uses, Docker will use just the application layer, but the visual

version will use the application layer plus the operating system and kernel layer.

So in result, Docker will be having a much lower file sizes and it will be very portable to move around

to deploy on any of the environments.

So with this being said, let's understand the different commands available in the Docker and how to

use those commands.



Transcript - 
So now let's see different commands of Docker.

So what we will do is we will work with one of the images.

So let's go to the Docker hub first.

So let me go to the browser and let me go to the Docker hub here first.

And I will go to the Explorer.

And you can see that I'm getting a lot of images, right?

You can work with any of the images you want for this tutorial.

Let's work with the this image.

We will install Redis using the docker and we will see the different commands around it, how we can

navigate through the different things using the darker and the Docker containers and images, everything.

And we'll also see how we can go inside the containers as well.

So let me just go to the red is here and if you come over here, you can see there is a lot of different

versions available and you will also see the documentation available, like how you can run the different

commands and what are the commands that you can use to install this in your system using Docker.

All this information are available everything, and you can see that by default you will get one command

that is Docker pull is which will download and install or start the container in your local machine.

So what we will do, we'll start with this command that is Docker pull is so we have just copied the

command and we will go to the.

Terminal and make sure that your Docker is working.

Docker is started.

And then what we'll do, we'll just copy this command and we'll run this command.

Pay attention to the screen that what is happening when we do this command.

So we have just added the command that is Docker pull is and we will hit enter and you can see that

it is trying to use the default tag that is the latest by default.

It will always do that and you can see that the next days it is pulling from the library readies and

you can see it pull all the things everything over here and you can see that you got all the different

IDs.

These ideas are nothing but the different layers available we talked about earlier as well, like all

these images will be having different layers, right?

So you can see these are the different layers.

With those layers, it will create a image for us.

And at the end you can see that the status that is downloaded in your image for red is colon lettuce.

This is the tag that is the latest version.

Now your image is downloaded, but it is not running.

So if you want to see what other images are available, you can see using Docker images command.

So with Docker images, you can see that you have one Redis repository with the tag latest.

This is the image ID and what is the size of the image itself.

So this way you can see that whatever the images are there, you can see all those images.

So you can see that we saw two command that is Docker pull command and Docker images command.

Now how to run this image.

So to run this image, we have to use the command that is Docker run.

So let's see this command.

If you want, you can get this Docker run command as well.

So if you go over here and if you scroll down, you will get this command that is Docker and you can

see this Docker run command is also available.

So let's use this similar command itself.

So what I will do is I will use Docker run command and after this command, Docker run, what you can

do is you can give the name of the container you want to create.

So ideally what we'll be doing is we will be using this image to create our container.

We talked earlier as well that images will have all the different layers and everything and with that

particular image we will spin up or we will start our container and that container will be our running

entity.

So now with this image, whatever we have, we need to start our container.

That container will be having red is running inside it and we can access that Redis.

So now with Docker run command over here, after this, I can give the name of the container.

So with Hyphen I have one name, I can give the name of the Redis so I can say read this latest.

This is the name that I give.

Then after that you need to give the image ID with which image ID you want to create the container.

I want to create the container using the Redis repository that is read this image so I can give this

over here.

If you don't, if you just give this over here this way, then it means that it will assume that you

are assuming the latest version available with the tag latest.

So by default, if you don't give anything, that means it's expecting Docker to get or work on the

latest tag.

If you have the different tags available, then you can specify which tag you want to use.

You can add using the colon and you can add the tag as well that I want to use this latest tag.

If you don't want to give this as well, rather than the name and the tag information, you can provide

the image ID as well.

You have two options.

Whatever you want.

You can give the options available and we give the options here.

So this particular command will run Redis for me with the name Redis list and the image it is going

to use is red is call and latest.

So once I run this command, you can see that the container is started.

It started the Redis starting here and Redis version and all these are logs available.

And at the end you can see that it's ready to accept the connections.

Now, if you want to see this container, let's see this.

So let me just open the new tab here and if I see Docker with PS, this will give me the list of containers

available so you can see that this is the container with Redis Colon latest image.

We created the container and that container ID is this and the command it's executing is Docker entry

point search it created 50 seconds ago and the port it is exposed is six three, seven, nine and the

name is red is hyphen latest.

You can see all those information available here.

So from here you will be able to see that.

What are the different containers running now?

You can see the container is running.

But one thing you can see one important thing here is you can see that six, three, seven, nine is

the port exposed.

This is one thing, but this is the port exposed inside your Docker container itself, not from the

host port.

So from the host machine, you are still not able to connect to your address.

You won't be able to connect.

So for that we need to add another extra.

Configuration as well.

So what I want to do is now suppose you want to do some configuration change or anything, whatever

you want to do, right by running this command.

Now this content is running.

Now you want to stop this container or you want to delete this container as well.

So to stop this container, what you have to do is you have to run the command that is Docker stop and

then the container ID or you can give the name of the container, whatever is feasible, either of the

two you can do because that will be the unique ID for a container.

So I'll just give you the container ID.

Look, once I give this, this container will be stopped.

You got this particular ID back and this container is stopped.

Now, if I see Docker piece, you can see that there is no container available.

So the container is stopped, but this container is only stopped, not deleted.

OC If I, if you want to see what are the containers available which are not running, but those are

available in your system, then you can use using the docker PPS command space hyphen.

A hyphen means all.

Once you hit this, you will be able to see all those containers which are still not running currently,

but those are available.

You can see that this was exited 32 seconds ago.

So 32 seconds ago.

We stop this container.

This container will have all these status available like whenever I whenever this container was running,

this container was running with the name Redis hyphen latest.

And it was not exposing any port to the host information.

So this stopped container.

If you want to start it back again, then you can use the command that is Docker start command and you

can give the container ID or the container name.

So just pass this and the container will be started.

If you go ahead with Docker Command, you can see you will be able to see that this container is running.

It started 5 seconds ago.

So now what we will do is we will remove this container.

So let me just remove this Docker piece, not PS Docker stop command, because we need to stop this

command, stop this container first.

So I will copy this container ID.

And I will stop the container.

Now, once the content is stopped, if you want to delete, let me just clear everything out first and

let me just see.

Dr. PS Nothing is available, right?

If I see with Dr. PS I phone a this container is available which is stopped now.

If I want to delete this container I can delete by using Docker item command that is remove command

item and then you can give the container ID or the container name and you can see that you will get

the response and this container will be deleted.

Now if you again see with Docker PS hyphen command, there is no container available, so your container

is deleted, but your image will be still there.

If you see with Docker images, your image is still there.

So let me just clear this and let me just list down the images and what I will do is I will start this

image with port exposure as well.

So let me just run this command again.

Docker run.

Hyphen iPhone name and I will name as this latest OC.

And this should be using the read this column latest as the image OC.

Now what I want is I want to expose the port 6379 was the default port and that was exposed within the

container.

I want to expose that port for my host machine as well.

So my host machine will also be able to access this container because ideally that's the use, right?

That's what we are going to try.

That's what we are going to use Docker for.

So what I will do is I will define that with hyphen p oc hyphen p will allow us to expose the ports.

So what I want to expose is I want to expose 6379 port of the host.

This will be your host port for the container port.

So container port is again 6379 OC.

So this 6379 port is there of the Docker container.

I want to expose the same as 6379 for my host machine.

So from my host machine I will be able to access Redis with 6379 port information.

And what I want is I want to start this docker in the background mode, in the detach mode so I can

do other operations as well.

I don't have to wait until this or if I close this screen as well.

This lettuce should be running so I can define hyphen D to start this in the detached mode.

Once I hit enter, you can see that you got the SSID and your container is started.

Now if I again go and see Docker PS Command, you can see that your container ID, your image ID, your

command that is being used to start at the starting of the container it created 5 seconds ago.

And now you can see that

0.0.0.0637946379 of the container.

So you can see earlier you were not able to see this.

So your port was not exposed for your host system.

Now that port is exposed for your host system for which port that is a63, seven, nine same port.

So all this information, you can see that it's available.

Now, consider one scenario that you have to use the different versions of Redis for the different application

that you are working on currently.

You are using Redis letters, right?

For one of the application, you have the scenario that you want to work with a different version of

Redis for the different application.

So how you should be able to do this.

Currently, if I see with Docker Images Command, you have one that is available that is Redis with

the tag latest OC.

So if I go back to my browser and there are different versions of Red is available, what I want to

do is I want to use a different version.

Suppose I want to use the 6.2.7 version for my different application.

So how to get that?

So to get that, what we can do is let's go to the terminal again.

Let me just create clear this out and let's see.

The image is only these are the images now to get the different version right.

What we will do is we'll just define Docker pull, right?

Is if we just pass this this way, then it will by default take the latest version.

But if we define the specific version with Colon here, that is a tag information.

So the tag I want to use is the version I want to use is 6.2.7.

So let's define 6.2.7.

Once we run this command, this particular version will be pulled from the Docker repository.

So let's wait for that to complete.

First of all, it will check that if that version is available within my local repository or not, if

not, it will fetch from the repository and you can see that it was pulling this version and all the

different layers pull completed and this is the digest version and downloaded the newer image for ready

6.2.7 OC.

Now if I see Docker images, you can see that there is a two ready version available.

One that is is the latest one that is easy.

6.2.7 Both is having the different image ID, so both of these images are of the different versions.

Currently I only have the one container running now.

If I want to run this redis as well, I can do the same thing.

I will use the same command here.

This is the command OC docker run hyphen hyphen name that is lettuce.

So what I will do rather than.

Latest I will define red is all virgin OC.

And the tag information I have to use is rather than latest.

I have to use 6.2.7.

This is the reddest image that I'm going to use to start this container.

Now, this container is will always by default, start on 637, nine port information.

Now for my host information, my host, I have already exposed 6379 for my host.

The same I cannot use multiple times.

Right because that port is already used.

So I have to change my host port.

I can change to 78 suppose 6378 but my container port is still 6379 because in my container docker container

this will still work on six six, seven nine.

But for that port for my host machine I'm exposing 637, eight.

If I run, this container should be getting started for this as well.

So if I run this so you can see we got the digest.

Now let's clear the screen and see the Docker containers.

So to see the Docker containers will again do Docker command and you can see that these are the two

containers.

This is the first container ID with the image that is 6.2.7 with the command 13 seconds ago it was created

and you can see the port information.

The port information that is exposed is 637846379.

That is a Docker container port.

And you can see that this is a different container ID still running the is on the different port.

So you can see at the same time I'm able to run two different versions of this onto different port information.

You can see how easily I was able to do this right.

Just the one command I was able to start both of them.

Now with your application, you will be able to easily connect this.

So you can see that we saw a lot of commands, right?

We saw the Docker pull command, we saw the Docker run command, we saw Docker Images Command, we saw

Docker Command as well, and we saw Docker stop Command.

We saw Docker start command as well.

So you can see we saw a lot of commands and these are the commands that you will be using day to day.

Now if you want to see more commands, what all the different commands do, you can always go ahead

with Docker Command and with hyphen F and help.

You can get more information so you can see with hyphen F and help you will get the complete manual

like for the Docker main command.

How many commands are available and for a specific command as well?

You can do the same thing.

If I clear this out and if I see Docker run command and I want to get the help for Docker run command,

I can get.

So for this Docker run command, these are the different options that you can pass to do the different

operations.

So all this information are available, so you can always go to this particular manual and you can get

the information like what particular command will do, what are the different options that you can get

to do all those operations.

So it's really easy to go through the different Docker commands, and these are the basics that you

will be using.

Now in the next video, we will understand how to debug our Docker images and containers.






Transcript - 
Now let's see how we can debug our containers and images.

If you see Docker images here.

We have all the Docker images and if you want to see all the stale images as well, you can get using

the Docker images hyphen the same way that you used to do for your containers, you will get all the

images.

Now suppose you want to see what all the things are there for this particular image for Docker.

6.2.7.

What are the configurations and environment variables and everything directly?

So for that you can use the command Docker inspect and you can pass this image ID here.

Okay.

And once you do this, you'll be able to see all the information for this image.

So this is the JSON format that you will get and you will get to see all those details.

So you can see that the idea of this image is this.

This is the wrapper tags.

This is the digest information.

Is there any parent for it when it was created?

What is the container created for this particular image?

That also that information will also get you will get the information about what environment variables

are set for that particular image.

You will get to see what are the commands added, like which commands will run when the container should

start for this particular image?

Is there any volumes attached?

Okay.

What is a working directory?

What are the entry point commands?

All this information you can see which docker version is there, who is the author configurations,

All this information is there and from here you will be able to easily identify what's going on for

this image.

Now let me just clear this and let me go to the Docker containers.

The same thing you can do for your containers as well.

So if you go to Docker inspect.

And use any container ID to inspect that container.

So you can see you are getting the same information here.

But this information will be particularly for your containers and or the images.

So these are all the information.

You can see that this is the container ID when it was created, the part where the entry point command

has been added.

What are the arguments?

What is the state of the container?

Currently, it's running right when it was started.

What is the image used?

And you will also get the information regarding the IP address.

You can see what are the port information?

Port bindings 6379 port is bind to the host port 6371 and all this information you will get from this

particular command, you will also get the IP address and everything here.

So if you want you can explore this command and this command will be helpful when you want to write

your bash scripts, if you want to get the particular information and based on those information, if

you want to do some operations, you can always do that.

So these are the you can see the IP address and everything, right?

So these are the two commands that you can use to inspect your images and containers.

Now let's see how to see the logs.

So if I have the containers available, right.

So now if you want to see what happened or what logs are trailing for this particular any container,

right.

Because in any of the container you will be deploying your application and that application will be

constantly spitting out the logs.

Right.

So if you want to see the logs, you can use the command docker logs and then you can pass the container

ID.

Suppose I'm passing the first container ID and with this you can see the logs available currently for

this there are only this much logs available so you can see this much log.

So whatever logs are there, all those logs you will be able to see for a particular container.

So these are the logs available.

So with the logs command, you just pass the container ID and you will be able to see the logs for that.

Let me clear it again and let me just list down again the containers.

Now, suppose with the logs you are still not able to identify if there is any issue or anything that

you are trying to understand.

So what you want to do is what ideal thing we would do is we will have to go inside the container and

see what is happening.

Right?

So to go inside the container, the command is Docker exec.

That means you are trying to execute any of the commands within that container itself.

So this is my container, right?

I want to log in or I want to go inside that container and then I want to execute some commands.

Those are Linux or whatever the commands, and you want to do some operations on it.

So.

So once I go in.

So what I want to do is once within this container, I want to log in to this container.

I want to go inside this container and I want to start or I want to log into the radius inside it.

So the command is Docker exec, Docker space exec and then hyphen i.t.

This means that you want the interactive mode to run the run the commands.

So with this you will be able to access the shell of the particular container and with that will be

able to access the different commands.

Now, after this, we need to specify which container you are trying to access so we can give the name

or we can give the container ID.

So let me give the container ID.

And after this container ID, what command do you want to run for this container?

We need to pass the command slash bin.

Slash.

As such, I want to run.

And you can see that your cursor is changed.

So now you are inside the container.

So if I just do ls here.

If I do just p, you can see that you are inside slash data folder.

If I just go back here and if I do ls You can see these are all the folders of the container itself.

So I'm inside my container Now, if I want to log into my Redis, I can do using the radical command.

You can see that I just logged into the redis inside my container and I can do all the commands keys

star, if I want to check all the keys in anything.

So you can see that how easily I was able to go inside my container as well.

And within container I can do all the different operations as well.

If I just want to come out of it, I will just use exit command to get out.

Same way.

Exit command to get out.

And if I do ls here you can see these are the different folders of my host machine.

So this is a difference.

This is my host machine.

And this was your container.

So this way you should be able to go inside your container to run the commands.

Also, you need to make sure that different container uses different shells.

So currently this Redis is using SSH.

Some may be using XSS, some may be using bash as well.

So accordingly you just need to identify if it's not working, try with Bash or XSS, it will work and

you one shell will be available for sure.

So this way you will be able to get inside your containers as well.

So you can see we went through a lot of commands and we saw the different ways to debug and inspect

our images and containers as well.






Transcript - 

Now, in this section, let's understand how to clear out everything.

If you have multiple containers running or if you have multiple images and everything, if you want

to clear it out, everything.

If you want to clear your Docker environment, how we can do this.

So let me just clear this out first and if I see Docker images currently you have two Docker images,

and if I see Docker as command hyphen A, you can see that you have multiple containers also available.

So one thing I will show you is if you want to clear everything, you need to make sure that you're

all the containers and everything are stopped.

So if I just do Docker piece, not PS sorry, stop and I can pass both the containers.

This is the first container.

And this is the second container I can pass with space.

And now both the container will be stopped.

Okay, So you should be able to stop multiple containers this way.

Now, if I just do Docker as no container is available, Docker p, c1a all the containers are there.

Suppose consider you how many containers and deleting with item command.

One by one is also not feasible.

And if you want to delete the docker images, let me just clear it out and let me do let me do Docker

images command.

And if you want to delete the images, the command is Docker RMI.

For containers, it is just RAM, but for images it is my command.

And then you can pass the image ID or the image name call and tag name to delete that particular images.

So that command is also something that you can use.

If not, if there are multiple images and everything.

If you want to clear each and everything by once, there are different commands available inside the

Docker system.

So within this Docker system, there is a command that is prune.

This will prune everything you need to make sure that nothing important is available.

If you have to take backup, you can always take the backups and everything, but this will delete each

and everything for you.

So if you just pass docker system, prune hyphen name means everything and just hit run.

You can see that you will get all this warning here.

This will remove all stopped containers, all network not used by at least one container will be removed

all images without at least one container associated to them and all build cache.

Everything will be cleared if you run this command.

If you just do y you can see that everything is removed now your space is claimed back.

And if you clear this now and if you see Docker images hyphen, a new image is available.

If you see Docker WPS hyphen, a no container container is available.

So everything is clear and now your Docker environment is fresh now.

So you can again do all the operations on it.

So why I showed you this is because we are going to work on the Docker and there might be times that

we did something wrong or anything for just trying out some different things, right?

So make sure that you can clear all those things this way.

So this is the handy command as well.






Transcript - 
Now let's talk about how to create our own images, because currently, until now, we saw that we were

using the different Docker images available in the repository.

Now, how to create our own Docker images using the Docker file for our applications.

So earlier I mentioned that for every Docker images there will be a Docker file and with those Docker

files we can create our images.

So if I just show you for the Redis as well, we use the Redis, right?

So we use this latest version, right?

So if you go to the latest version here, so for this latest version as well, you can see there is

a Docker file where all the different commands are mentioned like create this Docker image from the

Debian run this command set this environment variables, then run this again commands, then set the

different environment variables, run this commands and you can see all those different steps are mentioned

here and all these different steps are the different layers added to it.

And at the end you will get one layer which will be having Redis installed on your system and you will

be able to easily access Redis.

So this way you can see that with a docker file, your images are created and those images you will

be able to run as a container.

So for our application as well, whatever the application that we are going to create or we had created

for that.

Also, if you want to create the images that can directly run, that means we have to create our Docker

files, right?

So let's see how we can create the Docker files and how we can run those Docker files to create our

images.

And with those images how we can run our containers.

So for that, let's go to the IntelliJ idea and we have different applications available.

There is a Cloud Gateway config Server Order Service Payment Service, Product Service and Service Service

Registry.

These are the different applications that we have now for this application.

We want to create our Docker images, all the different Docker images for the different applications.

And within that Docker image, I want to bundle my application and whenever I start my container, my

application should also start.

That's what something I want to do.

So what I will do is let's start with the service registry first.

So within the service registry, let me just close all this.

Files here.

So within the service registry, you can see that we have the application.

This application will be running.

On board.

Let me just see.

On board.

Eight, seven, six, one.

This is nothing but our service registry, where all the services will come to connect.

Now, this application, we need to create the image of Docker image.

So for this we need to have a Docker file, right?

So let's create a Docker file.

So within this root directory itself, I'll create a new file and I will mention this as a docker file.

Simple Doc, your file and your Docker file will be created.

Now here we can add the different commands.

So if you see over here, these are the different command, right?

Similarly, we will add the commands here.

So what I want to do is I want to use the image.

That image will be based on the Linux and it has a Java installed in it.

So that is something that I want to use.

So I will define here from my base image.

So my base image I want to use is open JDK.

Okay, open JDK and the version I want to use is 11.

So you can see that one step is done, which is defined that which base version I want to use.

Ideally, you always have to make sure that whatever the command that you write here and whatever the

base image and anything that you use, you always make sure that your image size is as low as possible.

So ideally at that time I've seen that people are tend to use the Alpine OS.

Alpine is a Linux based distribution which is very lightweight.

And on top of that you will add the different applications that you want to use.

It will not come with all the basic applications and anything, it will be just a barebones Linux.

And on top of that you will be adding whatever you need.

But here we are using open JDK because I want my Linux layer on top of that.

I want the Java installed with all the different things so I do not have to worry about each and everything.

Now, once this is done, what I want to do is I want to add my application, my jar file.

Okay, so service registry file will be created right whenever we build our application.

That jar file I want to copy inside my container or inside my image and that jar file.

I want to start when I start my container, I hope you get the idea.

So if I open this in the terminal.

Okay, let's let me open this in terminal.

And if I do make one clean install here.

Look, you can see that the build is successful and when the build is successful inside the target directory.

You can see that your profile is created, right?

So this is a jar file that we use to start our application.

So whenever we will run this jar file, your application will start, your Tomcat will start and on

top of Tomcat your application will be deployed.

This is how generally it works.

So what I have to do is I have to get this file and add in my image.

So for that let's do that.

So what I will do is I will refer this file in one of the argument, in one of the variable, and that

variable I will use to copy the file.

So I'll define argument as RG and I will define the name that is jar file.

Jar file equals to what is my jar file.

Jar file is inside my target directory slash I want star jar any jar file available.

We have one jar file available so that jar file will be picked here.

Once I pick this jar file I want to copy, so I'll use the copy command to copy this variable jar file

so I can refer refer this variable with dollar braces, jar file.

I want to copy this file space.

That means this is going to be your source after space.

You need to pass the destination.

So destination I want to pass as service registry dot jar.

So I want to copy this file that is service registry hyphen zero or 0.1 hyphen snapshot jar to my image

with the name service registry jar.

Simple thing.

So I just have renamed my jar file here.

Then once I copy this, I want to start this jar file.

Now, to start this jar file, I had to pass the command that is Java hyphen jar and the name of the

jar.

That is the command that we use to run our jar file.

And that command I want to run when my container starts.

So whenever you want to give any command at the time of starting your container last command, that

command you will give as the entry point.

If you want to run any other commands you can give using the CMD command that will be run when you are

building the images.

But while starting your container we will define entry point.

This will be used when your container is started.

So at that time I want to pass multiple commands.

So I want to pass Java command, then hyphen jar.

Then I want to go inside the directory working directory and I want to start this service registry.

Char So my java hyphen jar slash service registry jar will be called when the container starts.

And when my container is started, I want to expose that port as well, like which port I want to expose

so my host system can access that port.

So this particular application will be running on which port eight, seven, six one.

So I will be exposing that port.

So expose port which 18761.

So you can see that your Docker file is created, so you can see how simple it is to create your Docker

file.

And what we did is we use the base version vs image, like which image I want to use.

I want to use open JDK 11.

My application is also going to run on Java 11.

That's why and I have a jar file available that jar file directly I have to use.

So I will just define that.

This jar file I want to use.

Then I define that copy this jar file inside my image and then I define that entry point command.

So whenever my container starts which command I want to run, I define that.

I want to run Java Event Jar Service Registry Jar, and then I expose the code.

Now what I have to do is I have to build this image.

You just define the Docker file.

Now, with this Docker file, you have to build your image.

So to build your image, let me just clear this.

Okay.

So to build your image, the command is Docker build.

Now, with this command, your Docker will build the image based on the Docker file that you mentioned.

Now, if you're seeing that every Docker image will be having its repository name and the tag version.

So what I would give is if I go to my Docker up, I'll just show you your.

So if I go to my now, you can see that these are the direct images repository.

And because these are official images for that there is a permission to do that.

But for our, our purposes, if I go to my repository, I already have a few of the images created.

You can see there's other different images.

So for everything you can see, there is a repository name that is daily code, buffer slash.

Then the image name itself, this is how it is mentioned.

So similarly, I have to do the same thing.

So what I will do, I will mention this way and make sure everything is in the smaller case.

So I'll just mention daily code buffer.

This is my repository name Slash.

I have to give the image name.

Image name I want to give is the service registry.

So I'll just mention Service registry and then I can give the tag information like what is the version

of it?

So my version is 0.0.1 first version, so I'll just define 0.0.1 clear.

So Docker build then the repository, name, slash, image, name, call and tag information, and then

you need to give the file information.

So what Docker will do is Docker will by default search for the Docker file mentioned.

And for that you just need to mention the directory as I mean the same directory I just mentioned.

Dart that from the current directory, I want to find the Docker file and build the image for me.

Cool, right?

So I'll just run this command and there is a matter.

Yeah, because we did not define the tag information.

Right.

So this is the mistake that I did.

Don't worry.

So I need to define hyphen tx that this is the tag I am using.

D code buffer slash service registry column 0.0.1 and from my current directory.

Now if I run this, it should work.

And currently you can see that it started building and it will add the different layers and it will

combine everything and one image will be created for us.

So you can see that all the different steps are going on.

These are the different layers.

You can see that and you can see it.

It is showing these different steps as well that which step it is performing.

So first time it will take some time, but from the rest of the time it will be very fast.

Cool.

And you can see that everything is done and your image is created.

So if I go to my terminal again and if I see Docker images.

I can see that the image is created daily code buffer slash service registry tag is 0.0.1.

The image ID created 17 seconds ago and this is the size.

It's cool, right?

So you can see that how easily I was able to create the image out of my docker file.






Transcript - 

Now let's see how we can run this.

So to run this Docker image, we have to again use the Docker run command.

Currently, if I see there is no containers available, so I want to run this.

So my command would be Docker run.

Then I want to run in the dash mode and I want to pass the port information.

So my port would be eight, seven, six one for my host and 8761 for my Docker container because it

turns six.

One is the port in which my application is running.

Cool.

Right after that I need to give the name of the container, so I'm just giving service registry.

So once this is done service registry, we have defined and then I need to give the image name, which

image name I want to use, or you can use the image ID as well.

So let me just give you the image ID, which image ID I want to use.

So with this command I should be able to create my container and I should be able to access my application.

So let's run this and you can see that we got the digest.

SSH And if you go and see Docker piece, you can see that one container is available.

This is the container ID, this is the image and this is the command that we use Java hyphen jar.

And it was created 8 seconds ago.

This is the port information and this is the name of the container.

So let's go to the browser and let's see if it is working or not.

So we'll go to the local host Colon eight, seven, six one.

And it's not working because my spelling is wrong.

You can see that your application is up and running.

Cool, right?

So you can see that how easily we were able to tokenize our application, create the image out of it,

and create the container out of our application.

Super cool.

Right now let's go ahead and do for our other application as well.

That is the config server.







Transcript - 
So let's go to the idea.

And what I will do is I will copy this file and we will go to our config server.

And let's piece this file.

As a docker file.

Cool.

Right.

So within this config server as well, what I need is I want the open JDK, I want the same file, but

my file name would be config server.

Okay.

And the same thing.

This is also going to be my config server and I will see that which port this config server is exposing.

So if I go to the CRC main resources and application dot YAML and this is using port 9296.

So let's change that.

I will be using port 9296.

Cool.

Right.

So you can see that my another docker file is also created so now I can go ahead and build the image

out of it.

So I will what I'll do, I will create a new terminal open a new terminal here.

Okay at this point, and I will do my green clean install because I need the jar file.

Right.

Okay, so my jar file is created.

Let me just clear the console here.

Now I can build my image.

So what I will do, Docker will command again with hyphen tx tag.

What is the tag I want to do?

That is daily code buffer slash config server.

Collin 0.0.1 OC.

Now you can always add the multiple tags to a particular image.

So suppose let me give the multiple tag.

I also want to give the latest tech for this, so I'll just define another hyphen dx and I'll just define

daily code buffer slash.

Config server.

Colon latest.

And then I need to give the current path.

Cool.

Right.

So with this I will be able to create the image of the config server.

So let's run this command.

So you can see that the command is started and it will be very fast to create our image.

So you can see that the image is created because our open JDK image was already downloaded.

And on top of that only we just copied our config server to our image now.

So if I go back to my terminal now and if I clear this out and if I see Docker images now, you can

see that I'm able to see two images.

You can see that daily code buffer config server that is the 0.0.1 daily code buffer config server that

is latest and both have them is having the same image ID.

Right.

So that means with one image I have the two tag information.

Cool.

Right.

So now with this image, let's try to create our container.

So let me just do Docker run command with hyphen dx in the detached mode.

Hyphen p that is a port information.

Port information is 9296.

Right?

This is the one that I need to expose.

That is 9296.

We just define the port information.

We can define the name as well that this is the config server and then I can define the image ID as

well.

This is the image I want to use.

Cool.

Right.

So now let's run this command to start our container and we got the digest message.

So my container should be started.

So if I go ahead and see Docker as.

You can see my container is started.

So that means what it should happen is my service registry now should be able to connect to my config

server as well.

So if I go to my browser and if I refresh so you can see that I'm not able to connect my config server

to my service registry.

So let's see what is the issue.

So now if I go back and if I want to see the issue, what is the thing that we have to do?

We have to use the Docker logs command, right?

So I will just do Docker logs and I will pass the container ID.

So this is the container I want to see the logs for.

That is the config server.

And you can see you are able to see the logs.

So if you scroll up.

And I can see that.

There is a connection refused.

So it's not able to connect to my service registry OC.

Okay.

So here you can see that it is trying to connect to the local host.

Right.

And now we need to understand the important concept about the networking in Docker.

So the network that has been created within the Docker environment is different than you have for your

host environment, right?

Because obviously it will be very different.

So we need to handle this scenario as well that whatever the local host that we are passing.

So internally, whatever your application is right, that application should be able to connect to your

Docker.

Also because your service registry is also running on Docker and a config server is also running on

your Docker.

Both are not running on your local host.

So with local host they will not be able to connect internally, right?

So we need to provide a configuration for that that both internally should be able to connect.

Right?

So for that we have to use the Docker host.

So for that, Docker allows us to use the host dot Docker dot internal.

So with this, if you pass that information, it will be able to connect.

So rather than passing local host if we passed host dot Docker, dot internal, it will be able to connect.

So let's use that.

So what I will do is I will just stop this container and I will remove it.

So let me just do that Docker piece.

Let me just clear it so it will be very clear Docker and I will just do Docker stop command and I will

pass the container ID to stop the container.

And I will.

Remove this container as well with item command.

OC.

This content is also deleted.

So if I just see Dr. PS I only one content available.

Let me clear this.

Let me see Docker images.

Now I want to run this config server again with the latest configuration.

So if I go back to my IntelliJ idea and if I go to the config server and if I go to the application

YAML file, if I scroll down here, you can see that I have defined one thing that this config server

should go to connect to localhost 8761 when the environment variable is not available.

So what I will do is whenever I'm starting my container it will not be able to connect to localhost.

Right.

Because we need to parse the docker host.

So what I will do is whenever I'm starting my container, I will pass this Eureka server address as

the environment variable, and in that environment variable I will pass the correct URL so that it will

be able to connect.

Got it.

Right.

Why we're doing that is because with that environment variable, wherever we want to deploy, we just

change the value.

Whenever we are running the command and everything would work easily.

We do not have to change our code.

So that's why we have defined the variable as well here, that if this variable is available, use that,

otherwise use the local host.

So for our higher environments we'll always pass this and moment variable whenever we are running our

containers.

So let's define that.

So what I will do is I will go back, I will use the same command again.

So let me just go back to the command OC Docker run hyphen dx 9296 the name and everything.

And what I will do is I will define the enrollment variable with hyphen E and the name is Utica Server

Address.

So I'll just define Utica server address and then I will pass the value and the value.

I'll pass this, I'll copy this OC.

And I want to remove this local host.

Instead of local host.

I need to use host internal OC so I will just pass.

Host dot Docker dot internal colon 8761 slash Eureka.

The name of the container is config server and the email that we are using is fc fi.

c595804.

That's the same one OC.

So you can see that we are passing an environment variable.

So with this environment variable, that particular instance URL that we have mentioned here, right,

this will be replaced default zone and it will be able to connect to my service registry.

OC So let's run this now.

Okay.

So you can see that we were able to run this.

If you go ahead and see Docker piece, container container is available.

And if you see with Docker logs here to see what is happening with this container and you can see that.

It is able to connect to my registry that we are getting status to zero four.

So if you go here and if I refresh.

You can see that your config server is visible now, so your config server is now able to connect to

my service registry as well within as that on the Docker environment itself.

Cool.

Right.

So you can see that now we are able to connect our applications.

Now let's do the same thing for our Cloud Gateway as well.







Transcript - 
So what I will do is I will go to my IntelliJ idea again and I will just copy this Docker file from

your and I will add in my Cloud Gateway.

So if I come here in the Docker file here for the Cloud Gateway, I have to change to Cloud Gateway.

Look, that's the OC Cloud Gate Gateway, and I'll just copy this name here as well.

And Cloud Gateway is running on Port 9090.

So that is something that I will expose.

So you can see that my Docker file is ready.

So what I will do, I will just open this in my terminal now.

And I will do Docker.

Not Docker and VN clean install.

I just create the jar file.

Okay.

So you can see that your build is completed.

Let me just clear this.

Now, let's create the image out of it.

So I'll just use Docker build hyphen dt to add the tag daily code buffer as the repository slash cloud

gateway.

Right.

I will just pass the latest version.

Not this way.

Cullen Right.

You can see that.

I just added the tag information, and then I will pass the current directory to use my Docker file.

Cool.

Right.

So you can see that with this, I'll be able to create my Cloud Gateway image as well.

And we'll wait for a couple of seconds to complete the process.

And you can see that your image is created.

Now, if I go back to my terminal.

Let me clear this everything.

And if I see Docker images go.

All right, you can see that now.

My cloud gateways are also available with the latest tag.

Now we need to run this as well.

So if I go here right to my IntelliJ idea and if I open the RC main resources and application or YAML

file.

So also you can see that it is in the application yaml file of the cloud gateway.

It is connect trying to connect to the local host.

Right.

But within the docker as we see earlier, it won't be able to connect to the local host environment.

Right.

So we need to handle this scenario as well.

For the cloud gateway, it is coming from the config server, right?

So what we will do is we will need to add the enrollment variable for this as well.

So let me go ahead and do that.

So what I will do is I will add the dollar curly braces and I will pass local host here.

And before that, I'll pass.

Config server.

U r l.

So when this enrollment variable is available, it will use this, otherwise it will use local host.

So whenever we'll be starting our Cloud gateway, we will pass the URL of the config server.

So now you can see that we modified the changes.

So that means we need to rebuild and recreate our image.

Right?

So in the Cloud gateway we will do an VN clean install to create our.

Jar file and you can see that build a successful let me clear it out so in my jar file is created now

I have to build it.

So I will use the same command again.

Docker build hyphen liquid buffer slash cloud, gateway call and latest dot to create my image again.

So we'll wait for a couple of seconds to create the image and the image is created.

Now if I go again back to my terminal, if I do again Docker images, you can see that it created 7

seconds ago and you can see that this is a new image and previous image.

You can see that it is marked as none.

You can see that this is the same image.

I write this and say this.

You can see that new image was created and that new image got the name and the latest tag and the old

image is still available with everything else.

Mark Everything marked as none.

So whenever you will see that anything is marked as none and none, that means a new image was created

on top of it.

And this is pointing to nothing now.

So now what we will do is we are going to use the earlier command that we use to create the config server

container.

So let me just use this so you can see that this is the command that we used.

So I will be doing some changes.

I will name this as a cloud gateway.

Okay.

And then I will give the.

Image ID this one here.

Other than that, I will be needing this environment variable.

I will change the port information because this is running on port 9090.

Look when any moment variable we have said that is for Yureka server address.

We also need to set one more environment variable because this we mentioned right currently.

In the Cloud Gateway.

This is what we mentioned, config server URL.

So this also we will pass as the environment variable.

So we will pass hyphen e again for the enrollment variable.

We'll pass the config server url equals to.

Now whenever internally they have to connect, we have to pass the internal local host.

So we'll mention host.

Dot, dot, dot.

Internal.

So this is one and one environment variable that is the config server, URL, host, Docker internal,

and then the other environment variable is your server address.

And we pass the URL and the name is Cloud Gateway.

And the image that we are using is this.

Now let's run this and you can see that we got the image here.

And if I go.

Let me just clear this.

And if I go to Dr. P's command to see all the containers, you can see that the cloud container is created.

And if I go to the browser, let's see if it's connected or not.

And you can see that it is able to connect.

So you can see that now it is able to get the information of the both the environment variables that

we have set.

And it is taking the configuration from the config server, whatever we have mentioned, because in

the API gateway we have not mentioned the configuration of the service registry.

If you see here, right, if in the Cloud Gateway we have only mentioned the configuration of the config

server, not for the Yureka server that is coming from the actual config server, whatever we mentioned

in the GitHub.

So you can see that everything is coming together, right?

All the applications are connecting.

So now if you have to add any other application, you have to do the same way that you need to create

the Docker file, You need to create the Docker image based on the Docker file.

And whenever you want to run, you need to run the command, you need to pass the proper port information,

which particular ports you need to expose, and you need to pass the proper environment variable as

well, like which particular environment variable you want to use to connect.

Now for the rest of the application it will be the homework for you to create a Docker file and to build

the images and build the containers of it and to identify which variable it will need and to pass the

environment variable as well.

Cool, right.






Transcript - 


Now let's try to understand how to push these images to the registry.

So currently you can see that.

Let me just clear this out.

And if I so if I see Docker images, you can see all these images are available.

But all these images are available within your local repository.

These images are not available in your public repository.

That is where you can publish those images and anyone else can use it.

They can take those images and they can work on it.

So for that, what you have to do is you have to go to the Docker hub that is the public repository

available and you can create your account.

I created the account for me and I have multiple images also available for you.

It will be blank.

Now you can see that there are different repositories available altogether, there are different downloads

and all those options also available.

Now what I want to do is I want to publish this repository that is this image that is cloud gateway

to the Docker hub.

So for that, what I have to do is I have to first log in first of all.

Right, So, so you can see that you will be having the account, right?

So you need to first log in and then you need to push the images.

So to log into your Docker using the command line, what you have to do is you have to run the command

that is Docker.

Login command.

Okay.

And once you hit enter, it will ask you or it will prompt you for your username and password.

You can add your credentials and you will be able to log in.

But if you want to pass the argument, you can pass hyphen u as the username, whatever the username

is.

And after that hyphen p with your password as well that also you can pass to login.

So once you are logged in it will show that you have successfully logged in and then you will be able

to push your images.

So currently let's try to push this cloud gateway image.

So if you see here, there is no cloud Gateway.

It's a different cloud Gateway Cloud hyphen Gateway, which was added a year ago.

Currently, we are going to add this fresh without hyphen.

So here the command is Docker push and then you need to give the repository information.

So I'll just copy this.

OC Docker push Ellicott Buffer slash Cloud Gateway Coghlan Latest OC.

This is what I want to push, so I'll just run this command.

And you can see that it started pushing.

So push refers to repository Docker dot EOS slash daily code buffer slash cloud gateway and it's preparing

everything and the end.

It will create the image in your repository.

So we'll wait for that to complete.

So make sure you log in first and then you do the Docker push command and you can see that it pushed

and mounted as well.

And if you go back to our Docker hub and if we refresh.

We should be able to see that daily code buffer slash Cloud Gateway, which was published a few seconds

ago.

And if you go inside this, you can see that all the information that this is the daily code before

slash Cloud Gateway, and here you can add all the different information.

Currently the tag information, you can see there is only one tag available.

And if you open this, you will be able to see the different commands as well.

So all this commands, you can see that we added this command, right?

Add arguments, then copy, then entry point, then expose.

The rest of the commands are part of the open JDK itself because we use that.

So that will add all these things.

Then we added all these things.

So all this information you will be able to see here and you can see that this is a command and everything

came here if you go to the public view.

You will be able to see that Docker pull daily code before slash, whatever it is, and you will be

able to get all the information so you can see that how easy it is to push the images to the repository

as well.

So we did.

For one, you can always go ahead and do for the others as well and make sure that all your images are

available at your repository if you want anyone else to use your.







Transcript - 
Now let's understand about Docker compose.

Currently, if you see that we have multiple images available, right?

And all these images we start using one of the commands.

So all those images that we start for a container to start a container for those images, we did a different

commands and those commands were used separately and all those commands was having the different sets

of environment variable, different sets of port information and everything.

So what you had to do is you have to keep those commands handy whenever you want to work or whenever

you want to start your containers from the images.

That's a little bit hassle as well, because if you're working on ten different images and if you want

to start all those ten different images, then you have to go through all those commands and execute

all those commands and everything.

What if I tell you there is a way that you can define each and everything, whatever steps that you

did for all the different commands to start your containers based on the images into one single YAML

file, and by running that one single YAML file, it will start all your containers based on the images

that you've mentioned.

Right?

It will reduce a lot of burden from us.

Right.

Rather than remembering all those commands, mentioning those all commands, keeping it handy somewhere

and executing with those commands.

Just have one file handy with you and directly you can execute that file.

Cool.

Right.

So let's try to create the Docker compose file for our different applications or different images that

we have created.

And let's see how we can reduce a lot of work.







Transcript - 


So currently we have a lot of containers available, right?

Let's stop all those containers and let's clean everything.

You can see that for cleaning as well.

We have to stop the containers, we have to delete it and a lot of things.

Right?

So let me just stop everything.

Stop.

Look, all the containers are stopped now.

We'll wait for it to complete.

Now I will just use docker sys prune hyphen eight to clean everything.

Yes.

I'll just clear it out.

Everything okay?

And what I will do is let me just see Docker images.

I will clean that as well.

But ideally, I was needing those images.

So let me go ahead and create those images very quickly.

So what I need is I need one cloud gateway.

So let me just create the cloud Gateway image.

I need the config server.

So let me create the config server and I will be needing the service registry as well.

So let me create the service registry.

So all the three images are created for me now.

So let's wait for this to get completed so you can see that everything is completed.

If I go back and if I see Docker images, I a and you can see that I have all the images now.

Now what I will do is I will create a Docker compose file that will use all this images and within a

command everything will start forming.

So let me clear this everything.

Let me just keep this handy.

Okay, So now let's create a docker compose file, and with that one file we will start all the containers.

So what I have to do is I have to create a file, right?

So let me just open the new terminal and let me just go to the directory.

So you can see that all my projects are available here.

So what I will do is I will create the file here.

Docker compose dot y html.

Okay.

And let me open this file.

This is my Docker compose file.

Now, let's write this file.

So if I go back here, you can see that I have all the three different images available.

That is the config server.

So it's registry and Cloud Gateway.

So I'll mention all those three and I will add the configuration for it.

So what is the format?

First, you need to define the version information.

So version I'll mention as three.

So make sure that whatever the latest version is available, you use those version.

Currently, I'm using the version three, and after you define this, you need to define the services.

So let me define the services.

Now, within the services, you need to define all the services that you need to start.

So currently we want to work with Service Registry, Cloud, Gateway and Config Server, right?

So that all thing we will define.

So here I will define service registry.

You can give any name over here, keep the habit to give the same name as your image name or whatever.

That will save you a lot of efforts in debugging and fixing any issues.

So with service registry, so with this service service registry, you need to mention which image you

have to use with the image tag.

So I will just mention image I want to use for the service registry.

So I will go here and I will copy this.

I want to use this image.

Collin zero or 0.1.

You need to provide your repository or whatever the entire name and the tag information as well, like

which version you want to use.

If we see here, this is the 0.0.1 version, right?

Now after that you need to give the container name, what container name you are to give.

So container name equals to service registry.

I'm giving the same thing.

And then you need to give the ports which ports you want to expose.

And this will be list because from the same container you can expose multiple ports as well if there

are multiple applications are there within it.

So for the service registry, I want to expose eight, seven, six one as a host port from the 8761

from the Docker container port.

So you can see that these all information you were passing when you were starting the container from

the image, right.

Like which image you want to use.

What is the name of the container, What port you want to expose all this.

Cool, right?

So you can see that one service is done.

Now let's add another service.

Let's add config server.

Okay.

Now within this config server, which image I want to use, I can define image.

I will copy the image name from here.

So I want to use this config server called an 0.0.1.

And then the name of the container container name I want to go as config server and the ports information.

So I want to use port config server is running on 92969296 colon 9296.

OC config server is also done.

Now.

The third one was cloud Gateway.

So let's define cloud Gateway and I will define the image, which image I want to use and I will mention.

So I just need to use this cloud gateway.

And the version is latest.

Ten ports.

Information which port I want to expose is 99 because this will be running on port 1990.

So you can see that all the configurations that we have added, you can see how simple it is.

We just define that.

How many services are there?

There are three services and for each and every services we mentioned which image I want to use to start

the service that is the container.

What is the container name that you want to mention and what is the port information as well?

You want to mention Cool.

Right now, one thing also you can mention here, that is which service is dependent on which service.

So what I want is whatever I mentioned, what it will do is it will try to start all the services together

alongside it will create one network as well for my and our Docker compose.

So all the services which are mentioned here for that, it will start the services and it will create

one network as well for it.

There are multiple network types available that is the host network, Bridge Network, whatever.

It will create the bridge network for us for the different services that we have and it will give the

name as well.

So all those services will be there within the same network.

So whenever we want to refer to any of the services, we can refer using the service name itself.

So this gives us the extra functionality as well when we are working with Docker compose.

So now what I want is I want to add the depends on functionality like this service registry should start

first, and then once that is done I need to start my config server.

So that means my config server will be dependent on my service registry.

So whenever the container will start for the service registry, then the container for the config server

will be started and my cloud gateway is dependent on the config server.

So whenever the container for the config server is getting started after that, my cloud gateway should

get started.

So let's add that as well.

So we'll just add depends on and here we can pass the list, so we can pass the multiple list.

So here for service registry we don't need.

So let me just remove from here and I want to add your OC.

So this is dependent on service registry.

So I'll just mention that this is dependent on service registry.

Now I will copy this as well, and I'll mention here your indentation should be correct in the YAML,

so make sure you have the correctly.

Now.

My cloud gateway should be configured or should be dependent on my config server.

Now one more thing we had done.

Like whenever we were working with the command, right, we used to pass the environment variable as

well, like which enrollment variables are there.

So those environment variables also we can define here.

Cool, right?

So let's define those enrollment variable as well.

So here within the service registry there was no enrollment variable, right?

Because this doesn't need anywhere, no variable config server needs the environment variable that is

the service registry server you are.

All right.

So let's give those information here so I can define those using the environment attribute and wire

environment.

And this is also going to be the list so I can add multiple.

So what I wanted was if I go here, if I go to the config server application or HTML.

This is the one, Right?

So I'll just copy.

This equals two.

I'll copy this.

Right equals to this.

So I'm giving the URL address for the Yureka server.

But we saw that we were giving the host dot, dot, dot internal.

Right.

So this way we can give your one more thing.

I mentioned that everything will be created within the same network, right when we are working with

Docker Compose and if everything is there within the same network, I can access this information,

I can access the network or I can access the other services based on the name itself.

So if I don't pass host dot, Docker, dot internal, I can still access using the service registry

name.

So if I pass a service registry instead of host dot Docker dot internal, this will also work.

So let's pass this and it's more readable as well, right?

So we'll pass this the same information.

I can pass for my cloud gateway as well.

Right.

So let's do that.

Now, Cloud Gateway will need one more environment variable.

So let's define that and moment variable as well.

So that environment variable was if I go to the Cloud gateway and if I go to the application yaml of

Cloud Gateway, you can see that when was config server URL.

So let's add that as well.

So configure you are equals to.

Config server itself.

So you can see that you have defined your entire Docker compose.






Transcript - 

Now let's see how we can run this.

So let's go to the terminal.

And here we have this docker compose yaml file.

And currently if you see with docker piece hyphen a, there is no services available.

Cool.

Right.

Let me clear that as well.

So now to run this Docker compose file, what simply we have to do is we have to use Docker compose

command and if you want to see the help, you can go ahead and see the help information.

You will get everything Docker compose and you can see that you need to give the file information,

hyphen F with the file OC and if you want you can get the project information and there are the different

commands available.

All these commands are there.

So let me just clear this and we will just do Docker compose and then with hyphen F, we will give the

file information that this is the file that I want to use, that is Docker compose hyphen f, Docker

compose yaml.

And what I want to do is I want to up all the services.

So I'll just define up, up OC and I can pass hyphen dx to run everything in the dashboard.

I'm not passing it everything.

So I want to show you the logs as well because everything will start together and all the different

logs also will be mentioned.

So let's start all the services all together and if you just hit enter, you can see that it first it

is trying to create a network.

OC After that you can see that all the containers are there and you can see that different colors are

mentioned for the different log, so you'll be easily able to identify different things.

So you can see that everything is running here, right?

And if you go back here.

And if we go to the Utica.

You can see that your yureka server is running, your config server is running, but I'm not able to

see my cloud gateway.

So let's see what happened.

So if we go back to our terminal and if we go to the cloud gateway here, right, this is the Cloud

Gateway logs and you can see that it's showing it exited with code one.

So what is the error?

The error I can see here.

Let's scroll up.

Right.

We'll see what is the error.

And I can see the error that it could not locate the property.

Source of the resource is not optional.

That means what are the config server that we have defined.

Right?

It was not able to get those information, but we have the config server running.

Right.

We have the convicts are running, but we need to understand this.

Like whenever we add the depends on.

Right.

If you go here, whenever we add the depends on what it will do is it will only wait until that container

is started.

It doesn't mean that your application inside the container started.

So the moment the container of the config server is started, your cloud gateway will get started as

well.

It will not wait until your config server is started and config server is started giving you the configuration.

So to prove this, what I will do is let me just open this and if I do Docker piece so you can see that

two services are running, but if I do Docker PS hyphen A, this is my third service which is not running,

that is exited 2 minutes ago.

Now if I start this again, it should get the config server because now config server is already running,

right?

So if I start this docker start command and I'll just do the container here.

You can see that the container is started, right?

And here you can see that I'm getting the cloud gateway logs now and you can see that your cloud gateway

is started and now it should be able to connect to my service registry as well because so now my cloud

gateway will get the configuration from the config server and it should be able to connect to my service

registry.

So if I go back, if I refresh, you can see that my API gateway is now also available.

So that means that depends on doesn't wait until your application is started.

So we need to make sure that we implement this functionality right that until and unless my config server

started my API gateway should not start it then only it should get started.

So it will work.

Fine.

So what I will do is let's do that configuration and let's clear it out.

Everything, whatever we've created.

So what I will do, I will just do control C here so you can see that once we do control C, everything

will be stopped.

Now everything is killing.

So let's do Docker piece again.

Okay.

Everything is top now I will remove all this container.

So Docker piece.

Sorry.

Docker.

Am.

Okay.

Everything is clear.

If I do Dr. PS hyphen a, I should not get any services.







Transcript - 

So now let's implement the health checks.

So if I go to my YAML file, what I want to do is I want to start my cloud gateway after my config server

is healthy.

That's the one thing that I want to do.

How I will get to know that when my config is healthy.

So whenever I'm able to hit my config server itself, right, whenever I'm able to hit my health endpoints

of my config server.

And if that shows that this is healthy, after that, I want to start my cloud gateway.

That's the simple thing I want to do.

So for to implement this depends on attribute is not enough.

We need to add the health endpoints as well.

Health checks as well.

So let's implement the health checks that will allow us to start my cloud gateway once my config is

healthy.

So I will not encounter with the failures that I got.

So let's implement this.

So here within the config server I will implement the health checks.

Now let's go here and implement the health checks.

I will add the attribute health check here.

And within the health check, I need to add the test command.

What I want to test, I can do the command for the health endpoint that I have in our config server.

We have added the config server dependency.

So health endpoint will automatically come with that, which will allow us to hit that endpoint.

And if that endpoint is available, if I'm getting the success result with that, I can say that my

health checkpoint is working fine, my application is healthy.

So let's see that we will add the commands here.

We will do the CMD command.

CMT Command is used to do the any commands on the Docker containers, so we'll do command with hyphen

F and then we will add the URL.

So we'll just define HTTP colon config server because we can access using the service name itself colon

9296 slash actuator slash health.

This is the default health check endpoint whenever you are added the actuator, this is the health point

you will be using and you can define on what interval this health point should work.

So I can define.

Interval should be maybe 10 seconds and I can define timeout as well for 5 seconds and I can define

how many tries I want to do.

So I define the hell check over here for my config server.

So now Docker will know that when this particular service is healthy OC, not the container is up,

it will know that whenever this is healthy.

So whenever this test is successful, docker will mark this config server as healthy.

Now what I want is from my cloud gateway, I need to make sure that whenever the config server is healthy

then only start this cloud gateway.

Simple.

Right.

And not just depend on config server.

Depends on config server with the condition oc and in the condition I'll mention service as healthy.

So there are a couple of options available in the conditions which condition you can pass in this we

are passing the service is healthy so whenever this config server will be marked as healthy, then only

my cloud gateway will start because in the depends on where mentioned that this config server's condition

should be healthy.

Then start this.

Now let's start this again.

So let's go to the terminal and I will use Docker compose command, Docker compose, have f docker compose

yaml up with a detached mode oc.

Now you can see that this is in the dashboard, so it will create the so you can see that first service

registry was started, then config server started and your cloud gateway is just created.

It will wait until this is healthy and then you can see that it got started.

Now you can see that this is also started now.

If I see with Docker piece you can see that everything is started and you can see there is a mark as

well that your config server is healthy.

So once this was healthy then only it got started.

You can see the gap as well when there is a difference in the timing.

So now if I go back.

And if I refresh.

You can see that everything is connected.

Everything is working completely fine.

Cool.

Right.

So this way you can use the Docker Compose Command to mention each and everything.

Like what images and what services you need, which is dependent on what and everything.

And it will just start everything for you using one command.

Now how about deleting everything?

So the command to delete each and everything would be just Docker compose hyphen f Docker compose with

the same yaml configuration.

If you just do down command, it will bring down each and everything and it will delete the containers

as well for me.

So can see that container was stopped and then removed also also.

And you can see that it will do in the reverse order because we have added the dependencies accordingly.

So you can see that everything is removed.

Now if I clear this and if I see Docker command, you can see that there is no containers available.

If I see the Docker piece hyphen as well, there is nothing available, only I'll be having the images.

So you can see that how easy it is to work with the Docker compose.

So this was all about the Docker compose and how you can create the Docker compose files and how you

can start the Docker compose services and bring down all the services.

Now in the next video, we will see how to automate the project building image creation and pushing

those images to the registry in a simpler way in the simple steps.

Because if you remember, we did a lot of things.

We were doing the ambient clean install to create the images or to create a jar files.

Then we need to have the Docker files.

We need to create the Docker files.

And after the Docker files, we were creating the images out of it and then we were pushing those images

to the Docker repository.

So we will simplify all those steps in the next video.






Transcript - 

Now let's talk about Maven Git plugin, which will allow us to package our applications into our Docker

images very easily.

So let's go ahead and see this in action.

So let me just go to the browser and let me search for the.

Maven ship plugin.

GitHub.

So this is the MAVEN plugin GitHub page over here.

And from here you will get all the information about how to use Git plugin in your springboard projects

so you can see you will get all the information like GBS is Maven plugin for building Docker and OCR

images for your Java application.

So it's an open source plugin by Google which will allow us to package our applications into Docker

images.

The main thing about this is it will allow us to create the Docker images without the use of the Docker

file, plus without the use of the Docker install as well.

For of course, if you want to run our Docker images, we need to have the Docker.

But to package this using the Java in the Docker images, we do not need the Docker as well.

Install jib will do all the stuffs, it will package your application and it will push our Docker images

to the repository altogether.

So we are going to use this plugin in our applications.

So if you scroll here you will get all the information and you will get the plugin information as well,

like how we can use this particular plugin.

So what we'll do is we'll use this and we will add in our included in our XML file.

So what I will do is I will be copying this plugin section and this plugin we will be adding in our

XML file.

So let me go to the IntelliJ idea and let's start with the service registry first.

So what I will do, I will go to the service registry.

Let me close all this file here first, okay.

I'll go to the service registry, go to the XML file, and your what I will do is I will go to the plugin

section and I will add the plugin.

So you can see that I just added the plugin that is com google Cloud tools as a group ID and git plugin

as the artifact.

Just add this information and the version information.

Let me just remove the version as well because with the spring pattern it will take the latest version

available.

And now here in the configuration, we need to add all the information.

If you see the Docker file, whatever we have added right, like from where from which particular base

image we are building, our Docker images that we need to add and all the other information.

The similar thing, we will add those configuration directly in our XML file.

So here in the configuration we will add the fields.

So here in the configuration we will add from which particular image we want to use.

So I will just define from image here and we will define that we want to use open JDK call an 11 version.

This is what we have defined now from this image, what we are creating to image.

So here we will need to define our image name.

So we need to define our entire repository, full repository name and the image name.

So our repository is where the Docker I.

All right, that is the registry hub, dot, dot, dot com.

So we will define that as well.

Registry dot, hub, dot, dot, dot com.

Within this, my repo is daily code buffer.

Let me just spell it out correctly.

Daily code buffer and slash what would be the name of my image.

So what I will do is I will use the same name as the group name here.

So this is the artifact I right.

So this is something that I will use.

So you can define this as well, that this is the artifact that I want to use, or you can give any

other name as well.

So let me just define services issue here.

And if you want, you can define dollar artifact idea as well this way.

So here I will define service registry and within the tags information, we can define the tags.

Here I will define dollar project dot version.

Okay.

So let me just define this way that I'm defining project dot version, whatever the version would be.

Currently, you can see that the version is 0.0.1 snapshot.

I can change to zero or 0.1 as well.

And you can see that I have just modified my POM configuration which will be using my plugin that is

the give me one plugin where I will define the configuration like which is the base image I want to

use and what image I want to create out of it and what would be my image image name and what would be

the version information?

Simple.

So this is the only thing that we have to define now.

Other than that, what we have to do is we have to do the MAVEN configuration as well in these settings.

That XML file, if you are not aware, MAVEN uses the settings XML file.

If you go over here in the MAVEN, if you go to the configuration and MAVEN settings.

You will see that there is a user settings file within this particular folder.

So if I go here in my MTU folder, which was mentioned, this is the settings or XML file.

If you do not have the settings XML file, you can create one.

And in this settings XML file, we will define the configuration for our Docker repository, which means

we will define our credentials.

Because this particular configuration, it will try to create the image and it will try to publish to

my Docker hub, to my registry.

But for that we need to authenticate as well, right?

We need to give the authentication detail as well.

So rather than not giving the authentication details here, we will give the authentication details

in our settings or XML file, which we are not going to commit, which will be in my local always.

So if you say this, this is going to be your configuration file.

So I will be providing the settings or the XML file.

You can save this as a settings or XML file in your MTU folder wherever your maven is pointing to where

we have defined the server information within this server, we have defined that we are going to connect

to registry dot, hub, dot dotcom.

This is our Docker registry and within that we have defined our username and password.

You have to give your username and your password.

Just add this information and rebuild your projects.

Once you have defined the settings of the XML file and you have defined this configuration, we can

build our project.

So what it will do, it will create the project.

That means it will create a jar file.

And with that jar file it will try to build the Docker image and that Docker image will be published.

It will be pushed to the registry.

Everything it will do automatically.

We do not have to do anything manually.

So if you go to browser and if you go to Docker Hub, currently you can see that there is a service

registry available.

But this is within DASH and this is one year old.

Now we will go back and we will go to service Registry, will open the terminal here and we will do.

And VN clean install jib colon build.

This is the MAVEN command which will clean the project install.

It will create the package for us and then it will use the zip colon build plugin to build our docker

image and it will publish it everything it will do here.

So let's run this.

So you can see that it is running.

And you can see that will happen and.

It is trying to push here as well.

So you can see that pushing blobs.

So we'll wait for it to complete.

So you can see that it is publishing to our repository.

Cool, right.

You can see that build a successful which means my build and pushed image also completed.

Here you can see that registry Docker Hub daily code before service registry.

And the version is this.

Now if I go here, you can see that there is two available, Okay?

One is service registry and one is service Registry zero or 0.1.

So by default it will also create one tag for the latest tag, one will be latest and one will be 0.0.1.

So if we go to the browser again and if we refresh our page.

There is our darker rub.

You can see that daily code buffer slash service registry.

Pushed a few seconds ago.

And if you go inside that you can see that we have two tags available available that is 0.01, which

was the tag that we mentioned and one was the latest.

So you can see that it will create the two tag.

So whenever we will do publish the whatever the latest version would be for that latest version, the

latest tag would be created for us.

So always we'll be creating this way to tag.

And you can see that with just one command, we were able to push everything right now.

One more thing.

This service registry will not be available within your local environment currently because it will

not use Docker.

It will directly package everything and it will push the registry.

Now, if you want to use this image, you can take this image and you can pull this image into your

Docker environment and you can run this similar way.

How we run when we created the image using the Docker file.

You can also use this with your Docker compose as well.

So this is if you go to public view, you will get all the information as well here that these are the

tags, information.

You can see these are the tags, these are the digest and version information and everything you will

be able to see here.

Similar thing.

You have to copy this plugin and you have to add in all your projects.

That's it.

What you have to change.

You just need to change the name of your image.

That's it.

If you do this and if you run this command, you will be able to create the images for each and everything.

One more thing you can do is you can add this particular steps, right?

This particular plugin into your maven faces as well.

So whenever you will do and then click install if you want at that time, only you want that everything

should be created that also you can do that configuration is also available when you go here.

So if you go here.

Okay.

If you scroll down, you'll get all the configuration.

You'll be able to see each and everything so you can see the command is available and everything is

there and you can see this step build binding to lifecycle.

So if you define this execution phases, so at the time of build and packages, if you want to create

your Docker images that also you can do, you don't have to mention git build by just mentioning the

min clean package, you will be able to create your jar file, create your docker image file and publish

that image file to your repository, everything you can do within the lifecycle itself.

So you can see that how easy it is to create our Docker images using the give me one plugin so the homework

for you would be to implement the same thing in the other repositories as well and make sure that all

those repositories have this plugin and you are able to publish all those repositories, all those image

files to your registry.

So that's all with the implementation of the Git plugin and that's how easily we were able to add the

plugin.






Transcript - 
So in this Kubernetes tutorial, what are you going to say is we are going to study a complete Kubernetes

architecture and all those different commands.

We are going to study what is Kubernetes.

We are going to understand its architecture, why there was a need of Kubernetes.

We are also going to understand the different components of Kubernetes, what all the different components

means, and how we can create the different components.

In Kubernetes architecture, we are also going to create the different Kubernetes YAML configurations.

So with those configurations, what are the applications that we have created?

How can use the custom images to create our Kubernetes architecture?

And with those YAML configuration, our entire application will be deployed to our Kubernetes cluster.

So these are all the details that we are going to cover in this tutorial.

So it's going to be a pretty comprehensive and we are going to learn a lot of things.






Transcript - 


So first we need to understand what is Kubernetes.

And before understanding Kubernetes, we need to understand why there was a need of a Kubernetes.

So first, let's go back to the history and see how the traditional deployment was happening.

In the traditional deployment, what we used to have was we used to have our hardware, and on top of

the hardware, we were having our operating systems and on top of our operating system, we were deploying

our applications that might be different applications.

And all those different applications were deployed all together on our operating system.

Now with this approach, there was a limitation like, suppose my one application is using the Java

eight, my other application is using Java 11 and other application is using Python then to maintain

all those different versions and everything was a little bit cumbersome and the proper utilization of

the resources was not happening.

So for that reason, virtualization comes into picture like on one hardware, you can have multiple

different virtual environments and you can use virtual environments to complete or to deploy your applications

in the virtualized deployment.

What we used to have was we used to have our hardware and on top of the hardware, we were having our

operating system and on top of the operating system, whatever the OS that we were using.

On top of that, there was a virtual layer which provides us to have the virtual environment on top

of our system.

Yet I'm taking the example of the hypervisor.

So on top of the operating system, we were having the hypervisor and on top of the hypervisor we have

the different virtual environments.

So you can see that we can have multiple virtual machines on top of our hypervisor layer that's on top

of our operating system.

And in this virtual environment or this virtual machines will also have its own operating system.

And on top of them we can have different libraries install whatever is required.

And on top of that we can deploy our application.

So in one virtual environment, I can deploy my Java application and Python application.

In another environment, I can deploy my Java 11 application and Java 17 application and so on.

So this way both the environments are different, so there is no conflicting environments or conflicting

results.

So it's pretty smooth than the traditional deployment.

But here you can see that on top of our hypervisor we have virtual machines and that virtual machines

also uses its own operating system.

And with the operating system, there is a lot of things that comes into picture.

So you can see that there is a lot of resources utilized by the operating system and the different things

involved in that.

So the proper utilization of resource is not happening.

I just want to deploy my application.

I don't want that there is an extra layer of resources being used by the operating system and everything.

So to avoid this overhead of the resource utilization by the virtual machines, the containerization

comes into picture.

So in the container deployment, what we have is we have our traditional hardware and on top of the

traditional hardware, we have our operating system installed.

And on top of the operating system we have our container runtime.

There are multiple container runtimes available.

So any of the container runtime we have on our operating system and on top of that we have our containers

installed or container deployed to our container runtime.

There are many like Docker Container and many others are also there.

But on top of our container runtime there is a container and those containers will have the libraries.

And with those libraries we will be deploying our application.

So you can see that the base of the container runtime is still our host operating operating system,

the host kernel.

So that means that the kernel and the operating system will remain the same, the kernel will remain

the same.

So if you have the Linux operating system, then you will be having the Linux kernel.

So whatever the container runtime or containers that you are going to create, that will be also with

the Linux only.

So you can see there is no extra headroom that needs to be utilized by your containers.

So just define your containers and those containers will be deployed easily.

So earlier we saw that using the docker we used to create the containers and in those containers we

were having our applications deployed.

Now suppose you have a very huge application and in your environment you have hundreds and thousands

of containers needs to be deployed.

You need to maintain everything.

So to maintain your entire lifecycle of your containers by doing it manually, it's really difficult,

right?

So what if that is a tool that allows us to do all those things, managing all those containers by default,

it allows us to maintain each and everything.

It will orchestrate everything for us.

We do not have to worry about anything.

We just need to concentrate on developing our application deployment and everything will be handled

by the application or by the tool itself.

So for that reason, Kubernetes comes into which Kubernetes allows us to handle or to orchestrate each

and every containers that we have in our application.

So if I have hundreds of application and for each of different applications, I have the different configurations

like this application should be connected to this environment, This application should be connected

to this database, it should be connected to this Redis or Kafka or anything.

All those configurations and everything should be configured.

All those things and everything should be configured with Kubernetes.

And Kubernetes will take care of each and everything to make sure that each and everything is running

at all times.

So.

That's the idea behind Kubernetes.

Now, let's see its architecture.






Transcript - 

So if I give you the very basic components, architecture involves two things.

That is the master nodes and the worker nodes.

So it will always have a master node that will control everything and it will have the worker nodes

that will work according to the master node.

So that will be your entire Kubernetes cluster in your Kubernetes cluster that will be having multiple

nodes and one of them will be master.

There might be multiple masters, but at least there should be one master and the worker node.

And then in the worker nodes, you will be deploying your applications, your containers.

And with the containers, there will be one Kubernetes component that is called a cube.

So what cube will do is Cube will run on each and every node and it will make sure that it has all the

proper sets of information based on our proper set of configuration, based on the master nodes or whatever

the configuration and everything has been provided within the cluster, it will be handled accordingly.

Now let's talk more about the master node, what it will be there in the master node.

So Master Node will have the API component.

So what is the API component?

It's the API server or the front end part of the components control plane.

So whatever we will be doing in our Kubernetes cluster, everything will happen using the API.

So we have the access to this API, using the UI or using the API itself or using the CLI tool as well.

So with either of the one, we can access the API server that is the control plane of the Kubernetes.

So whatever we have to do, like creating a port or creating containers, doing some extra configuration,

checking the logs, anything, we will give the command to the API.

And based on that, either of the one that is a UI API or CLI, we will interact with the Kubernetes.

So that is one of the components in the master node of the Kubernetes cluster.

The next come is the control manager.

So control manager is the one that keeps track of each and everything happening in the Kubernetes cluster.

So it is also divided into another four parts.

That is the node controller, repetition controller and points controller and the service account and

token controller.

And from the name itself suggests that it will be doing the operation based on the name.

So Node controller will handle all the nodes that application controller will handle the replication,

like whatever the replication sets that you have defined.

Right?

Like this particular application should have at least one node available or at least two nodes available.

So it will maintain that whatever we have defined replicas, it will maintain those replicas and accordingly

OC endpoints will handle our endpoints.

And so this accounting token will handle all the service account and token information.

So there are different controllers available and all those controller will handle all the different

operations accordingly.

Then the next come is the scheduler.

So scheduler will maintain the port placement in our Kubernetes cluster.

So if I have three nodes available in my cluster and I'll be just creating the pods within the cluster,

so scheduler will handle that, which particular part should go to which particular node.

So it will schedule based on the resources available or based on the configuration available, and it

will place those parts within the actual nodes so that scheduling and everything will be handled by

the scheduler in our Kubernetes cluster.

The next component in our master node will be the seed server.

So seed server is a highly consistent and highly available key value store.

It stores the information of each and everything happening in our Kubernetes cluster.

So whatever the configuration or whatever happened till now in your cluster, everything will be stored

in the seed server and from that we will get all the information.

So we should always make sure that we have the backing store available for it.

So we should always make sure that we have at least two must node available.

So whenever there is issue with one of the master node, another node can act as a master node and it

will have all the information that happened within the cluster.

Internally, it will handle each and everything.

We do not have to worry about it, but we need to make sure that it has multiple master nodes and how

they will understand each and everything.

Because for each and everything, like Master Node and the worker nodes, they will have its own virtual

network.

So everything will be happening within the virtual network, within the cluster itself, and they can

interact with each other.

So you can see from the diagram that we will be having master nodes and worker nodes and connecting

between them would be using the virtual network within that particular cluster environment.

Now let's talk more about the worker nodes.

What worker nodes will be having worker nodes will be having the pod component.

So POD is a single unit of deployable component in the Kubernetes cluster.

So anything that we want to do within the Kubernetes cluster, we will be doing using the pod.

So whenever I want to deploy an application or whenever I want to deploy a container within the Kubernetes

cluster, I need to wrap around with the pod and that I can deploy.

And whenever we'll be creating the pods with the containers in it, whatever the application that we

are going to start on, that particular container within the pod, it will assign an IP address to it

so each and every pod will have its own IP address.

So it's a dynamic IP address created.

So whenever you create a new port, the new IP address will be created to it.

And as this IP address will be dynamic, we need some component which can have a static IP address so

our different components can connect to it.

So for that reason we have a service component.

So for multiple pod components we can create a service component.

So suppose if I have one application for the user management, so for that user management, I can create

multiple pods.

All those multiple pods will be having its own IP address, but all.

Those pods will be connected to one user management service.

So whenever any other application wants to connect to that user management service, they can call using

the service IP address or the service name itself.

And with that service name, the service name will have the load balance service so it can call all

the services attached to it.

So if I have for user management service attached to that service, I can call accordingly.

So this was all about the Kubernetes architecture where we saw the different components available in

the worker node and the master node.







Transcript - 

Now let's talk about the different components available in the Kubernetes cluster.

So we'll see the different components that are there in the Kubernetes cluster and we'll understand

what all these components means.

So let's start with the worker node.

We already saw that the worker nodes will be the nodes that will have the application deployed to it.

And the next component is the port.

So I explained you briefly in the earlier video as well.

That port is the smallest unit of Kubernetes component that can be deployed to the cluster.

So you can see that these are the pods available and all these pods will be having the different application.

So consider that in one of the port, I have my application container and in the other port I have my

DB container.

So in one of the port I have application and one of the port I have database and these are the abstraction

to my containers.

So within each port I'll be having the container.

I can have one container as well and multiple containers as well based on the architecture or based

on the design that you are using.

And each port will be having its own IP address.

So you can see that both the ports will be having its own IP address and both the ports are connected

with each other using the IP address available to it.

Now suppose one of the port gets down, my application is down or my database is down.

So what happens is whenever the port is down, the entire new port is created and our new port is created.

And whenever the new port is created, it will have its own new IP address.

So you can see that new IP address is created for the new port.

And now the IP address is different.

So how your application will connect to the database or whatever the different paths you have because

the IP address is changed, it's not the static IP or a constant IP, it's a dynamic IP.

So for that reason we have the service component available.

So whatever the port that we create for those ports, we will create the service services.

Now the service will help us to connect all the different parts together because service will also have

the different port attached to it in the load balanced format.

Now these services are available within the two different ways it will be internal service and the external

service.

Now you can see that if any components are interacting within each other, it will be called as the

internal service.

So you can see that I have two ports available and both the ports are within the Kubernetes cluster,

so both can connect together within the cluster.

So for that we'll be creating the internal service, and the external service would be any request coming

from outside of the Kubernetes cluster that I need to handle.

So your application would be used by the external user.

So any user can do the request to your application.

So for that, we have to create the excellent service, but no one would be doing the database access

directly.

So that would be only within your environment, within your application.

So that's why that's going to be the internal service.

And for the external service we have, the other component that we can use to create is the ingress

component.

So ingress component will allow us to create the external service, which will allow the traffic from

the public internet or public network to your Kubernetes cluster.

Now whenever we are creating the different application, we'll be having a lot of different configurations

as well.

Like this particular application that I have, it should connect to which particular database it should

connect to, which is environment or a lot of different information.

So a lot of configuration.

Also, we are going to add in our containers and the container lifecycle is pretty unique, like it

is going to be destroyed as soon as there is an issue with it and the new port is going to be created

and the new configuration or new IP address is going to be created.

So for that reason, what the ideal way to do is to handle the configuration separately.

So we have the separate configuration component that is going to be called as a config map.

So that is a component, that is the config map which will allow us to add all those configurations.

So all those configurations will be added and we can have like all these configurations are being used

in this particular port itself and within this configuration.

So that configuration we can define.

So all the configurations are externalized and it can be used within the Kubernetes cluster.

Now in our application, we are also going to use the sensitive data as well, like storing the passwords

and the secret information and everything.

So for that reason, we have the other component that is called the secret.

So this secret will store the data in the base 64 encoded format so it's not directly visible to anyone.

So here we can store all of our sensitive data like password secrets, credentials and everything,

and those can be used within the pod by the containers and the application to access those.

Now as based on the pods lifecycle, like whenever there's an issue with the pod, the entire pod will

be deleted and the new pod is going to be created.

So whenever we are storing data to the container or ports, like in case of the database, we are going

to store the data, right?

So whenever we are going to store the data in the database and the port goes down and a new pod is created,

it's not ideal, right?

Because a new pod is going to be created and entire data is going to be wiped out, Right?

So for that reason, what we have to do is we have to attach the storage to that particular pod.

So for that reason, we are going to use the volumes and we can use different types of volumes available.

So what we can do is like an extra storage is provided and that storage will be constant and the new

pod is going to be created.

And whenever the pod is going to be created, it will use the data from that particular volume.

So whatever the data is stored till that particular point.

Before the board was deleted, our board was crashed.

That information we will be having.

So with that, we won't lose any data and we can use two types of storage available.

That is going to be the local storage that's going to be inside the cluster.

But what will happen is when we go ahead with this approaches, whenever the cluster crashes or cluster

is deleted, then our entire volume is also going to be deleted.

So in that case, we can use the remote volumes or remote storage which is outside your cluster and

that we can use to connect our data.

So there are different types of storage providers.

All the clouds provide the storage to use within the Kubernetes cluster, so we can use those storage

and attach our volumes to that storage.

So whenever there's an issue with the pods or entire Kubernetes cluster, we won't lose any data.

So this will also we can go ahead and attach our volumes to the pods available, and that volumes will

be used whenever a new port is also created.

Now, we already saw that whenever the port is destroyed, we won't be able to access it.

Right?

Your application is down.

But our goal of using Kubernetes is to make sure that we have the highly scalable and highly available

infrastructure, highly available applications.

Right?

So for that reason, whenever any port is down, you won't be able to access it.

So for that reason, what we have to do is we have to make sure that there is a replica available so

we can directly access it.

So it provides us to have the replicas available.

So multiple applications or multiple ports running within the Kubernetes cluster so we can have the

multiple worker nodes.

And on those worker nodes, we can have the multiple port running for our application.

So whenever one port is down, all the traffics will be routed to the other port.

So our application will be highly available.

And whenever we have to do this, like we have multiple applications available and multiple ports and

ports available and all those should be maintained uniformly, like within one component, everything

should be created, right?

Because I do not want to create port again, then service again and create another port and service

and everything.

Everything should be streamlined within one component.

Everything should be created and replicas and everything should be maintained.

So for that reason we have the component called deployment.

So within the deployment we can define everything like this deployment will have this many number of

containers.

All those containers will be having minimum number of this replicas and everything and all those information

we can attach to the deployment.

And whenever we will create the deployment, all those components will be created for us.

So we do not have to manually do everything.

Everything will be automated using the deployment component.

And this deployment component will be created as a stateless component.

And whenever we want to create a stateful components out of it at that time, we can use the stateful

set components and whenever we want to work with the stateful set component, it's a little bit tricky.

There is a lot of extra configuration is also required, so most of them is preferred using the deployment

component.

But whenever you want to work with this, the stateful applications like database and the service registries

and everything at that time, we will prefer the stateful set component.

So these are all the basic components available in the Kubernetes cluster and we are going to work with

them.

There are a lot of other components available, but whenever you start working on it, you will get

the understanding of it and you will learn all those concepts as well.

But you should know the basics of each and everything and which component is used, why and where.






Transcript - 

Now let's install Kubernetes in our system for the development purposes.

Now, there are a lot of cloud providers that gives you the Kubernetes cluster very easily created within

that cloud environment, and it is relatively very cheap as well.

So if you want, you can directly use that as well.

But if you want to do completely locally.

So for that reason, we are going to install in our local machine.

So what we are going to do is we are going to install two components.

One is the Kubernetes cluster itself.

So for that reason we are going to install mini Cube.

Mini Cube allows us to create the local Kubernetes cluster in our local environment.

It will be a one node cluster created in our local environment.

And we are also going to install Cube CTL, you can call it as a Kubernetes controller or cube cattle

or whatever, but that will allow us to interact with the Kubernetes APIs.

So whatever interaction that we have to do for our Kubernetes cluster that we will do using the Cube

City.

So that two things we are going to install in our machine.

Now, if you install the Docker desktop, then Docker desktop allows us to create the Kubernetes cluster

within that environment itself.

So that also I will show you let me just search for Cube CTL install here and we will go to the Kubernetes

website here OC.

And yet you can see that all the tools required to install the CTL.

So you can follow this and you can install, you can install in Linux, Mac and Windows.

And here you can see that you have the option to install the new cube as well.

Now, I would highly suggest you to use any of the package manager for your operating system.

So if you're using Mac, you can use the homebrew and with the help of Homebrew, you can install mini

cube and cube CDL.

If you're on windows, you can use the chocolate, a package manager, and with that chocolate package

manager you can install Mini Cube and cube CTL.

And for the Linux, the same way all the information is available here so you can go through it.

So as I'm using Mac, let's go ahead and start with the Mac.

So install Cube CTL on Mac OS.

I will just open this here and if you scroll down here you will have the option to use the intel based

as well and the apple silicon as well.

If you scroll down here, you will have the option to use the homebrew as well.

So with this you install Cube CTL or you install Kubernetes hyphen CLI, you can install Cube CTL in

your machine.

Similarly, if you go to the Linux one sorry, Windows one, you can get the same information you can

use using the chocolate E as well.

So if you scroll down here, you can see install on windows using chocolate.

So with this command you can install the cubes in your windows as well.

Similarly, you can check with the Linux environment as well.

It will be different for your Ubuntu or Debian based OS based on the distributions.

So make sure that you are installing cube CTL and also you are installing the mini cube.

So if you go ahead and click on The View Mini Cube, get started guide, you will get the similar information

that how you need to install based on the Linux Mac or Windows.

If you're on Mac and select the platform and the homebrew, it will be easier when you are going with

your homebrew.

Otherwise you can install the binary and do the same way as well.

So with the homebrew you can just use the command you install mini cube.

Similarly for Windows, you can opt for the chocolatey and you can install using the Chocolate Command.

Similarly for Linux as well, you can use any of the architecture and release type and any of the package

manager and accordingly you can install.

So it's very easy to install.

I will show you in the Mac and I have already created the video on the windows that is there on the

channel, but I will add that video as well in this course so you can follow both of them.

So what I will do is I will open the terminal, let me clear everything first.

So what I will do is to install the mini cube.

I will use the command view because I'm going to install using the blue.

So I will just do command to install mini cube and I will just run it.

So it will start downloading and it will update everything.

You can see that it is already installed.

So it is saying that it's already updated, but for you it will start installing.

Similarly, we can go ahead with install Cube, Ctrl OC.

This is going to be your Kubernetes controller.

So you can see that this is also updated and the latest version is available.

So this way you can see that your Cube, CTL and Mini Cube both are installed.

Let me just clear everything and if you run the Cube CTL command, you should be able to run the command.

And with the mini cube as well, you should be able to run this command.

So both are running now.

You need to start the Kubernetes server.

So what you have to do is you have to check that your mini cube is running or not.

So with the mini Cube command mini cube status, you can check the status of your Kubernetes cluster.

And currently you can see that my Kubernetes cluster is running, my control plane is running, host

black API server in cube config, everything is running.

So if I just use the command mini cube stop, my entire mini cube will stop.

So my entire Kubernetes cluster will stop.

You can see that powering of mini cube.

I will show you how to start as well.

So we'll wait for this to complete.

You can see that one node is top.

Now to start, I can use Mini Cube start command to start my mini server.

That is your Kubernetes cluster.

So I'll just hit my spelling is wrong, so I'll just type it correctly.

Mini cube start and you can see that it's starting the server for us so you can see everything.

All the steps are displayed here.

So we'll wait for this to complete.

Generally, it will take around 30 seconds to one minute to create everything for you, and you can

see that your cube is configured and everything should be running.

You can see that it's done.

Cube City is now configured to use Mini Cube.

Now, if I clear this and I use Mini Cube status, I should get the status that everything is running

completely fine.

Now, within this Kubernetes cluster, I can use my cube Ctrl commands, so if I use Cube, it'll get

all I will get all the components available.

So you can see that there is one default Kubernetes service available within my Kubernetes cluster created.

Now if you have already installed the Docker OC, So if you can go to Docker Hub, that is a Docker

desktop install from that as well, you can create your cluster.

So let me just show you that as well.

Let's wait for this to open.

So if you go to the settings here and within the settings, if you go to the Kubernetes and you can

see that you can enable the Kubernetes.

So with this, you do not have to install the.

So when you enable this, a Kubernetes cluster will be created when you are starting the Docker desktop.

So this also you can use and whenever you will install Cube CTL, it will configure to use this version

of Kubernetes cluster.

So that's how you'll be able to install Mini Cube and Cube CTL in your local machine.

I've shown you with Mac.

I will also include the video of the windows that I have created.

So that's all in this video for installing Mini Cube and Cube CTL in your machine.








Transcript - 
So now as we install the Cube, CTL and Mini Cube in our system, we can see different commands available

in the Kubernetes.

So we'll go through the bunch of commands, and with those commands will be able to understand how we

can access the Kubernetes and how we can go through around the different Kubernetes resources and how

we can manipulate them.

So let's see that what we will do is we'll start our terminal.

Look, you can see that I started my terminal.

Switch to Windows System because this is much more powerful than my Mac.

So it will be able to handle all the components cluster and the recording as well.

So what you will get is your capacity is installed and you will check the status of the mini cube.

If it's not started, we will start our mini observer.

So let's check the status.

We'll just do mini cube status Command to check the status of our mini cube.

That is the Kubernetes cluster.

And yet you can see that everything is stopped.

So we need to start our mini cube.

So let's start it.

The command is Mini Cube.

Start.

So it will start this hour.

So you can see that it has started.

It is restarting our cluster.

We will wait for this to complete.

So now we can see that our mini cube started.

And now if we check the status mini cube status, we should be able to get that Everything is running

fine, everything is configured.

So let me just clear the console here and now we can start working on our commands.

So let's see the different commands in the Kubernetes.

So as we know that there are different resources available, that is the pods available.

That's the single unit that we can deploy to a Kubernetes cluster.

Then we have replica sets, then we have deployment, then we have services.

All the stuff that we saw briefly in the previous videos.

Now we will see everything with the commands, like how we can get all those information.

So everything we do or we will get is using the Kubernetes controller that is the cube serial.

So this is the Cubes command.

And with the get command, we will get all the different resources.

So if you want to get the namespaces, then we will add the or we will give the command that is a namespace.

If we want pod information, then we will give pod.

If we want deployment, then we will give deployment.

So this way all the different commands are there.

So let's see with the pods.

If we go cubes, it'll get pods, we will get all the different pods available.

So you can see that these are the different paths available running in my cluster.

If you want to check with the deployment so we can check with the deployment as well.

Cube, CTL get deployment and here we will get the deployments.

These are the different deployments available in my cluster.

If you want to check with the services, then we can check with Cube.

It'll get services to get all the services.

These are all the different services available in my Kubernetes cluster.

So we saw that pod is available, deployment is available, service is available.

We can check with the namespace as well.

So it'll get namespaces.

It will give you the different namespaces.

So you can see that these are the different namespaces available in the Kubernetes cluster.

So these are just the basic commands to get all the resources information from the cluster.

Now let's see how we can create the deployment.

As we saw, that deployment is a wrapper around the ports, so we can have multiple pods as well and

all the different things that we require to have four pods, like pods and the services and everything

we can wrap around each and everything in the deployment and we can deploy it.

So yeah, we will see basic example how we can deploy a deployment in our Kubernetes cluster.

But before that, what we will do is we will clear it out each and everything.

Yeah, So let me just do Cube CTL.

Let me just clear this first and I will run the command that is Cube CTL delete all with hyphen, F

and all, and you can see that it will delete each and everything available in the cluster.

So there's also a handy command.

When you want to delete each and everything in your cluster, you can use this command.

Make sure that nothing is important in your cluster.

Don't try this in your production or staging, and this is just for your local tool to clear it out

each and everything when you are working in the local environment.

So you can see that everything is clear.

So I'll just use the clear command.

And if you want to get all the resources at a time, then you can use the command cube.

CTL get all this will give you all the details available to you.

So this will give you pause, deployments, replica sets, services and everything.

Currently, we have only one resource available that is the default Kubernetes service.

So we are getting only one detail here.

Let me just clear it out and let's create the deployment.

So to create a deployment in Kubernetes cluster, the command is Cube.

It'll create everything you want to do.

We can do using the Create OC.

These are the sub commands.

If you want to get the help for it, you can pass hyphen edge to get the help information you will get

each and every flag's available and each and every sub commands available.

So with the create command, what we can create is we can create cluster role, we can create config

maps, cron job deployments in grass job and each and everything.

So we are going to create the deployment.

OC So let's clear it out again and let's do Cube.

CTL create Deployment deployment.

OC Now after this we need to give the name of the deployment that we want to create.

So I'm trying to deploy Engine X in our system.

So for that what I will do is I will use the engine X image available.

OC And for that what we'll do is we will give the name of the deployment as the engine X itself.

And after this I want to create the deployment I have to pass like which particular image I want to

use.

So if you go to the Docker up there is an engine X image available, so I'm going to use that.

So I will just define with hyphen, hyphen image equals to and then x OC, that's it.

You can see that I want to create the deployment and the name of the deployment is Engine X and for

this deployment what image I want to use.

So I want to use the engine X image.

OC This will be available in your Docker hub.

So that's the default image I am using here.

Once I run this command, it will create the deployment and it will create the port as well alongside

the deployment and a replica set as well.

So let's see that.

So once I run this command, you can see that we are getting deployment dot app slash engine X is created.

Once that is created, if I do Cube, it'll get all you can see that I got all the resources.

I got the pod.

You can see that this is the pod created.

Then the deployment is created and the replica set is also created and you will be able to get all the

information over here.

Now, if you want to get more extra information, then you can use cube Ctrl, get all hyphen for output

and you can pass wide.

So here you can see that it will give you more information.

You can see that previously this was the information and now you are getting more information.

You are getting the IP address, the nodes available, the nominated node, the readiness gates and

each and everything.

So you will get more information.

So whenever you are trying to get information this way, you can get using the hyphen or that is the

output flag.

So now let's see all the other tags.

Now if you want to get the details about a particular deployment or a port like what's happening or

what happened when the port was created or when the deployment was created, we can get all those details

using the described command.

So let's see that we can use to describe command.

So with this described command, we need to give what we need to describe.

So suppose I want to describe deployment and then after that you need to give the name of the deployment.

So the name of the deployment was Engine X.

So once I run this command, you can see that I'm getting all the information about this deployment.

What was the name in which namespace this resource was created?

When was created?

What is the labels information?

We will get to what labels is because labels are really important.

Part of the IS that's how each and every resources connects together.

And what is the selector, the how many replicas are there.

You can see that the desired are one.

One was updated because there is total one replica and one is available.

And what was the strategy type?

How you want to create the different parts that the strategy information and the entire port template.

You can see that which container we used.

This is the engine X image name of the this is the service and the image name is engine X and the conditions.

All this information you will get from your OC.

Now, if you want to get the information about the ports, we can get it similarly.

So let me just clear it out and let me just do Cube CTL get ports to get the port information and I

will do Cube CTL.

Describe board, and I will give you the name of the board.

Okay.

And this will give me complete description about this part so you can see that it is giving me what

is the name in which namespace it is.

What are the service accounts available?

What is the node available in which node it is will get the node and the IP address of it when it was

created.

The labels and the status of it.

It's running and the IP address and it's controlled by what?

It's controlled by the replica set and all this information.

You can see that from where the image ID is everything.

And here if you scroll down here, you will get the events as well, like what events occurred on that

particular part.

So you can see that first it was scheduled, then it pulled the images, then the image was pulled,

then the part was created.

And after that the part was started.

So first you can see that it was scheduled.

So in the earlier architecture video, we saw that that is a scheduler.

So that scheduler, you can see that that's from the default scheduler, right?

So that scheduler will identify like which particular parts needs to go to the which particular cluster

or which node of the cluster and all those information scheduling and everything will be handled by

it.

And after that we saw the cube as well.

Right?

So this is the cube plate information which will be there in the worker node and it will do each and

every operations according to the information by the master node on that particular work or not.

So there was information about creating the deployment with the ports engine X, so that was created

using the cube plate in the worker node.

So all these events and everything you will see from here using the described command.

So let me just clear it out now.

Now let me just again, Cube CTL get pass information.

Now, what you can also do is you can check the logs as well, right?

Because whenever there is something wrong or whenever we are doing any any debugging at the time, we

need to see the logs, what's happening within the container or what's happening within the ports as

well.

So for that there is a command to check the logs as well.

So the command is Cube, CTL logs, and then after that we need to give the name of the port.

So let me just copy this and give the name and you can see that you are getting the complete log of

that particular container running within the product.

So within this port, what container it is running, it is running the engine X container and that is

using the engine X image in it.

So for that you can see entire log is available here.

Similarly, you can get the logs for all the different parts also available within your cluster.

Now sometimes by just seeing the logs as well, the information is not enough, so sometimes you have

to go inside your port information or go inside your container and you need to check all the relevant

details, like what are the configuration that you did is correct or not.

The application should be in the correct path or not.

What is the logs and everything?

Whatever you want to see.

So for that we can go inside a port and a container and we can execute the different commands in it.

So to go inside a port, the command is Cube, CTL exec.

This is almost the similar command that we saw in the Docker as well.

So the command is Cube, CTL exec and hyphen IDE to get started with the interactive mode.

And after that, what is the name of the port that I want to check?

This is the name of the port and after that what command I want to run.

So I want to run slash B slash bash command OC And here you can see that you are inside the engine x/8f458

DC five B, whatever the idea is.

So you are now inside this port information and you are now inside this port.

Now whatever the command you will execute, All this command will be within your port.

So you can see that you are getting all the information, all the directories and the files and everything.

So this way, whenever you want to execute any commands within your port and the container, you can

use Cube CTL exec command to go inside that port and you can execute the different commands.

Now suppose we want to do any edit in our deployment.

We already created the deployment with one replica set and one port running with the engine X server.

So at any given point of time, one replica site would be there.

That means one instance of the engine x server will be running in our cluster.

But if you want to do any changes in the deployment.

So for that we have the edit command available.

So any changes in the deployment can be directed on using the edit command.

So the command is Cube, CTL edit and then you need to give which resource we need to edit.

We need to edit deployment and after the deployment we need to give the name of the deployment.

But if you do not provide any name information, it will refer to the one instance available.

Currently in our cluster we have only one deployment available so directly it will refer to that.

And if we hit enter, you can see that we got the entire deployment metadata available over here and

this is your entire deployment.

So you can see that this is the YAML file and this is something that we are going to create in the future

that each and every resources we are going to create in our Kubernetes cluster that is going to be using

the YAML file.

You can also create using the JSON file as well.

YAML file is more readable so we will go ahead with the YAML file.

So here you can see that you are getting all the information like the API version.

Use what is the resource type?

And the metadata information.

And what is the specs?

Information.

Specs.

Information will define what is everything in the container.

So here you can see that it is using the container as engine X, and after that you can see that you

are getting this status information as well available, like how many replicas are available and what

all the other status information.

But now what I want to do is currently in the specs, you can see that it should be only having one

replica available.

So I want to change this configuration.

So what I'm doing is I am just changing that.

I want three replicas.

Just it is.

Okay, so now my three instance of engine X server should be available.

So once I change any information here, just save this file and just close this file.

Once you do that, you can see that the engine X was edited and if I do Cube CTL get all, you will

get all the updated information.

You can see that two new pods were created and after that you can also see that the deployment was changed

like it needs three instance of available and the replica set is also changed that it needs three desired

state.

So now you can see that we just changed the information of the deployment.

Like rather than having only one replica, we need three replicas and we just change the file.

We just saved the file and everything was deployed according to the configuration.

We do not have to change anything.

Everything will be handled by the Kubernetes cluster itself.

So you can see that how easy it is to do each and every thing within the Kubernetes cluster.

Now you can see that all the resources and everything is created, but now we need to delete each and

everything.

So how we can delete.

So you can see that there are a lot of different resources available.

That is the replica set.

Is there, deployment is there, then the pods is also there, but we're only created one deployment.

So what we will do is if we delete the deployment, all the relevant resources also will be deleted.

So we will just delete the deployment and everything should also get deleted.

So the command to delete is Cube, CTL delete deployment and the name of the deployment.

So the name of the deployment is engine X OC.

Once we give this, you can see that deployment dot APS engine X is deleted and if we again check with

the commands, get all you can see that nothing is available, just a default service is available.

So you can see that we were able to delete each and everything and you can see how fast it is as well

to do all those operations.

So these are all the basic commands that you will use in day to day while working with Kubernetes in

the upcoming sessions, Whenever there is a new command available, I will point that out.

And those command will also be used in day to day activities like creating different resources, using

the YAML file and deleting all the resources, using the YAML file and all those stuffs that will come

in the later part.

But these are all the basic things that you need to know while working with the Kubernetes.

So in the next video we will see what is the Kubernetes yaml configuration?







Transcript - 

So in this video we are going to understand the kubernetes yaml configuration to understand better about

the kubernetes yaml configuration.

What we are going to do is we are going to create a basic yaml configuration and we will see different

things within the Kubernetes YAML configuration and how we can create the YAML files.

So you can see that this is our project where we have the different microservice components, we have

the other service with which we will be able to create the orders.

We have the payment service with which we will be able to do the payments.

We have the product service with which we will be able to get the products and the products and all

those information.

We have the service registry component that will interact with all of our different services.

We have the config server which will provide the default configurations and we have the API cloud gateway,

which will be the forefront of all our applications.

So these are all the components available.

So what we'll do is we'll just get the brief about how to create a Kubernetes YAML configuration for

service registry.

So we'll get the basic understanding what a YAML file should look like.

And in the upcoming sessions we will create the YAML configuration for each and every components.

So let me just create one folder here and I will just name it as it is.

Demo Kubernetes, in short, is also known as K is because starting from K and ending with S that are

in between eight characters.

So that's why it's also known as Kate S.

So I'm just creating the Kate s demo here.

And within this we will create the file.

So within this, what we will do is we will create one deployment and we will create one service.

So we'll get a basic understanding about the different resources available and the YAML configuration.

So for that what I will do, I'll create a new file and I will define deploy YAML when YAML file should

be created.

Now all the configuration will have four main things.

That is the API version that we're going to use, like which API version we are going to use for our

Kubernetes cluster.

Then the next important thing is the kind what type of resources that we are going to use.

The third important thing is this specifications like what is the specification for that particular

resources?

Like if you are creating a deployment, what image you are using, what container you are using, is

there any announcement variables and all those stuffs that all will come within the specification and

this specification section will be different based on the different resources that you use.

That's for obvious, because service will have the different specifications, deployment will have the

different specifications, and replica sets may have different specifications, and the next is the

status information.

We won't be providing the status information, but the status information will be added when you create

a Kubernetes resources within the cluster.

So at that time, whatever the status would be, that status information will be attached to that particular

resource as well.

So whenever we are creating, we will define three things.

That is the kind that is the EPA version, and the third one is the specification.

So let's start with the API version.

So we'll just define API.

Version year and the EPA version for the Kubernetes cluster that I'm using is APS.

We won.

That's the one thing.

Then you can define the kind information.

What is this kind?

I'm creating deployment.

So I'll define this is the deployment deployment.

Make sure the spelling is correct and the indentation is correct because we are working with the YAML

files.

The third information we had to provide was the specifications.

And one more thing we need to define is the metadata information for any files or any resource that

we create because we need to have some information about the resource also that we create, right?

So that also we can provide, so we can define meta data.

And within this metadata we can give any information.

So what I will give is I will give the name here.

So I'll just define this is a service registry.

That's a simple name that I'm giving here.

Now, within this pack information, we need to provide the configuration about the parse like which

particular or how the replica users should be able to select the paths for this particular deployment.

So for that we need to give the configuration.

So for that the configuration we will give using is this selector tags.

So within the selected tags we will use the match labels tag.

As I informed you earlier, that everything within the co-branded resources works with the labels.

So whatever the different labels that you define based on those labels, different resources will attach

together.

So the labels are really important part of the different Kubernetes resources.

And within this label, I will inform the app information which app it should be.

So I will just give the same information.

It should be service registry.

You can give any name here.

So now you can see that this specification that you described, that is for the deployment.

Now, within this you need to create a template for your pods like within the pod what it should be.

So let's define that.

So here with this selector, you have to define the template and you will get all the information here.

You can see that this template describes the pods that will be created.

Now, all this information, how you will get you need to install the Kubernetes plugin within the Visual

Studio code.

Now you can use any of the IDs or any of the text editor to create the HTML files.

I just prefer to use Visual Studio Code.

So I'm using Visual Studio code.

So within this template information, you need to give the template of how your pod should be so as

every resource will have the metadata information.

So we'll do the metadata here as well.

And within the metadata, we have to provide the labels which label that it should match each like we

described.

Right.

Like our deployment should match the labels for the service registry here.

Right.

So the same thing we will define like the label should be app and the value should be service registry.

OC.

So this board, whatever is created and for that replica set, whatever will be created, it will be

attached with this name.

Now you can see that we have to define this pack of this part.

That is the container that we are going to add.

So this specification, this specification is for the deployment and this pack we are adding inside

the template that is for your part that we are going to create.

Now within that what container we are going to use and the name of the container.

So what I will do is I will give the same name here and after this I can use the image which image I

want to use so I can use daily code buffer slash service registry.

So you can see that I will just define the name of the image that I want to use.

Now, in the previous tutorials of the Docker, we created the images using the Git plugin, right?

So for this service registry also we had created and those images are available within our Docker hub

registry.

So the same image I'm going to use here and after this you can define the tag information as well.

So my tag information is latest, so I'm just using the latest, whatever the version would be, you

can use that version.

Then you can define the image pool policy.

So you can see that three policies are available that is always, if not present and never accordingly.

The name itself will identify what you want to use.

I'm just defining always.

So every time the port is created, it will pull the images and it will create the containers on top

of it.

And then you can provide the port information.

So with the ports you have to define the container port like which particular ports will be exposed

by your container.

My services will be working on port 8761, so I can define eight, seven, six one year that I am going

to expose eight, seven, six one.

So this is all the configuration you can see that we added for our deployment.

We'll go through it again once it's very easy.

First for each and every resource is what we will be having is we will be having the EPA version, the

kind, the metadata information and the spec information.

After that, we'll be having the status as well once we create the resources.

So we just define that.

The EPA version that we are going to use is the APS slash V one.

If you have questions like what how to use here, you will get the information from the documentation.

You can see that you will get all the information from here as well.

So once you are using the Kubernetes plug in here, you will get each and everything.

So the API version I am using and the kind is deployment because I'm going to create a deployment and

deployment will contain the ports and the port will contain the containers and the containers will contain

your application.

So this much layer is and port is the basic unit or the most smaller unit of a resource that you can

create within the Kubernetes cluster.

So everything we are going to create is wrapped using the pod and those pods will be wrapped around

our deployment.

And for every resources, we will give the metadata information like what is the names and labels and

everything.

So for this we define that the name of the deployment is going to be Service Registry, and then we

added the specifications.

So within the specification you can see that we added that this should select the.

Labels, which has app as service registry.

So what this defines is like whenever I'm creating the deployment, the deployment will create the replica

sets.

So for that it should select all the parts containing the label as App Service registry.

That's a simple thing.

Anyone contains the label app's registry I should pick up and should be available for the deployment

for that.

Now we need to define the template of the port within this, like within this deployment.

What is the template that we want to create?

So within the template as well, we want to have the metadata information, and this metadata information

will have the labels as app service registry.

So whatever the number of paths that we are going to create here, those number of paths will be available

for this deployment.

So that is service registry.

Now, after this metadata, we define the specs of the.

Part.

OC You can see that respect is a description of a part.

So this spec defines what it should have within the part and we define that we should be having a container.

The name of the container should be service registry.

And this container should be created with the image that is daily code buffer slash service registry

call latest and the image will policy we define that is always so every time we have to pull the image

and we need to create the container and the ports, we define that the ports should be exposed at 8761

because 8761 is the port that we have defined in our container, in our images.

So now you can see that your deployment is created.

We have one file is created.

Now let's see another type of resource.

That is the service we will see in detail about service later.

But let's see the basics of service here.

So.

Let me create a new file and I will define that as we see yaml.

HTML service is nothing but a service on top of your parts available.

Right.

We talked about earlier as well.

Like pods will be having its own IP address and whenever there is some issue, paths will be destroyed

and the new pods will be created.

And for that pod new IP address will be created.

So whenever we have to connect our different services, we won't be able to connect using the IP address.

So we need to connect using these services.

So for those reasons we create these services and this service will be attached on top of our pods and

the IP address for this service and the host information for this service will remain constant and the

underlying pods may change any time.

So for that reason we are going to create a service and this service will also have the same thing like

the API version, the kind information, the metadata and the spec information.

So let's define API version.

And here I'm just defining API version as V one, the kind I'm defining as service.

And for this I have to define metadata.

So in the metadata we have to define the name.

So I'm defining name as service registry.

SVC.

And then we have to define the spec of this service.

So we define this pack and within this spec we need to define the selector like which particular paths

I need to select here.

So in the deploy as well, right in the deployment we define the selector, Right.

So you're also will define in the service that it should select all the things with the app.

Name as.

Service registry.

OC and this service will be exposing your porch information.

So we need to define the pause here.

And this may contain multiple ports will define the port information like what should be the port of

this service.

I'm defining that its port should be 80.

That is the default HTTP port.

So this service in any of the node will work with the port 80 that is the target port like what we are

targeting.

So here we define that port is 8761 that we are exposing from the container.

So this is the port that we are targeting.

So we'll define that.

Target port is 8761.

So you can see that this service will be running on port 80 and it's targeting the port 8761.

So what it will do is it will select the applications running with service registry as the name OC.

All those services will have the eight, seven, six one port as exposed and those will be used to connect

to this service.

And this service will be running on Port 80.

That's the simple thing it's doing.

OC So you can see that we have the deployment file created.

OC We have defined each and everything YAML configuration and for the service as well.

We have defined each and everything with the YAML configuration.

OC Now let's see how we can deploy this in our Kubernetes cluster.

So I will go to the terminal here and I will go to this directory and go to the rectory here, see daily

code for GitHub Spring, Microsoft Booth Microservices OC And here you can see that within this I have

the Gates demo.

Okay, So let me go to Kate's demo.

Let me clear it out so you can see that we have two files available that is deployed YAML and CC html.

And if I see it'll get all.

We do not have any resources available.

Now let's see how we can use this.

So we can use CTRL apply command to apply the configurations.

You can see that these are the configurations for the different resources.

So we have to use apply command to add all the things.

And with hyphen f command we need to give the file information.

So with an f I am giving deploy command deploy YAML file.

So with cube it'll apply command that is, apply my configuration with hyphen f, that is the file that

we have that is the deployed yaml.

Once I run this command you can see that service registry is created.

Now if I run the another command that is the cube CTL apply hyphen f service dot yaml file.

So for this also you can see that the service is created.

If I clear it out and I do cube Ctl get all, you can see that your pod is created using the deployment

file so you can see that the services are created right?

If we have the Kubernetes service, that was the default service and this is the service registry,

as we see that we created and we have the deployment created, that is a service registry and we also

have the replica set created.

So you can see that all those information are created based on the YAML configuration that we have provided.

So you can see that we just defined two YAML configuration files like in the YAML configuration which

is defined like what we need, like we need to have the deployment.

That deployment should be having a name and within that deployment what container I want to use like

what port information.

So we define the template for the pod.

And within that port we defined, okay, I want this container with the name this and within that container

should be created using this particular image.

And these are the pods that I need to expose.

So all those information I give here so you can see that it's a really simple file and these are the

only things that we are going to create.

And the similar thing we did for the service as well, like I define that API version, the kind information,

metadata, information, the specifications and everything, and we define the port information as well.

And accordingly, I applied all the changes and all the details are available here.

Now, if I want to delete each and everything I can do using the delete command, so I'll just define

Cube, CTL, delete command.

And what I want to delete, I want to delete all the resources mentioned in the deployed YAML file.

Similarly, I want to delete all the configuration mentioned in the SVC dot YAML file.

You can see that everything is deleted now.

If I see it'll get all.

You can see that nothing is available, everything is deleted.

So you can see that how easy it was to create the YAML configuration for our Kubernetes resources to

be deployed in our Kubernetes cluster and how we can apply all those changes, how we can remove all

those changes as well, very easily.

So this was all about how we can create the different resources in our Kubernetes cluster.

Using the YAML configuration, we will see a lot more in detail about the different YAML configurations,

the environment variables and everything in the later part where we are actually building all these

different resources for our application that we have created.

This is just a basic to get you started.

Now, in the next video we will understand about the Kubernetes namespaces.







Transcript - 
Now let's talk about Kubernetes namespaces.

Now, Kubernetes namespaces are just a virtual clusters inside a Kubernetes cluster.

So if you have a Kubernetes cluster and if you want to divide that Kubernetes cluster into different

small clusters that you can use or that you can achieve using the Kubernetes namespaces, there are

a lot of advantages of using Kubernetes namespaces, and there are different namespaces also available

by default.

So let's understand about the Kubernetes namespace.

So by default, Kubernetes comes with the default namespace where you can deploy all your resources.

So by default, if you can see here, whatever the resources or whatever the deployments, applications

and whatever you deploy in the Kubernetes cluster, when you're not defining the namespace, everything

goes into the default namespace.

So you can see that this is how it looks like within the Kubernetes cluster, within the default namespace,

like different applications installed.

But if you use different namespaces for the different things and you try to manage each and every resources,

it looks a lot cleaner as well.

So suppose if you see here that you have different applications deployed in the different namespaces

for your different types of components.

With that you will be able to handle your resources much better and you will be able to handle your

resources in a much better way.

So you can see that you can create different types of namespaces and within those namespaces you can

deploy the different components available.

Let me just show you how to create the namespace.

So if you go to a terminal and if you want to create the namespace, the command is to create namespace

and then you need to give the name of your namespace.

So suppose I'm giving my namespace so this will create a namespace within the Kubernetes cluster with

the name my namespace OC.

You can see that the my namespace is created now.

If you want to see the namespace, you can check using the cube CTL get namespace OC Here you can see

that your My namespace is created 15 seconds ago.

This is what we created and by default Kubernetes comes with the four default namespace that is the

default cube node least.

Q Public and cube system.

This Kubernetes dashboard namespace is a different namespace comes with the Kubernetes dashboard.

We will see that as well, like how to create the Kubernetes dashboard.

But these are the four namespace that comes by default, and this is the namespace that we have created.

So let's say about this namespace.

So by default there is a default namespace available, and this default namespace is for all the resources

that you create.

So whatever you are creating like deployments, port services, replica sets, everything will go towards

the default namespace if you are not defining a specific namespace.

So for all those resources, default namespace is used.

The other namespace is Cube System.

Cube system is reserved for all the resources for the Kubernetes system, like your API server, your

control manager, your scheduler, your Kubernetes proxy, all those are within the cube system.

It is advisable that we should never use the cube system as the namespace to create any of the resources.

We should always create a new namespace and within those namespace we should create our resources.

Then the third namespace is Q public.

This Q Public namespace is for the public access.

So all the information related to the Kubernetes cluster and everything will be getting from this particular

Kubernetes namespace.

So all the information we will get from here and the next namespace is the Kube Node list namespace.

So Kube Node list namespace is to create or to get the list object from the Kubernetes cluster.

So suppose if you have the pods created and for that pods, if you have to have the heartbeat information

like just for the health check information that you can have those lists from the cube node list and

all those information will be stored within the cube node list namespace.

So these are all the four basic namespaces available by default.

We just need to be aware about this by default.

If you are creating any resources, everything will go towards the default namespace and if you want,

we can always create a new namespace and within that namespace we can define our resources.

Now let's see how ideally we should use the namespaces.

So suppose if you have multiple teams available, right?

Suppose we have the project ABC and Project X, y, Z.

So both the teams are working on the different application, but the common custom for your organization

is same.

So how we should restrict all those resources within one team and restrict other resources within the

other team.

So for that the namespace can be created and within those namespace all the resources can be created.

So suppose within my ABC team I can create the deployments and everything using this particular namespace

itself where we can define like whatever the resources are created should be created within this namespace

for the project XYZ we can tell all the resources should be created in the XYZ only so that way we can

buy.

Forget all the resources using the namespace.

Suppose if you want to bifurcate using the environment that way.

Also we can do like all this resources within the cluster should go to the dev namespace.

If you have a staging environment, all these things should go to the staging namespace that we also

we can do all the different applications and services.

This will also we can do the bifurcations.

Now let's see how we can deploy the resources in the Kubernetes in a particular namespace.

So we already have a namespace created that is the my namespace, right?

And we already have the Kubernetes YAML configuration that is created.

So let's try to deploy this particular components within the my namespace.

What I will do is I will define the same.

Apply configuration that is the cube CTL apply hyphen f dot slash deployed XHTML.

By default, it will create each and everything in the default namespace, but if I want a particular

namespace I can define it using the hyphen and OC and then I need to give the name.

So my name is my name.

Space OC.

This is the name of the namespace.

Now if I apply all this changes OC you can see that all the things are created.

And if I do with Cube CTL, get all you can see that you are not getting all the information OC because

by default it'll get all will give you the information from the default namespace.

But if you provide with Hyphen in my namespace you can see that you are getting each and everything

so you can see that the other resources will not be visible to the different namespaces as well.

So all the resources that we created are within the mind namespace only and we are able to see now there

are different types of components and those components can be within the namespace, so those components

will be bound to the namespace and there will be some components which will be not bound to the namespace.

So I will add the link in the description for you to check like which resources are available for or

within the namespace and which resources are not available within the namespace that you can create

throughout the cluster, throughout all the namespaces.

So this way we can work with the namespaces.

We can create a different namespaces according to the requirement.

You can have any of the workflows created like based on the different teams, based on the different

environments accordingly, and we can create the resources accordingly.

So this would be a much neater way to create the different resources in the Kubernetes cluster.






Transcript - 
So now let's talk about the different services available.

So within this Kubernetes cluster, we have the different services available.

But first, understand why we need a services.

So suppose if we have the multiple applications, right, and for all those multiple applications,

we might have created the different paths available.

So currently you can see that we have App one, app two and three, and for all those applications we

have the different paths available.

That is multiple paths like F one has three, app two as three and app three as three, and as the paths

have the ephemeral lifecycle.

That means whenever there is an issue with the port, that part will die and a new port will be created.

And for that new port, new IP address will be created.

Right?

So suppose a new IP address is created and a new port.

And if that particular application one has to connect to the application three, then there is a constant

configuration change, right?

So a new IP address has to be connect to the new IP address and that is not a good case, right?

So what services provide us to have a one service attached to all the different paths that we have?

Right.

So suppose we have created the three different services that is the CCF one as we see APP two and as

you see APP three and all those services connects to that port for that particular application.

Now how they will connect to this ports like service, one will connect to this port only service,

two will connect to this port only and service three will connect to this port only.

Those will be happening based on the selectors that we define in the YAML configuration.

Those selectors will be defined based on the labels that we created.

Right?

We saw earlier in the configuration as well like that we will be creating the labels and based on those

labels the resources will attach together.

So this services will be having a label like it should select the labels and those labels will be having

the details of this port.

So that way this all ports will connect to this service, this all ports will connect to this service

and accordingly.

Now, whenever any port dies and a new port is created, that port will be also assigned to this service.

So now behind this service, whatever the ports are created, we do not have to worry about the ports,

that IP address and all those information.

We can directly have the access to the service and this service can talk to the different services available.

Right?

So it doesn't have to directly go and connect to the different ports.

Right.

Because based on the ports lifecycle, it can delete and recreate with the new IP always.

But for the services we will be having the same IP address and the same host information so they can

connect directly.

What it can do is service.

One can go ahead and connect to service three based on the services that we have defined similarly.

So this one can go ahead and connect to the service to based on the services that we have defined.

Similarly with service two to service three as well.

So you can see that all the paths will have the services in front of it and those services will interact

with each other in the Kubernetes cluster.

So that's the importance of service.

And for this services, we have the different types of services available.

So basically we have four services available divided into two categories.

That is the internal service and the external service.

In the internal service, we will be getting the cluster IP service and the headless service and in

the external will be having the node port service and the load balancer service.

Internal service means those services will be created that will talk internal to the cluster itself

and the external service will be created that can talk outside the cluster as well.

So let's start with the internal service that is the cluster IP first.

So in the previous example also we saw that we had created the deployment and for that deployment we

had created the service.

So here you can see that this is the sample that I'm showing you, that we have created a user service

app and this user service app has the different information and we have created this service also for

this information.

Now, this particular service will connect to this deployment based on the selector that we have defined.

So you can see that deployment has the label that is the app user service app, and the service has

the selector as app user service app.

So based on this connection, both will connect.

So this deployment will have the service in front of it.

And alongside that we need to match the port information as well.

So the deployment has the ports exposed in the container is 902 and in the service we have defined as

my server should be running on Port 80 and the target port it should go and connect to is 902.

So that way our service will assign to a particular port and the target port will also be assigned to

the container port that we have mentioned.

So this way a cluster IP service and the deployment or the ports will attach together and it will serve

the request.

The next is the headless services.

So when we need the headless services, so whenever we are working with the stateful set at that time

we will need the headless service.

So stateful sets we create for our sticky identity.

So whenever you need a sticky identity at that time we'll be using the headless service, and the headless

service will be created as this way.

Like in the specification, we will define that this is a cluster IP as then that means this is not

a cluster IP as well and we have not defined any time.

Well.

So that means this is be a headless service.

And the headless service will connect to a stateful set using the name itself, using the metadata.

So you can see that with metadata and name Eureka and the service name is Eureka, they will connect

and they will serve the request.

Now, whenever we will be creating the stateful set, it will be like you take a hyphen zero ug hyphen

one.

We will see about the stateful set in the later videos, but this is just the basic.

We'll just need to understand the headless service.

Yeah.

So this is also one type where we can create the headless service using the cluster IP as none described.

The next one is the node port service.

Now the node port service is the external service that you can create and you can see that you how to

define this pack type as node port.

Once we define this node port, it will create a node port service.

And for this node port service, we have the port range defined.

So you can only use the port from 30000 to 32 767 port information only to create this node port and

the node port service will be exposed based on the nodes that we have.

So within the cluster, different nodes will be available.

So for each and every node, that port will be exposed within the cluster using the node port service.

The next is the load balancer service, as the name itself suggests that it's going to create a load

balancer for your entire Kubernetes cluster.

So only one port will be created for it for your entire Kubernetes cluster.

Only one port will be exposed.

And based on that port open, it will take all the requests from the external world and it will traverse

all the traffic to the respective services that we have.

So this way we can define the load balancer service as well.

It will be exposed for a cluster and the node port will be exposed per nodes in the cluster.

So it is always advisable to not use node port services, always use the load balancer service.

So these are all the basics about the different services that we have in the Kubernetes cluster.

So we will be creating this type of services later in the configuration files when we are working with

our application, where we will create the configurations for those applications.

So this is just the basic services we should know, like which type of services that we get from the

Kubernetes cluster or how we can or which type of resources that we can create.

In the next video, we will be talking about the ingress services.






Transcript - 
Now let's talk about the ingress service in the commodities cluster.

You can see that within the Kubernetes cluster, we have defined that there is one app available and

for that app there is a service available.

Now, how ideally we talk to the services, we talk using the IP address that we have and we have the

port information.

That's how generally these services will be exposed and that's how we will be able to access those services

within the Kubernetes cluster, because directly the external traffic is not allowed within the Kubernetes

cluster.

So we forward those port information and those IP address will get and those IP addresses we will use

to access those information or access those services in the Kubernetes cluster.

But in the ideal world, we do not call all our applications using the IP address.

We call them using our DNS name.

Right.

Suppose you can see that we have a DHCP app for that.

We have a DHCP pod and the service.

But rather than calling all those services using the IP address, we call them using the domain name

that we have like suppose dxb dot com dxb dot or whatever it is.

Right?

So that's how we call those services when the external traffic is coming to your Kubernetes cluster.

So to handle all those things, we have the ingress component so we can create the ingress component

within the Kubernetes cluster.

Ingress services and ingress services will understand all those requests coming from the external world

based on the DNS, and it will route all the traffic to the respective services that we have created

within the Kubernetes cluster.

So for that reason, we have the ingress services in the community cluster.

So like all the other resources, we can create the ingress component using the YAML configuration,

We can define the kind as ingress and the metadata information and we can define the host information

in the specifications.

We have to define the host information.

And within those host information, we have to define the different paths.

Like when we get this path, we should route this particular request to which services accordingly.

This services will be your cluster IP services that is internal to your Kubernetes cluster.

So with this YAML configuration, we can define this ingress components in our Kubernetes cluster.

And for the host that we have provided.

Right.

So for that host information, we need to provide the entry point IP address as well.

So we need to configure those IP address such a way that that IP address belongs to the one of the nodes

in the Kubernetes cluster, because from that only we will be able to understand that this request should

come to this Kubernetes cluster and your request should be handled internally, right?

So that way we have to configure our DHCP app that is our DNS name in such a way that it points to the

node of the Kubernetes cluster.

And from there we have the different services available, and all those traffics will be routed to the

different services.

Now, all this information, you can see that it is traversing the traffic and each and everything ingress

component, right, is doing that, all those things.

So how ingress component will be able to do this right?

So ingress component or ingress service will also have a ingress controller.

This ingress controller will be able to handle all these requests.

It will able to handle, okay, I'm getting requests from this service and it should be routed back

to this services.

So by default, these are the different ingress controller available and any of the controller, any

of the ingress controller we can use to create our ingress services.

So what we have to do is we have to add this ingress controller in our Kubernetes cluster.

And according to that, our ingress service will work.

So you can see that this is the entire list available.

I will also add the link in the description for you to check the different ingress controllers available.

But ingress controller will handle each and everything for our ingress service that we create.

Now, all the cloud providers that we use, right?

All those cloud providers by default gives us the load balancer.

Those load balancer will be able to handle all our traffic, so that will have the ingress controller

for us and we can directly create the ingress services to handle the traffic accordingly so we do not

have to create or install any new components when we are working with the cloud as well.

But if you want to do within your local system, then with the cube add ons list, you can see the different

add ons available and you can see that there is a ingress component available, right?

So this particular ingress component, by default it will be disabled.

So we need to add it so you can add using the mini cube add ons, enable ingress command.

Once you add this or once you run this commands, this will be enabled.

You can see that the ingress component is enabled and now with this ingress component, you can create

the ingress services in your local environment and you can run all your services in the local environment.

But by default, whenever we are working with the cloud environment, by default we have the load balances

available and those load balancers are configured in a way to handle the ingress traffic.

So we don't how to add any add ons that we just need to configure our ingress services.

So this is all the basics about the ingress services.

You just need to know about how the ingress service works and what is the ingress service and how it

can be created.

Now, in the next video, we'll talk about the stateful sets in the Kubernetes cluster.





Transcript - 

Now stateful set is similar to what we had seen in the deployment.

It is also a wrapper around the port and the different resources, but the only key difference between

the deployment and the stateful set is stateful set will work with the stateful applications.

So suppose if we take the example of the MySQL database, MongoDB, Oracle, Cassandra, right, this

all stores the data.

So all this data will store the state of the application.

So whichever the application stores state for that, we will be using the stateful set.

So to understand more on the difference, let's take one example.

Let's take the example of a Java application.

And for that Java application we have the my SQL as a stateful application that will store the data.

So whenever the request will come from the browser to your application, it will contain some of the

data.

So suppose in our application it is containing the JSON data that it will pass the data and that data

from the Java application will be processed and it will move that data to the MySQL database and vice

versa.

So whenever we want any data, we will be querying those data.

So your application in your Java application, which is which you are going to deploy will be a stateless

application.

It is not storing any state of any information, it is just processing your data.

All the state information is being stored by your MySQL database.

So your MySQL database is your stateful application.

Your Java is just getting the data.

It's processing as you can see that we got the JSON data and it's passing the data back to your my SQL

and whenever your application needs any data, it can query the data and my SQL will send the data accordingly.

So this is the difference where your Java application would be, your stateless application and your

my SQL, where you are going to store the data that is going to be your stateful application.

And for the stateful application, we use stateful sets.

So let's see the difference between a deployment and a stateful set.

So we will get a better understanding about when we need a stateful set and when we want a deployment.

So let's start with the deployment first, like when we need the deployment.

So taking the example of a Java application, you can see that in Java application.

Also we have one port and in my SQL also we have one port available.

But when we scale the application right in the Java, you can see that I have instantiated multiple

paths available and for each and every port, when the new port is created, the port name would be

like the name of the port or the name of the deployment that we create.

Hyphen.

There will be a hash value generated and this has value will be completely random.

So whenever a port is died, right, and a new port is created, it will be having a new hash value.

So every time the hash value will always be different.

And for that what we do is to make sure that all the applications are able to connect to this port.

We create a service, and this service will connect to each and every part that we have.

And when the load balancing will happen, it will be based on the different load balancing scenarios

and we will not be able to make sure that which request will go to which node, which whichever nodes

are available, the request will be served to that particular nodes.

Now let's take the example of the MySQL in my SQL.

When we scale up the application and when we scale the application, what we need is we need to scale

our stateful applications.

Scaling a stateless application is really easy, but scaling a stateful application is really difficult

because we need to take care about the data consistency as well.

So when we create a stateful application, you can see that it will create the ports based on the identity.

So it will have a sticky identity in the stateless application.

That is the deployment.

When we create a deployment, a hash value is generated for the port, but when we create the stateful

application, a unique ID will be created starting with zero index and accordingly it will increment

the value and the port value will be assigned to that accordingly.

Let's deep dive into this because when we are working with the stateful applications, the most important

thing is the data consistency.

So ideally what we do is we create one node.

That node is for our read and write operation and we have the other nodes available that will be supporting

your read operations.

So currently if you see that I oh, my SQL zero is a master node and the other we have as a slave or

a worker nodes available for your my SQL.

So your application is designed in such a way that your my SQL zero node would be for your reading write

operation and my SQL one two and whatever the number comes that would be for your only read operations

accordingly, your application will work.

So what you would have defined in your configuration, like always go to my SQL zero port.

That is the container for my read write operation so that we have defined as a reader operation here

and for my SQL one and we have just defined the read operations.

Now whenever we do the scaling operation, what will happen is whatever the last state of the application

that we have write, suppose the last was created was my SQL to and based on the my sequel to the my

sequel three would be created.

So whatever the previous state would be that.

Revisited would be used to create the MySQL three.

So this way the new node will be created based on the previous node that we have already created.

Whatever the state would be.

Based on that state, a new node will be created and when we try to delete as well, when we when we

try to downscale the last node which was created, this was my SQL three, this will be deleted and

then my SQL two will be deleted, then my SQL one will be deleted and so on.

So you can see that with stateful set, it will be having a sticky identity and a sequential flow would

be there for scaling up and scaling down.

It won't be random as what we saw in the deployment.

One more thing to note over here is like whatever the operations that we talked about here, right,

like the replications and scaling up and creating the snapshots and everything, right.

When we have rewrite operations.

So all those scaling and everything has to be handled by the user itself, like by MySQL.

So all the configuration we have to provide Kubernetes does not provide any configuration to handle

all such cases.

Mainly Kubernetes suggests that we should use stateless application to be deployed in the Kubernetes

cluster.

But if we want to do stateful application as well, Kubernetes supports that.

All the configuration has to be provided by a user, like how your replication should be handled, how

your data consistency should be handled, and all those configuration for a particular stateful application

that we use like MySQL, Oracle, Cassandra or anything we have to provide those Kubernetes will just

support each and everything.

It won't provide you by default configuration for that.

So these are the differences between the stateful sets and the deployment.

We are going to use both deployment and stateful sets for our application because in our application

we had used the my SQL.

So for this MySQL, we are going to create a stateful set and with that stateful set we are going to

create our applications.

So we'll get to know in detail about how to create the stateful sets, deployments and everything.

But this was just a gist about what is the difference between the deployment stateful set and when to

use which one.

So that's all about the stateful sets in Kubernetes.

In the next video, we will see about the volumes in Kubernetes cluster.





Transcript - 

Whenever you are creating the applications for those applications, we use any database, and those

database will be a stateful applications, right?

Because it is going to store the state of your application, it is going to store the data.

And based on the port lifecycle that we have understood till now that whenever there is an issue with

the port, that port is entirely destroyed and a new port is created and it will assign a new IP address.

So if there is any issue in the port and if that port is destroyed, we might lose our entire data altogether,

right?

So we must not lose any data that we have stored.

Right.

So for that purpose volumes comes into picture, Volumes are nothing but your storage providers where

you can use any kind of storage that is outside your cluster and that storage you can use in your Kubernetes

cluster to store any data.

So you create a port in that part.

You have your stateful application like MySQL, Oracle or anything, but the back end storage, which

it use to store all the data to back all your file system and everything will be outside your Kubernetes

cluster.

It might be your local system, it might be your on-premise, it might be your cloud provider as well,

whatever it would be, but it would be outside your cluster.

And with that you will be able to access each and everything you can create inside the cluster as well.

But it is not recommended.

It is always best practice to use anything outside your cluster so you would be able to use it.

So this is what the volumes will be used for.

Now, there are important things that we need to consider when we are using a storage for our Kubernetes

cluster.

We need to make sure that storage should not be depend on the pod lifecycle.

So your pod lifecycle is like whenever there is an issue, port will be deleted, right?

But your storage should remain the same.

It should not be dependent on the port lifecycle.

So make sure that we create a storage in such a way that it is always available.

And whenever a new port is created, that port can use that storage which is already available.

OC storage should be available to all the nodes available.

We should not be like, okay, this storage is only available to one port or anything.

It should be accessible throughout all the nodes in the community cluster.

So wherever the port is created, right, those ports will be able to access your storage.

Storage should survive cluster crashes as well.

So whenever there is any issue with your entire Kubernetes cluster, your storage should be intact.

So that's why it is recommended to have your storage outside your Kubernetes cluster.

And that storage you can use as a volume in your Kubernetes cluster to access that storage.

Your volumes are just similar to all the other resources that you have, like RAM memory CPU.

And that way it would be a persistent volume.

So persistent volume is something that we are going to create in a Kubernetes cluster to access a storage

outside a cluster.

So the persistent volume is also created using a YAML configuration file.

You can see that we are going to create a persistent volume as a kind information.

We are going to give the API version and the metadata information.

And within this specification we can define the specs of that particular volume.

Like you can see that which is the host path information available, how we are going to access this

storage and what is the capacity and what is the storage class name.

You can see that all the details that we have defined as a YAML configuration and we can apply this

configuration to your Kubernetes cluster to get those resources ready for you.

And you can see that these are the different providers available.

So with all these providers you can access your storage and those storage, whatever the storage is

created within those providers, you can add all those as a persistent volume in your Kubernetes cluster.

Now, whenever we are going to use the persistent volume, you can see that persistent volume is created

in your Kubernetes cluster.

And this volume you have to use in your pod in any of the namespace.

So within this pod, what you have to do is you have to claim that particular persistent, persistent

volume as well.

Like this is the persistent volume that I want to use.

Once you claim it, you will be able to add in your pod and you will be able to access that.

So for that, we have to create a persistent volume claim as well.

That is that is also known as PVC, that also we can create using the YAML configuration.

So you can see that this is the YAML configuration where we have defined the API version, the kind

information as persistent volume claim.

The metadata that we give is the name of the persistent volume claim and the specification that we define.

We define that what is the storage class name, the access modes and the resources.

You can see that these are the different persistent volumes available and this claim is trying to get

a three GB version of it.

You can see that you will get this three GB assigned to this particular persistent volume claim.

Once we claim this, we will be able to access in our pods.

So you can see that here we have defined that persistent volume claim and the port as well.

And this task PVT claim, that is the persistent volume claim we have added as a volume in our pod information

in this packet information where we have defined that this is the volume that we are going to use and

this is the persistent volume claim and this is the claim name.

And here we can see that once we define the volumes here, we can add as a volume amounts in our container.

So this is how you will be able to create a persistent volume claim and up.

Volume in your Kubernetes cluster.

Now, who creates persistent volume and who creates persistent volume claims?

Ideally, your admin team, your resource providers will be creating the persistent volumes for you.

Suppose if you are working in any one of the project and you have the admin team or the intra team and

you need ten GB of storage to be used as a storage in your application.

So what you can do is you can request them.

Okay.

And the ten GB of storage of this kind, please create a persistent volume for us.

They can create a persistent volume for you and you can use that persistent volume using a persistent

volume claim into your resources.

So that's how generally it would work.

So it will be always a request that you will create manually, like, okay, this is something that

we need and you will be claiming that and you will be using it.

But what if you need something dynamically, right?

So storage class is a way that you can provision persistent volumes dynamically.

And this will be always like when persistent volume claims.

So suppose I want ten GB of storage and what I will do is I will create a persistent volume claim with

a storage class.

So what it will do is if the resource is available or not, it will dynamically create for us and it

will assign to me.

So that's why a storage class is used in the production applications.

So you can see that these are the different ways that you can create this storage class.

You can see that we have defined the storage class and these are the providers.

You can see that Google also provides and also provides and we can configure everything like how we

need.

And similar to other resources, we will be able to create each and everything using the YAML configurations.

So you can see that these are the different storage class providers.

We can use any of them based on our requirements.

So this was all about the basic information about the volumes, persistent volumes, volume claims and

storage class in Kubernetes.

So we are going to use this in our application when we are building the Kubernetes resource files for

our entire microservices.






Transcript - 
It's a really important part of any of the application and component is also provides a way to check

the health of our applications using the probes.

You can see that.

So it allows us aliveness, probe, readiness, probe and startup probes.

So let's talk about that.

So what's Aliveness probe so Liveness Probe defines that is your application live or not?

Is your application healthy or not to process your request or process the client's request?

So that is what Aliveness Probe defines your readiness probe is defined by.

Is your application ready to serve the request or not?

Suppose when you are starting your application, sometimes your application is dependent on the other

third party applications as well, so your application is only be able to serve the traffic once you're

all the dependent applications and a current application is completely started.

Once that is started, once everything is done, then only you can say that your application is ready

to serve the traffic.

So that is what a readiness pro defines.

So we can configure Aliveness probe and a readiness probe in our Kubernetes cluster for each and every

pass that we create in a Kubernetes cluster using this two probes and all these probes that is aliveness

probe.

And the readiness probe can be defined using the different ways we can define using the HTTP request.

We can define using the TCP, we can define using the commands, and we can also define using the gRPC

request.

It is in the beta for now.

But yeah, in the future it will be available to do the gRPC request to check the health of the application.

So let's take the basic example like what we can do here.

So this is a normal YAML configuration file.

Okay.

Here you can see that we have the API version.

The port information like this is going to be a port and it has the metadata information, okay, it

has a name, labels and everything and it has a spec information.

So here you can see it defines its having the containers.

The container name is this, the image is using the busy box and different arguments.

Okay.

So this is all the configuration about the ports.

Now you can see this is a aliveness probe.

This defines that your application is live or not.

So always you need to check that my application is running or not.

My application is running or not.

Is it healthy or not?

All this you need to do so.

For that you can see that first method used is the command execution.

So you can use any command to check the health of your application.

So here you can see the command is used is cat slash, temp slash healthy.

So this is what something used and then the attributes are used is initial delay second.

So after how much time they should start checking the liveness probe and at what interval you should

check the aliveness.

So you can see that file is defined.

So every 5 seconds it will check the health of your application.

So this way you can define the health check with liveness probe in your Kubernetes cluster.

Similarly, you can see that this is defined aliveness with the HTTP request.

So you can see this is a liveness probe with HTTP get earlier we saw with the command execute.

So this is with the HTTP gate.

And here we have defined the path like it will call this path on this port and the headers will be this.

And the initial delay seconds means three and period seconds is three.

So every 3 seconds it will try to check on this particular health endpoint.

Similarly, we can do with the TCP as well, So it will check for the TCP connection.

You can see that here we have defined with the readiness probe.

So it will check the TCP socket on the port information defined and you can see that LIVENESS probe

is also defined with the TCP socket information.

So this way both the probes can be defined.

It's very easy to implement.

We are going to implement in our application as well.

And similarly you can see that we have defined for the gRPC as well.

So if you have the gRPC request, you can define the gRPC as well on what port information you need

to check and the initial delay seconds and the other field.

So these are all the basics about the health checks in Kubernetes cluster.

You can see that we have just gone through the documentation yet, but this is something that we are

going to implement.

So I just want you to understand what is the difference between the liveness probe, readiness probe

and the startup probe?

We are going to use the readiness and liveness probe because Startup Probe is something that is being

used in the legacy applications generally, because legacy application takes a lot of time in starting.

So for that, a startup probe is defined, but here we are going to use the liveness probe and the readiness

probe.






Transcript - 

Now you can see that all the basics are covered.

We have gone through all the different resources available in the Kubernetes cluster.

We have also gone through the different commands available.

So we saw the different commands.

We saw how to create the YAML configurations, and we also saw how to apply those configurations in

the Kubernetes cluster.

So till this point, we are aware about all the different resources and how we can work with the Kubernetes.

Now let's go ahead and implement the different Kubernetes configuration for our microservices that we

have.

So now, before implementing the Kubernetes configurations for our application, let's first go through

the basics here.

We have this many applications available that is the Cloud Gateway Config Server Order Service Payment

Service, Product Service and Service Registry.

These are the applications that we have.

So this all applications we have alongside all these applications, we have other components as well.

We are going to use the my SQL, right?

So for the my SQL also we need to have the configurations.

We are also using the Zipkin and Sleuth.

So to install Zipkin also we need to have the configuration and we are also using the rate limiter.

So for that rate limiter we are going to use the read is.

So for the read is also we need to have the configurations available.

So these are the things that we will need to have the configurations.

Now let's get the basics clear out of all this.

We are going to create a deployment for each and everything.

Deployment will be nothing but a pod created for your application.

And for that part you'll be having one service.

So all the ports wrapped around a deployment and a service for that we will be creating.

So for the Cloud Gateway, we will create a deployment.

The deployment is nothing but a wrapper on top of your port.

That port will be having the image out of it.

We had already created the images for each and everything, but we will create it from here and for

the config server.

Also, we are going to create a deployment for the order service.

Also, we are going to create a deployment same way for the payment service on the product service,

but for the service registry we are going to create a stateful set because service registry will be

storing the state or it will be having a state of each and every application that connects to it.

Right?

Because what we need is we need to have this service registry available at any given point of time.

Everything should connect to that particular service registry and all the services will connect to that

service registry using the DNS name.

So for that reason we are going to create a stateful set rather than the deployment for the service

registry as it is storing the state of the entire microservices, all the different microservices.

Then comes the My SQL.

My SQL is also going to store the state of each and everything, right?

Because it is going to store the data.

It's your database.

So for that, my SQL also we are going to deploy using the stateful sets.

Zipkin and Red is will also store the logs and the rate limiter information.

But for those we are going to use the deployment for now, just to make it simple, we do not want to

create each and everything with the stateful sets.

So this is the plan that we are going to create.

And for each and everything, we are going to create a different configuration files and we are going

to deploy each and everything in our local environment.

And later in the tutorial, what we are going to do is we are going to remove the complete service registry

part as well.

Currently we are using services history, but Kubernetes also provides us the service registry, so

we are going to leverage that service registry also.

So when we are leveraging the Kubernetes services history, we do not require our own implementation

of the service registry.

So all this configuration we are going to do and we are going to deploy each and everything in the Kubernetes

cluster.

So it is going to be really fun in this video.

So let's start working on it.






Transcript - 


So now let's first prep up everything.

What we will do is I will create one folder here and I will name it as as OC.

So this is going to be my folder where I am going to create all the Kubernetes files.

The other thing that you can do is you if you are using the Visual Studio Code, then you can go to

extensions and you can search for Kubernetes and you can install this Kubernetes.

Then you will be able to use the different snippets to generate our code.

We know how to write each and every line we can generate each and everything by default.

We're using the Kubernetes plugin, so make sure that you install this Kubernetes plugin.

Once you install that Kubernetes plugin and you've created the S folder.

And we will make sure that at this point we have all the images ready in our Docker hub.

So what we will do is we will just open the terminal year, run the command to make sure that an image

is created in the dock.

So what we will do is I will go inside the Cloud Gateway and I will run my VN clean, install Gib Colon,

build command OC.

This will create the image for my cloud gateway and it will publish to my Docker hub.

So let's run this and we will wait for it to complete and we are going to do this for each and every

application.

So you can see that it is pushing the images and you can see that build a successful and the image is

pushed, the cloud gateway is done.

Let's do for the config server.

We'll run the same command that is the mx V and clean install gib colon build.

So this is just the prep work.

If you have not done in the past section to make sure that you do it because we are going to use all

these images to deploy all the things in our Kubernetes cluster.

So you can see that it is now pushing the images.

You can see that your config server is also done.

Now let's do for the other service.

And we can install the build.

Okay.

I can see that this is also completed.

So you get the gist, right?

Like you need to do this or you need to publish each and every images to the doctor up because from

that only we will be able to use.

Now, with this, we also need to make sure that whatever the environment variables that we have defined,

like whatever the configuration that we have defined, we need to make sure that those configuration

we can pass along via Kubernetes configuration as well.

We should not be hard coding anything.

There should be an option to pass the configuration from our outside as well.

So let's check all those information as well.

So within the Cloud Gateway, if you see if we go to the CRC main resources and the application dot

YAML file.

Let me just close it out and you can see that we have a way here.

We have defined the config server URL, so this is something that we can pass from outside.

But here you can see we have defined such a way that we can either pass local host or the host information.

We cannot pass the entire URL, right?

But when we are working with the Kubernetes, we do not need to have the port information as well.

We can directly access each and everything using the service name, right?

So this configuration is something that we need to change because this local host is something that

I can.

But inside, right?

Like if the enrollment variable is not available, I can pass on local host.

CALLER 9296.

Otherwise, this service name of the Kubernetes.

So what I'll do, I'll pass this as HTTP information as well.

Here's why.

To keep it right.

So now this will make it much clearer that if the environment variable is available, we are going to

pass a URL.

Else.

This is a localhost URL we need to use when you're working with the local OC.

This is the one configuration that we have added.

We just modified it OC y.

We modified it because we are going to pass each and every information from the configuration itself.

Nothing is going to be hardcoded.

OC Now all the configuration that you see for all this configuration we are going to create a config

map.

A config map is going to be created that will store all your configuration and that config map we will

use in our deployment that we create.

So this is something that we are going to create for this config map for config server server URL.

Is there any other information that we can use the rest of the things look okay to me in the cloud gateway,

let's check what the config server.

In the config server will go to the CRC main resources and the application dot YAML file.

This is the github URL directly can be accessible.

Right.

And here we have defined the Eureka server address and the localhost information that is the local host

path.

This is also looks good, right?

So yureka server address is also something that we are going to add as a config map.

And that config map we are going to use in our deployment.

Similar thing if you go to the order service and CRC main resources and the application or the YAML

file.

So yet also you can see that we have defined DB host here.

So this is something that we can pass, right?

This is going to be also your config map that you can pass on.

Right?

So this is all fine.

Is there anything else?

Conflicts over, you can define your right.

So here you can see that you have yeah, we have defined the config server.

So this config server, you can see we have just passed the localhost we need to pass as we defined

in the cloud gateway.

Right.

So in the Cloud Gateway, we have defined this way.

So this is something that we need to add here.

So we'll just copy this and we will add here.

OC.

So if config server URL is passed, it will use it.

Otherwise it will use the local host.

Okay.

Is there anything else?

This is a dave URL for your doctor.

This is all fine.

This is the same.

There is no configuration change required yet.

This is all good.

Now let's check for the payment service as well.

So we'll go to the resources application, not yaml.

DB host is already mentioned and yeah, we need to add for the config server.

So let's add for the config server.

Okay.

Config server is defined.

Everything else is good.

Then we can go ahead for the product.

Service in the product service will go to the CRC main resources application or the YAML file DB host

is mentioned and config server we will define as we define in the other service.

Okay, so this is all defined.

Now let's go to the service registry and in the service registry we'll go to the again as RC main resources

application, not YAML file.

And here you can see that we have defined the hostname.

Now this hostname would be for your local host, but when you're working with the Kubernetes cluster,

we need to define other than localhost, you need to parse the environment variable as well.

Right?

So we need to define that.

So let's define it.

So we will define instead of local laws will define host name call and local host.

Okay.

What this will do is this will be your environment variable.

So if this host name enrollment variable will be there, it will be used that call an otherwise local

host call and Eureka.

So this is what we have defined.

Now, these are the only application that we have that needs change.

Now what we have to do is once we define all this, all the confusion that we have defined, like all

the environment variables, whichever we needed now we need to build everything and we need to store

our images in our Docker up.

So we need to run all the commands again.

So I will open the terminal again and I will go to CD Cloud Gateway first and I will run the command

that is clean install Gible.

So I will do this for each and every applications again.

So Cloud Gateway is done.

Let's do for the config server.

I will go to the one fixed server and I will run the command and clean, install the build and we will

wait for it to complete and config server is completed.

We will go to the order service.

So.

And when you install that build and the order is also completed, we will go back and we'll go to the

payment service and we'll do the same thing.

And when clean install, git build.

And for the payment services also completed will go for the product service.

And we'll run the command machine clean, install the build.

And for the product services also completed will go back and go to the service registry and we will

do the same thing for this as well.

And you can see that this bill is also successful.

And now if you go to the doctor hub here, let me just sign in.

You can see that everything is published.

A few seconds ago, Service Registry, product Service payment, Service order, Service Config Server

and Cloud Gateway, everything is available here and we'll be able to see our latest tag information.

You can see that latest is there and the snapshot version is there.

So we are going to use the latest in all our Kubernetes configuration.

So this was all about the prep work here.

Now, we will go ahead and start creating our files.







Transcript - 
Now let's start with the service industry first.

So for each and every component, we are going to create the different files.

So in the Kubernetes directory, which is K.

S, let's create a new file and we'll define service registry stateful set because we are going to create

a stateful set.

So that's why I'm defining stateful set YAML.

This is a sample file we have created.

Now here we need to write our code and trust me, guy is writing the kubernetes yaml.

Configuration is really really easy.

This is going to be the most easiest part in our entire course because we are going to use this snippets

and we are just going to change a minor manage configurations.

What we used to do is earlier we used to write the API version information, right?

We used write API version, then kind information we used to give, then we used to give, then we used

to give the metadata information.

Right.

And this back information and everything, we were giving everything here.

But now with this plugin that we have added that is a component is we can create everything easily.

So suppose if we want to create a deployment, just right deployment, you can see that you will get

a suggestion that is Kubernetes deployment.

Just enter.

You can see that your entire configuration is ready and you can see you are getting the highlighted

as well.

So here you can do the name that is service.

Service registry.

And you can see that most of the thing is ready.

You just need to pass the image information and the port information your application is running.

You can see your deployment is created.

Similarly, if you want to create a service, then just define the service and your service is created.

You created.

You just need to give the name of your service and which app is going to target and the port information

and the target port information.

If you want to create a stateful set, then just define stateful set and the configuration for the stateful

set is created.

You can see how easy it is to create all the configuration files and all the templates and everything

will come.

We just need to provide a proper information.

So this is what we are going to use here.

So make sure you use it because we already covered the basics of the YAML configuration.

So now we are going to use this and in your project as well, you should be using this so you will be

able to create this configuration very easily and very quickly and you will not get any YAML configuration

error as well.

As we know in the YAML indentation should be perfect.

Right?

So here you can see everything would be indented properly.

We just passed the information.

That's it.

So now with this service registry we are going to create a stateful set.

Right?

As we talked about earlier, your service registry is going to store all the state of each and every

microservices.

This is going to be your stateful set.

And with the stateful set, we also saw that we have to create a headless service so both can attach

and we get a sticky identity to connect to it.

So this is a stateful set and we will give the name here.

Let's give the name that is Eureka, and we'll give the name here as well to Eureka and the service

name we will give this Eureka.

So we just define the stateful set and we give the name as Eureka.

We give the name of the app as also Eureka and the replicas.

I just need one.

And in the template information as well, we gave the metadata information as Eureka OC.

Now yeah, we need to add the containers in the containers, we need to give our image name.

So here in the image we are going to give us the code buffer slash service registry.

If you go here, right, you can get the services history, you can just copy this and you can add.

So we have just defined that and the port information, this is going to be eight, seven, six one

OC.

We don't need the name of the port, so we just can remove this.

We are not going to work with the volumes for the service registry, Right.

That's why we can remove that as well.

All these things.

We don't need it.

And yet you can see that your stateful set is ready.

Now we need to create the services for this port that we are going to create.

So when you're working with the YAML file and if you want multiple YAML files in a single file, you

can separate it out with triple dash and you can create another YAML file.

So now here we are going to create a headless service.

So with the service we'll be able to add a service and this is going to be my eureka service.

This is also going to be my eureka service.

Now, as we are going to define this as a headless service, right?

So for that we need to define the cluster IP.

So in the cluster IP, we need to define as none, because now once we define this, cluster IP has

none.

This is going to be your headless service.

If you see here, you can see that cluster IP is the IP address of the service and is usually assigned

randomly.

But valid values are not an empty string or a valid IP address.

Setting this to none makes a headless service that is no virtual IP, which is useful when direct endpoint

connections are preferred, not practicing it to the required.

So we are going to directly use this endpoint.

So we have defined this as a cluster IP and the port information we are going to use is as eight, seven,

six one and we are not targeting any port here because this is something that we are going to use.

This is going to assign to this stateful set.

Okay.

So that's why we will just give the name here.

And this name is also going to be Eureka.

So this is what we define as stateful set and a headless service.

Now, this is all the configuration we need for the Eureka service.

Okay?

But if we want to access this Eureka service, that is service registry outside your Kubernetes cluster,

we need to have one load balancer service or a node port service.

Right?

We saw that we have the internal service and we have the external service.

We define one internal service as a headless service, which will give us the key identity to connect

for other applications.

Right?

Other applications can connect to it.

But now if I want to or any user want to access this service registry, they will need one services

that can be exposed outside the Kubernetes cluster as well, and which can connect to this eight, seven,

six one port of this stateful set.

So for that, we can define one more service.

Okay, I can define one more service and I can define name as your load balancer, service and the selector

I will keep as Eureka the port information I will give as 80.

What I want is my service, right?

This service is going to run on port 88 is the default HTTP port.

So that's why what I want is it should run on default http port.

And they target what I'm defining is eight, seven, six one.

It should go and connect to the 8761 port.

And what I am defining is type of this service.

So I'm just defining this a node port service.

You can also go ahead and create the load balancer service because that's the ideal way you should do.

But I'm just showing you that this way we can create the node port service.

And in the other component, I will show you how to create a load balancer service.

So you will get the both examples like how we can create both the different services.

So that's the reason that I have defined Node over here.

So you can see that we just define one configuration file for our service registry.

The entire configuration of the service registry is done here.

We just created a stateful set.

In that stateful set, we defined the name of the stateful set and the port information, how many replicas

we need and the image we want to use, and the container port it's going to expose.

For this.

We defined the service as well.

We define service with cluster IP and then and we give the name as well.

That is going to be Eureka.

Now you can see that as the service name is Yureka.

We have defined in this stateful set in the service name, we have defined the Yureka.

So now whenever we want to access this service list, we can access using yureka hyphen zero, because

that's going to be your service.

Because every time when a stateful set is created, the port is created with the stateful set, it will

be appended with hyphen zero rather than the hash value.

So we can access using your archive zero dot service name that is going to be a YUREKA.

So that way we will be able to access this and we will add that configuration as well.

So you will get better understanding.

Other than this service, we created a node port service as well.

So this node port service is for the external users, like external users can connect to the service

registry and can see which are the applications are connected to the service registry.

This is for the external traffics coming to the Kubernetes cluster, and this is going to be used internally

to connect all the applications.

Now, this service registry is done.

Now let's go ahead and add the configuration for the config server.






Transcript - 

Now for the config server, let's go ahead and create a new file.

So within this case, this folder, I'll go ahead and create a new file and I will name as config server

deployment dot yaml.

Okay.

And for this config server, what we are going to do is we're going to create a deployment and for that

deployment we are going to create one service.

Easy stuff.

Now to create a deployment, we'll just use the deployment snippet and the deployment is created.

We will give the name as config.

Server app.

Simple.

Now we need to give the name of the image.

And the name of the image I will be giving is as Dili code buffer slash config server OC.

This is something that we have defined here as well.

You just uploaded it, right?

So this is going to be your config server.

So this is something that I copied and added here and you can see that the resource limit is also added

like what memory and CPU you should use.

Ideally, when you're working with the production application, you should define like how much memory

it should use.

But for the simplicity for now, because this is something that you need to tweak, like how much memory

your application is using.

You need to make sure that you do the load testing and everything.

Based on that, you will get to this particular number.

But for simplicity, we have only six applications and we can deploy each and everything within the

Kubernetes cluster.

I'm going to remove this, but you can add it based on your requirement.

It's nothing but resource limits and the memory information.

How much memory you need and what is the CPU you need.

So I just remove this for now and the port information, which port, my config server is going to work.

So my config server is going to work on port 9296.

So this is defined now with this deployment, we need a service as well, right?

So we'll define a service.

So we'll just use the service snippet here to create a service and we'll name as config service as we

see and we will name this as a config server app and my service is going to run on Port 80 and the target

port is 9296.

This is the one above one.

This will go here and my service is going to run on 80 by default.

When you don't define any type, this is going to be your cluster IP service because this is going to

be running within your cluster itself.

OC You can see your service is also created and your deployment is also created.

You can see that we just change a couple of details.

We just give the name here and we just gave the image information here and the port information like

which port it's going to run here as well.

Same thing.

When we create a service, we give the name of the service and which selector is going to use.

So the application that we are going to use is config server app.

This is something that we have defined here and the port information that we defined.

Now if you go to the config server and if you go to the RC main resources and the application YAML file,

if you come here you can see that we have defined one environment variable that is the Yureka server

address.

So this is something that we need to define, right?

Because this is the environment variable that we need to pass as the configuration.

Okay.

And as we defined earlier as well, that all the configuration that we are going to define, that configuration,

we are going to add using the config map and that config map we are going to use as the environment

variable.

Cool.

Right.

So let's define this.

What I will do is I will go here and in the case folder I'm going to create a config map.

Config map is nothing but to store the default configurations and whenever you want to store any passwords

or any credentials or or sensitive information, you can create the secrets.

So let's go ahead and create a new file that's going to store config.

So I'll just define config.

Maps dot yaml.

And here to create a config map, we are still going to use these snippets, so we'll just define config

map and you can see that the config map is created.

It's very easy.

So we'll give the name of the config map and we will give the data as key and value pair.

Okay.

So here I'm defining the name of this config map as you can see them, that is your config map and then

data.

In this data I'm defining key and value.

So key I want to define as your service address and the value I need to define as this SDB column slash

slash yureka hyphen zero.

That means whatever the.

Service that we have defined right yureka hyphen zero dot yureka.

Look, this is what we have defined.

So in the config maps, call an 8761 is the port information that we are going to use and slash yureka

is the path OC.

Now we just define this config map.

So this config map will be created and this config map will store this data information.

We can add a lot of data here within the same config map because config map will just store each and

every data in a key value pair.

Now this config map we can use in our config server to set in the enrollment variables.

Now to set the enrollment variables here.

After this port information, we can store the environment variables.

Okay.

And within this environment we can give multiple names.

So first name, we can go.

What was the name?

So if I go here in the application, this is the Eureka server address, so I'll just copy this.

And this is something that I need to add now for this Eureka Server address name.

I need to add the data value to it and this value I need to get from the config map that I have created.

So let's see how to get that information.

So here I can define I want to do I want to get value from config map key ref oc config map key ref

and within this config map key ref and it will define the key.

And what is the key?

Key is this eureka service address and we need to pass the name as well.

What is the name of the config map?

So the name of the config map is eureka siem.

Okay, so these two information is needed.

So with this information you can see that the value will come from config map and it will set to the

Eureka server address as the enrollment variable.

So here your entire configuration for your config server is done, your deployment is created and your

service is created with your config map and that config map as the environment variable as well.

Cool.

Right now let's add the configuration for the cloud gateway.





Transcript - 

Now to create the configuration for the Cloud Gateway.

I will create a new file here in the case folder.

I'll create a new file and I'll just name as Cloud Gateway hyphen deployment dot yaml.

And the file is created.

Now, this particular cloud gateway development will be again having the deployment.

So we are going to use the deployment snippet to create a deployment and we are going to give the name

as Cloud Gateway app and the file is created.

Now we need to give the image name, so let's give the image name.

So the image name was going to be daily code buffer slash cloud gateway.

Okay, This is something that you will get when you uploaded same year.

We are going to remove the resource limit for now because we need all the resources and the port information

is going to be 1990 because Cloud Gateway is going to be running on Port 1990.

You can see that your deployment is created.

Now let's create the service.

So we're going to use the service snippet here.

And the name of the service would be Cloud Gateway, as we see.

And the name of the app would be Cloud Gateway App and the port would be 80 and the target port would

be 1990.

Now, until now, we saw the example to create a cluster IP.

We saw the example to create a node port as well, and we saw the example to create a headless service

as well.

Now this cloud gateway ideally will be your API gateway and all the external traffic will come to your

API gateway.

And from the EPA gateway, only all the traffic will be routed to your internal service.

So this will be your forefront of your entire microservices.

So for this we are going to create this as a load balancer service for that in the specs, we are going

to define type as load balancer.

That's it.

Your load balancer service is created.

You can see that how easy it is right now, this particular cloud Gateway will also have some configurations

and some and moment variables, right?

So if we go to the Cloud Gateway and if you go to the CRC, main resources and the application YAML

file, you can see that we have one config server you will define.

This is going to be one and moment variable and the other environment variable is going to be what we

have defined in the config server.

Okay.

So if we go to the.

Config server application YAML and if you go to this configuration.

Right in the application of the YAML file, you can see that we have defined a configuration, right?

And here we have also defined the Eureka server address.

Right.

So two and one variable is needed.

This is something one and moment variable.

We have already defined that we use in the config server.

Right now we need to define for the config server url as well.

So let's go ahead and define that.

So what I will do is I will go to the K.

S, I'll go to the config maps and here I can create one more config map.

Right.

And this config map will store the data for the config server.

So let me just create a config map and this config map, I will name it as config config map.

This is going to be a config server and I will give the data as config URL and the value as http.

Colin forward slash forward slash name of the service.

This is the name of the service we created for the config server.

You can see that this is the name right config server as we see.

So this is the same name that I have given here.

I will just rename this to here.

This makes more sense, right?

Yeah.

Config server as we see.

So this name and this name in the config map should be same.

So you can see that we are able to access each and other resources either in other services by directly

the name of the service itself.

Right.

We do not have to provide any port or anything by just name.

We will be able to access it.

So that's why we have just defined config server as we see here.

Okay.

Now this config scheme we can use in our cloud gateway, first of all, what we'll do is we'll go to

the config server and we'll copy this because this is one environment variable that we need, right?

So we'll copy this, we'll go to the Cloud Gateway and we will add one environment variable.

Okay?

One enrollment variable is added.

Now I will copy this and I will add it again.

Okay.

And I will change the name.

So the name is config server URL, so I'll just copy this and I'm going to give this config server earlier

and the key is going to be config URL and the name is going to be config CM.

You can see that the cloud gateway configuration is added.

We define the two environment variables.

That is a Eureka server address and the config server URL as well.

And this service is also we have defined so you can see that your cloud gateway is also done.

Now in the next video we are going to do the configuration for our order service.






Transcript - 
Now for the other service.

Let's create a new file here.

And will define us order service, deployment yaml file and this yaml file or the service is also going

to be your deployment plus service.

So let's create a deployment.

And we'll give the name as order service app.

And we are going to give the image name.

So let's give that.

And the order service image name is going to be daily code buffer slash order service.

I'm going to remove the resource information for now and the container port is 8082.

So your container port is also defined.

Now we need to define the service.

So let's define service here and we'll give the name order service as we see, and the name of the app

as order service app.

The port is going to be 80.

It is going to be HTTP board write default and the target port, what it's targeting, it's targeting

8082.

Cool.

Right.

You can see the configuration is done now for this as well.

We need both the configuration as we defined in the cloud Gateway.

So that is going to be your config CM as well and your.

Yureka server address.

So there's all things we need, so we'll just copy this and we'll go to the order service and we will

add all this information.

So you can see that your configuration is done.

Now one thing is still remaining if you go to the order service.

So let me just go to the order service.

And in the order service, if you go here in the main Java resources, we have the DB host as well,

right?

Which particular DB we need to connect to.

So for this will come later.

But we have added for the config server URL and we added for the server address.

Similar thing.

Let's do the same thing for our payment service and product service and then we'll come for the DB host.

So let's go ahead and create for the product service and the payment service.

So let me just create for the new file product service, product service, deployment, YAML file and

in this product service, we need to create a deployment and service.

So let's create a deployment first.

And the name would be.

Product

service app and we need to give the image name.

So let's give that.

So the image name is product service, the port information.

So let me check the port first.

Okay.

So for the product service.

We have the port as.

h08 to sorry.

This is payment right for the product.

Product service is 8080.

So we'll go to the product service here and the port is 8080 and the product service is defined.

Now let's add the service for it.

So we'll just define service layer here.

Okay.

And the name of the service would be product Service SVC and the name would be Product Service App.

It's going to be on Port 80 and the target port would be 8080.

And this is also going to be have the environment variables.

So we'll go to the order service and we'll copy the same environment variables.

So this is something that we need.

Okay.

And when vegetables are added now similarly, we need to do for the payment settings as well.

So let's go ahead and do the do for the payment service.

So we'll just define payment service, deployment YAML.

And for here as well, we are going to create a deployment in the service.

So let's define the deployment and the name of the deployment as payment service app.

And the port information is 881.

We don't need the resources here and we need the image name.

So we'll define image name here.

That is going to be payment service, daily code, buffer slash payment service.

Okay.

And now we need to define the service.

So service we have defined payment service as we see and payment service app.

And the port information is going to be 80 and the target port is 80, 81.

Right.

At 81.

And this also will need the same enrollment variables.

So let's go to the other service and let's copy this environment variables.

Will copy this.

We'll go to the payment service and we'll add the enrollment variables.

So you can see that the enrollment variable for the payment service is also added.

So now the other service is created, payment service is created and the product service is also created.

Cool.

Right now let's go ahead and add the configuration for the mix equal, because that's going to be very

important.

And with that, my SQL, once that is added, we need to add the configuration back to our order service

payment service and product service to connect to that MySQL.





Transcript - 

Now let's add the configuration for the my SQL.

So let me just create a new file and just define my SQL.

Deployment yaml.

Now.

Yeah, we need to do a lot of things because this is going to be your stateful application and you are

going to store the state of the application.

And when we are going to store the state of the application, we are going to create a stateful set

with the handler service.

Plus we are also going to create a volume as well.

So all the data will be stored in that particular volume outside your cluster.

So what we will do here, we're going to create a persistent volume.

Okay, Then we are going to create a persistent volume claim.

Then we are going to create a stateful set for our SQL, my SQL, and then we are going to create a

headless service for that.

And we are also going to create a configuration, whatever the configuration is required for our my

SQL.

So these are the details that we are going to create.

So first, let's start with the persistent volume, then position volume claim, then stateful set,

then headless service and the configurations.

So let's add those things.

So first, to create a persistent volume, we are going to use the snippet.

So persistent volume.

Okay.

So you can see that these are the details that we need to add for the position volume by default.

You can see that it is using the NFS storage here, but we are not going to use the NFS storage here.

So let's define the name.

My SQL persistent volume and the size information I am giving us is one GB So we are going to remove

the information which we don't need.

So I don't need volume mode here, so I'll just remove this.

I don't need persistent volume reclaim policy.

I can remove this and each storage class name as manual.

Okay, you can give me any name here then.

I don't need this mount option with the NFS.

I want to add the host path because I'm running in my local and I am adding the path of my host machine.

So I'm just defining host path here and you can give the path of your system wherever you want to store

those volumes.

So I'm just giving this path and if you are in the Mac, you can give slash MT slash data OC in Unix,

either Mac or Linux.

Okay, you can go this way.

Currently I'm running in Windows, so I've just given this way that C temp test files, it will add

all the things.

So this way you can give the data and I will define type here, define as directory or create OC.

So here you can see that your persistent volume is created where we define the kind of position volume

we give, the name, we give the storage, like we need one GB of storage and we give the storage class

name and the access mode that we require that is read at once and we give the path information where

we need to create a volume OC.

We just give the path.

So your path you need to give OC and we give the type as directory or create.

So this is all the information we need now.

We need to create a persistent volume claim because based on that claim, we can use that in our MySQL

deployment.

So let's do that.

Let's create another file and let's define persistent volume claim so you can see that all the information

came here and I'll give the name as my SQL persistent volume claim and the storage.

I'm going to use this as 1gi OC.

This is the same that we need to do, and then I need to give a storage class name rather than volume

mode and the access mode as read write once and your persistent volume claim is also created, you can

see that how easy it is, right?

So you just need to give this information.

So with this storage class name and this storage that we have provided, it will get this particular

storage information.

Now we need to create a stateful set for our MySQL.

So let's define the stateful set here and we'll just define my SQL app as my SQL service name as my

SQL.

Okay.

And the replicas, I need one only and this information we need to do.

So let's define all this as well.

So within this template in the specs container, we need to give the image as well.

So let's give that SO image I'm going to use as my SQL 8.0 and the ports I'm going to use as 3306 because

that's the default port exposed.

And the port name I'll give as my SQL.

Okay.

And then we need to give the environment variable as well.

So let's give the enrollment variable.

And the enrollment variable name would be my SQL root password, because whenever you are setting up

your my SQL, you need to give your root password.

And the root password which I have given in my applications, right?

If I go to the order service and the application, you can see that I have defined root.

Okay, by default the username would be root and the password would be root.

So let me give you the value as.

Well, rude.

So any moment variable is set.

Now we need to mount the volumes as well.

We will come to that.

Now, whenever we want to mount the volumes, we need to first define the volumes as well.

Currently, volume claim templates is defined, but we are going to remove this and we are going to

add the volumes to it.

So let me just remove this thing and let's define volumes.

And within this volumes, we can define the name.

And the name I will be defining is my SQL purchase.

Tent storage.

And this will how what this will have the persistent volume that we have created persistent volume claims.

So let's define that.

This stent volume claim and the claim name.

What is going to the claim name?

This is going to be the my SQL PVC.

Okay.

So one volumes we have attached now this particular volume I can use in the volume mount.

So in the volume mounts, I can give that I want to use my SQL persistent storage and the path should

be where slash lib, slash my SQL, where it is going to be installed within within the container itself.

So that path we have given here, so you can see that we created a persistent volume and for that persistent

volume we created a persistent volume claim and we created a stateful set in that we defined that we

want to use the MySQL 8.0.

We define the port information and we define the enrollment variable as well, and we define that within

this stateful set.

We are going to use this volumes and this volume is going to be mount here.

So that mount we provided here.

Now let's create the service for it.

So let me just create a headless service here.

So this is going to be the service and this is going to be the my SQL Service app is going to be my

SQL and the ports is going to be 3306.

Don't have to give the target port here and the in this specs, I'm going to give cluster IP as none

because this is going to be your headless service.

Okay?

Now your service is also defined.

Now one more thing we need to configure here.

This is going to be really important.

Now what I want to do is I want to run a scripts, okay?

When I start my application because what scripts I need to run, I need to create the database actually.

Okay, So if I go to the order service and if you see here in my application, I am using the payment.

DB Similarly for the order, I'm using the order.

DB And for the product and I'm using the product.

DB So all those three databases, I want to create all those schemas I want to create in my database

server.

So when I want to create it, when I am starting my container right for the, my SQL at that time,

if those databases are not created, I want to create it.

So for that what I want, I want one script and that script I need to attach as a volume in my my SQL.

So that is something that I want to do.

So I want one script and that script I want to attach as a volume here.

And that is something that I want to execute when the server starts.

So for that, what I need, I want to attach a script for that.

I will create a config map where I will define some data and that data I will use as a volume.

So let me define here as a config map.

Okay.

And I will define as my SQL in it db config map.

Okay.

And in the key I'm defining as init dot SQL and I'm defining pipe here and here I can define my scripts.

So these are my scripts.

Okay.

You can see that the simple scripts create database if not exist or the DB create database if not exist.

Payment DP create database if not exist.

Product.

DP So these are the scripts.

Cool, right?

And this scripts you can see that are part of endpoint SQL as a key of my SQL init DB config map and

this config map I can use as a volume and I can attach as a volume mount.

So here I can use name as my SQL init db and I need this information from the config map.

Okay.

And the name of the config map that is going to be my SQL init BCM.

So this config map I attached as a volumes here.

Now this I can use in the volume mount, so I can use your hyphen name as the my SQL init db and in

the mount path I have defined here.

Okay.

Now you can see that this is something the init db file that I have used.

Okay.

So this is just a basic configuration.

When you google for it, you will get all this information how to mount your initial scripts and you

will get all this information from the Kubernetes documentation itself.

It is from there and only so you don't have to worry about like how we will get all this information.

Each and every thing is available in the Kubernetes documentation.

I have just removed all the unnecessary details to just avoid the complications.

So hope now everything is clear.

We will just go through it once, like what we did.

We had to create all these things like we need to.

We needed to.

Create a persistent volume to define our storage, we needed to define persistent volume claim.

So with that claim, we can get the access to that storage and we can use it in our deployment.

That is the paths that we're going to create using the stateful sets.

And then we had to create the headless service with a stateful set and the configuration config map.

So here we define the persistent volume, we give the name and we define that one GB size we need.

We give the access as a read write once with a storage class as manual, and we give the path information

like this is a path I need to create a volume for.

And then we created a persistent volume claim.

This particular position, while volume claim, will use the information like storage and the storage

class name to connect to this or to get the information from this persistent volume.

Okay, so this will connect to this.

Now, we created a stateful set.

In this stateful set, we defined that this is going to be my SQL and we give the service name as a

as well as my SQL.

And with this my SQL service name, we are going to create a headless service.

We define we need one replica set and we give the MySQL 8.00 as the image.

We define the container port as well that this is going to be running on 30 306 and we give one environment

variable that my SQL root password should be root.

Then we define the volumes, whatever the persistent volume claim we created, we use that to attach

as a volume here.

Okay.

And similarly, we created one config map here, and this config map is nothing but a config map creating

this script.

So these are the three scripts that I want to execute when I start my SQL and that I what is in it,

not SQL data and this my SQL DB config map I use as a volumes here and this volumes I have attached

as a volume mount here.

Okay.

So everything is attached here.

And alongside that I had created a headless service defined as cluster IP, none, which will .23306

with the name my SQL.

Okay.

So you can see that your entire configuration for my SQL is completed.





Transcript - 


Now let's use this configuration to connect our order service payment service and product service to

this MySQL that also we need to do.

So whatever we define right, like headless service, that is my SQL and the service as my SQL.

So what it will do is it will start as my SQL hyphen zero dot my SQL, that is going to be the host

name for this SQL.

So for this DB host, we need to define a config map and that config map we can use as the enrollment

variable in our different services.

So let's define a config map here.

So I'm going to create a config map.

Okay, new config map and I will name this as my SQL config map and I'll define host name here and whose

name I define as my SQL hyphen zero dot My SQL as we defined similar thing for the Utica Light, Utica

and zero Utica, similar thing we define for the my SQL as well.

Now this is something that we can use in the order service, payment service and product service.

So let's go to the order service and we need to have one environment variable.

The environment variable name was DB host.

So I just copy this and I'll paste here and this is going to be DB underscore Host Okay.

And the config key map reference key is going to be host name and the name would be this OC.

So config name name is my SQL Cwm.

The key is hostname and all this thing will be attached to the DB host.

So let me just copy this and go to the payment service and attach it here.

As a DB host, I'll go to the product service and do the same thing.

OC DB host is attached here so you can see that all the configurations are done now for my, my SQL

OC.

So my all the configuration for the order service payment service product service is done with the MySQL

connected.

We have service registry created, we have Cloud Gateway and Config Server is also created.





Transcript - 

Now we need to connect to the Zipkin as well.

So we need to have the Zipkin deployed as well.

Zipkin will collect all the distributed logs.

So let's create a deployment file for the Zipkin.

So I'll create a new file and I'll just name as a Zipkin deployment.

Now, for the Zipkin, we will be having a deployment and a service that each and every services can

connect to.

So let me just create a deployment here, and I will name it as a Zipkin.

And here, let me just remove this resources here, because I don't need this resources.

The port information is going to be nine for one because here is something Zipkin works.

The image name we need to give.

That's going to be the open Zipkin.

So let's define that.

Now, for this, let's define the service.

So here I can define service here and I can define Zipkin as we see, rather than.

I'll just define as Zipkin only.

Okay.

So directly with Zipkin, I can attach.

And here the Zipkin, the port is going to be nine for double one.

And this is also going to be nine for double one defines as physical only because this is something

that we have attached as a SVC to each and every services.

Right.

So this service is also defined and this is going to be your cluster IP service.

Now suppose if you want to access Zipkin, Zipkin provides the Zipkin dashboard, right?

So if you want to access the Zipkin dashboard from the external traffic, like if a user or all the

developers want to access this Zipkin server, then cluster IP is not suitable.

So for that we need to have a load balancer service also created so actual user can connect to this

Zipkin service.

Right.

Because this is going to be your cluster IP which is going to be used to connect to each and every different

services that we have.

Right?

So let's define that load balancer service.

So I'll just define a service here and I'll define Zipkin Load Balancer Service, and the app is going

to be Zipkin and the port information is going to be nine for one target.

This is also going to be nine for double one.

Okay.

So yeah, you can see that we have defined the type here.

We can define type as load balancer.

So your load balancer service is also created.

So we created two services.

Yeah.

Cool.

Right.

So your Zipkin is also done.

I have defined the wrong spelling, so let me just correct it.

Okay?





Transcript - 

Now let's go ahead and add the configuration for the radius, because we need a radius as well to be

deployed because we are going to use the rate limiter with that is right.

So let's add the configuration for that as well.

So here I'll create a new file and I'll just define red is deployment dot YAML.

Okay.

And here I'm going to create a radius deployment and the service for it.

So let's define the deployment here and I will name as radius app.

And here let me just remove this resources because I don't need it and I need to give the image and

here I'm going to give the major as radius 7.0.4.

This all things you can get from the Dakota book, like whatever the version you need you can define

here.

And the port information I need is 637, nine because that is the default port it's going to expose.

Now, whenever I want to start, I want to start as a ready server, not as a CLI.

So I want to run that command as well.

Like it's going to run as a server and we can connect to it.

So let's define the commands for that as well.

So in the command I can define this as a radius hyphen server.

This is the command that I'm going to run.

If there is any argument, I can define the argument as well.

And in the command, I'm going to define protected mode as no.

Okay.

So these are just the basic details.

You will get all this details when you search for it.

In the read is documentation itself and the Docker hub documentation.

Whenever you will go to the Radice image image page, you will get all this information.

Okay, so we just define one container with the radius 7.0.4, it will be exposing on 6379 and we are

going to run the radius server as a command.

Okay.

Now we need to have a service for this as well.

So let's define that.

This is going to be readiness this and the app selector is going to be ready is app port is going to

be six three, seven, nine and this is also going to be 6379.

All the port information we have defined here.

Now, here you can see that as we have defined that we are going to use red is here with the name Red

is okay.

We need to add the configuration in the Cloud Gateway because Cloud Gateway is the only application

that uses red is so Cloud Gateway needs to know that it's going to connect here in this red is with

the red is as a service name and the port is 6379 by default.

If we don't provide it will go to check in the local host 6379.

That's the auto configuration from the springboard.

But if we want to connect using this in the Kubernetes cluster, we need to define that configuration.

So let's go ahead and define that configuration.

So what I will do, I will go to the Cloud gateway in the application YAML file.

I will define this configuration that this is spring dot reddest, dot host is reddish and spring dot

reddish dot port is 6379 So that's the configuration that I have defined here.

This was missing.

So now it will be able to connect to this on this port information.

Now once this is done, we need to again go to the Cloud Gateway.

So I'll go here, CD Cloud Gateway, and I need to run a command and then install the build.

So we will get this information added to our Docker hub.

Now, similarly, we need to add the configuration for the Zipkin as well.

We define the Zipkin here, right?

So if we scroll down here in the Kubernetes, the Zipkin deployment, we define this Zipkin service,

right?

So currently by default, we have not added the configuration because it was working with the default

configuration.

Right?

But now, as we are everything adding to the command cluster, we need to define in our config server

that where we need to go and connect to Zipkin.

So that configuration we need to add.

So for that we will go to the spring up config.

That is our configuration provider, right?

So here we can edit the configuration and add our configuration here that we are using.

Spring zipkin baz url zipkin hyphen svc because that is something that we have defined, right?

If we go here.

We have defined as a Zipkin as we see it.

So this service name we need to give here.

So Zipkin is physical in 941941 is our port information.

So once we define it, will try to connect to the Zipkin.

So let's commit.

This changes.

Okay, so now I will be able to connect to Zipkin as well.

And now we will be able to connect to Redis as well.

So now all the configurations are done now.

Till now we have not deployed everything yet, but now we are going to deploy each and everything to

our Kubernetes cluster.

I am super excited.

Hope you are as well.






Transcript - 

Now let's deploy each and everything and let's see.

Everything is working fine or not.

So we have all the files created.

So we'll go to the terminal.

And let me just clear this and we should be in the correct directory.

Let me just check it here.

So yeah, within GitHub slash springboard microservices, we are correct.

Okay, so I'm here and if I do ls here, this is let me go back here.

Okay.

Currently we have the Kubernetes gate as directory and in that we have all the files available, right?

So that is something that we need to deploy.

So if we clear this and if we do, it'll get all you can see that we have nothing available.

So no resources is available, but we need to deploy each and everything.

Now we have 6 to 7 files available, right?

And we need to deploy each and everything.

But before that, I will show you one thing about the Kubernetes dashboard.

Kubernetes Dashboard will get you to see each and everything happening in the dashboard available.

So to start the Kubernetes dashboard, the command is Mini Cube.

Mini Cube provides a dashboard functionality.

So we will be using mini cube dashboard here to start our dashboard.

So once we do this, we will get the dashboard.

So let's run this.

And you can see that the dashboard is started currently.

Currently, if you see there is no pods available, no replica set, no stateful set, okay, nothing

is available.

And here you can see that you will get a different namespace available in everything.

Okay, so all this details about the Kubernetes cluster you can get from the Kubernetes dashboard,

and this will be very helpful to identify and visualize all the different resources available you can

get each and everything from the CLI commands as well.

But this is also a good way to get all this information.

So what we will do is we will go here and we will deploy each and everything.

So to deploy the command is Cube, CTL, apply hyphen F, and then we need to do the file name.

But we have multiple files, right?

So rather we will just provide the folder like deploy everything within the go button this folder.

So all the files we have write all the configuration files, everything will be deployed.

So if I hit enter, you can see that everything is deployed and we have one error as well.

So let's try to resolve that.

So in the my SQL deployment or HTML error validating unknown field volumes.

Okay.

So this is something that we need to fix.

So let's try to fix it.

So if we go back to the MySQL deployment and volumes, this is something that we need to solve.

So let's try to fix it.

The issue is only with the indentation.

So this volumes is where at which level specs level it should be at the containers level.

Okay.

So I will just.

Add tab here.

Now you can see that everything is aligned.

Now it should work.

So I'll just save this and I will go back and I will apply all the configuration back.

And now you can see that everything is successful.

Now you can see that most of the things are unchanged.

But you can see that the stateful set for my SQL that is created.

So now if we go here, so you can see that all the configurations are applied.

Now, if I.

Let me just clear this.

And if I do, it'll get all I should get all the information.

You can see that all the parts are there, all the services are there, all the deployments are there,

all the replica sets are there and all the stateful sets are there.

Right.

Every information you can see from here and you will get a status and each and everything.

Similarly, you can go to your Kubernetes cluster Kubernetes dashboard.

Sorry.

And here also you can see all the information.

You can see all the paths available and the status of each and every part.

You can see everything is running currently and you can also see replica sets available, right?

All the replica sets, you can see stateful sets available, Right.

My SQL, Eureka.

You can see the service is available.

These are all the different services available and the type and everything you can see here, you can

see the different config maps as well.

So these are all the config maps.

Okay.

And if I come here to the deployments, okay, you can check the logs as well here.

So if I want to check the logs, I can scale.

I did all those things and if I go inside this you will get the information about the deployment.

This deployment will be having a pod, right?

You can see the pod set is one updated, one total, one available, all those information.

And if you go to the pods.

Okay, suppose if I'm going to Cloud Gateway, if I go here, you will get the information about each

and everything.

You can see that how many times it was created, how many times it was started, because first time

some of the information may not be available.

So it has to restart.

But at the end you can see everything should be working completely fine.

If you want to check the logs, if you go to the pod section and from here you can check the logs as

well.

You can see all the logs and everything if you go to the services, right?

So you can see that we have the load balance service that is the node board so we can forward the service

by default.

Mini Cube doesn't provide us the functionality to run this, we need to do the port forwarding.

So we need to expose or we need to create a tunnel from the mini cube to see our services so that we

will do.

But ideally you can see that everything should be deployed correctly.

Okay.

If you feel that there is an issue, you can delete the service or you can delete the port and it will

recreate for you and it should solve the issue.

And if still you are not able to figure it out, you can always bring in the discussion forum.

Now, once this is done, everything is deployed.

You can see everything is green, okay?

And if you come here and if you open the new terminal and if you do Cube, it'll get services.

These are the different services available.

Now you want to expose your load balancer, Yureka service.

Okay, So for that, what you need to do is you need to run the command with the.

This is only for the mini cube.

Okay, That is mini cube service and the name of the service.

So what it will do is it will create a tunnel for you.

So with that tunnel you will be able to access this service within the Kubernetes cluster.

So once we run this.

You can see that it started a tunnel for us and we are able to access the Eureka server.

And you can see that each and every service is connected to it.

API Gateway Config Server Order Service payment service and product service.

Everything is up and running and we got one port information as well, like on this port.

The tunnel was open for us.

Now one more thing we can expose is let me open a new terminal and let's do Cube CTL, get services.

One more thing I can open is.

Cloud Gateway, because the Cloud Gateway is you can see it's load balancer, service and all the application

should connect to that.

So all the external traffic also should come via Cloud Gateway.

So for this also I will just do mini cube service and Cloud Gateway as we see.

So the moment you can see that I started the service, we got the port information and everything and

it stopped working.

You can see that it went to OCTA and we got the bad request because in the OCTA we need to define the

redirect URL.

Right?

So let's go ahead and define that, because once we define those redirect URLs, then only it will work,

right?

If you see here, we will get the other information right.

You can see this is the redirect URL and this is coming as wrong.

Our port information is what five nine, six, seven one.

So what you have to do is we have to go to the OCTA.

Okay, Here, only you can go go to the home page and log in with the account.

I log in with Google.

Okay.

So once you are logged in, you need to change the configuration.

So go to the application applications.

Go to your application Microservices.

Okay.

And if you are not able to sign in at all, then you have to go to the assignments.

And here you need to assign the groups.

Currently, you can see that I am seeing admin and customer.

If you're not able to say this, go here and assign the groups to your account.

Otherwise go to general and come down here and you need to change this information, these two things.

So if I just edit this and I need to change the port information so this is fine.

Nine double, six, seven.

So I will come here and I need to change to FY nine, 667159, 667.

So this port information you need to give that is sign out URL and the redirect URL, that's it.

Once you do this, save it and this, copy this and open incognito mode and we can log in here.

So the URL is slash authenticate, slash login to log in and to get the IDs.

So I'll just hit here and you can see that we got the sign in page and my user I can get from the directory

people.

And this is the user that I'm using so I can pass this user information and I can pass the password

and I can sign in.

And you can see that I got the authentication token as well, so you can see that now from my Kubernetes

cluster.

I'm able to connect to my octa core octa as well.

So I'm getting the token as well.

Now all of our services should work.

So now if I open Postman.

Let's check all of our services.

So your port information is fine and about six seven.

So let me just copy this and give here.

And this is going to be your post request.

So I'm just trying to execute or create one product here and in the body information.

I have given this particular product information that iPhone 1200 and the quantity is ten.

And in the authorization I need to give the access token.

So let me copy this access token here.

I'll copy this.

I'll go to the authorization and select OAuth 2.0.

With the request headers, I'll copy the token here.

Okay.

And now if I try to execute.

You can see that I got 201 created, so my order was created.

Now, if I try to get the order with Slash two, we are getting the two ID.

And if I do get here, I should be able to get the product as well.

You can see that I'm able to get the product as well.

So you can see that my product is working.

Similarly, your order and payment will also work.

And and you can see that you are only connected via API gateway and not anything else.

And you can see that everything is connected, right?

Let's try to expose the Zipkin as well so we will be able to see the Zipkin as well.

So we'll go here.

Let me see the Zipkin.

So I'll just see.

It'll get services and let me expose the Zipkin here.

So I'll just do mini cube service and I'll give the Zipkin load Balancer service here.

And you can see that your Zipkin is also working completely fine.

And if you run the query, you can see that you are getting all the requests that we did.

Whatever the failures were there.

What were the success?

Were there everything you can see here?

So you can see that everything is completely working fine.

Your Zipkin is connected, your address is connected, your security is connected and everything you

are.

My SQL database is also connected.

So you can see that this is how we were able to create each and everything in the Kubernetes cluster.

We created just files.

And with just one command, we were able to deploy each and everything.

Cool, right?

Isn't it awesome?

Now, if you want to delete each and everything, right, let me just clear it out.

If you want to delete each and every resources, we can use the command cube, CTL, delete hyphen,

F and the folder.

So if we just define Cait se and if you run this command, all the resources, whatever we have created,

everything will wiped out.

So you can see that everything is deleted.

Well then.

So everything is deleted.

Now, if you go here and if you go to the dashboard, you can see that no resource is available.

Only one default co-branded service is there no ports available.

You can see that they are terminating so it will be removed.

No stateful set.

Is there no replica set?

Is there nothing?

Everything is deleted Now if you want to create again, it's just one command and everything will be

created for you in one minute.

So you can see that.

How powerful is Kubernetes?







Transcript - 
Now, currently in our microservices application, you can see that we have a service registry, and

this service registry will connect all the services to it and we'll be able to communicate with each

other.

Services using that service registry.

And this service registry is something that we have created using the record Netflix dependency.

But when we are deploying each and everything in the Kubernetes cluster, Kubernetes also provides us

the service registry.

So if we want to communicate with other services or the other microservices deployed in the Kubernetes

cluster, we do not actually need a service registry.

That is the external service registry.

We can utilize the internal Kubernetes service registry that has been given by the Kubernetes itself.

Right.

So that means we can avoid entirely one application and we can leverage the service registry provided

by the Kubernetes.

So let's see how we can use the service registry of Kubernetes and what changes that we need to do in

our application to accommodate that.

Now, firstly, whatever the different services are created in the Kubernetes cluster, we can communicate

with different services using the service name itself.

So for each and every deployment or the stateful set we create, we create a service on top of that,

that's the cluster IP service.

So with the name of that cluster IP service, we can communicate between the different microservices.

So that is what something that we are going to leverage.

So rather than using the service industry, we can remove that service registry and wherever we have

added the dependency of the service registry as a client in all the different application we can remove,

all together we can implement our new implementation without the service registry.

So let's go ahead and do that.

So what we will do is we will go through each and every component and we will identify where this service

registry that is unique, a server or unique client has been used.

We can remove that and we can do the changes accordingly.

So let's start with the Cloud Gateway.






Transcript - 
So now you can see let's go to the Cloud Gateway.

And if you go to the boundary XML file of the Cloud Gateway, if you scroll down here, you can see

that we have the client dependency, right?

So I can just remove this dependency because I don't need it.

Right?

I'm not going to use the Eureka line now.

So let's remove the dependency and then let's go to the CRC Main resources.

Sorry, in the Java and in the main file, if we have used the Enable Eureka client, we can remove

that as well.

Okay, so let's remove this dependency as well.

Okay.

So you can see that that dependency also we removed from this project.

If we switch to IntelliJ idea, you will be able to see better.

So in the Cloud Gateway in the node XML file, you can see we had removed the dependency.

Okay, now let's go to the config server.

In the config server, let's go to the XML file and from the pond XML file we will remove this eureka

client dependency.

Okay.

Because we don't need it.

Let's remove this, let's rebuild the project and we will go to the main resources and application,

the HTML file.

And here we have the configuration for the Utica.

So this is also something that we don't need.

So we will remove this as well.

Okay.

And in the Java main configuration there is no enable Eureka client.

So this is all fine.

Now let's go to the order service.

So within the order service, let's go to the XML file and within the order service.

And within the order service we have also implemented the test cases, right?

And that test cases uses the service registry.

So if we see here in the test cases, we have created the test service instance list supplier, right?

This is something that we need, but we want to remove the Eureka.

So for this, if you go to the XML file and we go to the dependency analyzer and if we search for load

balancer, you can see that spring cloud load balancer is there and this is part of the Netflix Eureka.

But we want to remove the Netflix Eureka.

But we need load balancer because load balancer provides us the functionality for the test service instance

supplier that will allow to implement our test cases that will contain multiple service industries.

So for that reason we will remove this Netflix Eureka client, but we will keep the load balancer.

So here what I will do is I will go to the Netflix Eureka starter and I will change to load balancer.

Look.

Now, once that is done, once we have added the load balancer, we can go to the resources.

And if you go to the application, note the HTML file.

Is there anything related to the YUREKA server?

No, that's all fine.

If we go to the main resources, is there anything here related to the yureka?

It is already committed, so this is all fine.

If you go to the Java in the main this is only field line, so this is also fine.

So we removed the yureka client dependency from the order service as well.

Now let's go to the payment service and let's remove from that as well.

So we'll scroll down here and we'll go to the Yureka client and we will remove this yureka client dependency.

Okay.

Now let's check with the Srh's main resources.

Is there anything here?

Nope.

All good.

If you go to the Java, if you go to the payment service application, here is also all good.

Now let's go to the product service.

Let's go to the XML file and let's remove the dependency of the client.

So if you scroll down, this is something that I need to remove.

Let's remove this.

And we are good here.

So you can see that from all the project.

We just removed the dependency of the client.

Now, in the next video we will add another configuration related to the Kubernetes services.







Transcript - 
Now let's add the configuration for the Kubernetes services so that all the different microservices

can connect.

Till now, everything was connected using the service industry.

Now we need to connect each and everything.

Using the Kubernetes Services and Internally Service Registry of Kubernetes will take care of everything

about the load balancing and everything.

So all the services that we have used are mentioned in the Cloud Gateway.

So if you come here in the Cloud Gateway in the CRC, main Resources and the application YAML file,

if you come here, this is all we have defined that this is load balance, order service and this was

coming from the service registry, but now we need to use using the Kubernetes services.

So we need to change this configuration now rather than calling the load balanced order service.

We need to call the Kubernetes service and the community service that we have defined here.

If you come here in the Kubernetes, we have defined that other service is going to be order service

as we see.

So this is something that I have to use.

So if I just give this order service, as we see here and rather than load balance, I'll give HTTP.

So now you can see that we just change the So now you can see that we just changed the implementation

rather than using the service registry.

Now we are using the Kubernetes service, the same thing we have to use everywhere.

So all the service is done now we have to use for the payment service.

Now we need to give for the product service.

So let me give for the product service as well.

Now for all the services we defined that we should be connecting using the Kubernetes service.

So all the configuration is done here.

Now within the order service.

Also, we have defined the external clients, right?

Using the client like from the other service, we are going to call the payment service and the product

service using the client and the rest template there.

We have defined the service registry names, right?

So if we come here in the Order Service CRC mean Java External Client and Payment Service, if you come

here, here you can see that we have defined that we should be calling the payment service slash payment.

So now, rather than calling the payment service here, we should be calling using the payment service

as we see that is the Kubernetes service.

Okay, So let's change it.

So rather than giving the name here.

So what I will give is I had to give the URI you are now.

And what I want to do is this URL.

I'll be using it multiple places because I'll be using it for client.

Then I will be using with the rest template as well and I'll be using with the test cases as well.

So I'm going to use at the multiple places as I'm going to use the multiple places I can define in the

properties file.

And from there I can reference.

So let me go to the properties file and let's define those first.

So here, let me define those properties.

So here I'll define microservices colon and within this microservices, let's define the product service.

Okay.

And here I can give the URL and here I can define payment and year.

I can give the payment URL for the product.

Let me give the URL that is HTTP called and forward slash forward slash product service as we see that's

the name of the service slash product we are going to call the product.

Similarly, let's add for the payment and for the payment as well.

We have defined the same thing, HTTP payment service as see slash payment.

These are just the base URLs.

Now this properties I can use in our external service.

So let's go back here and I can use this way in here.

I can use micro services DOT and this is going to be your payment.

So that payment.

Okay, So we just define this, okay?

And this is something we need to add in the codes.

And for the name of this client, I will just change to payment.

Simple thing.

I will copy this client.

And for the product service.

Also, I need to do the similar thing.

So let me just add here and I will just replace with the product here Micro Services, dot product and

the name is Product and I will remove this.

Okay.

So you can see for Fin client, we have defined that we should be using the product service associate

and the payment service.

As you see, those two are the co-branded services.

Now similarly, we need to change in our rest template as well.

So if we go to the services and the orders of example here, we have used it.

So if you scroll down you can see that here we have used it.

That is the product service slash product and the payment service slash payment.

So this too is something that we need to change.

So let's define it.

So first we need to get the properties from the application to the HTML file.

So first let's define that.

So here I will be defining it.

So let me define private string product service, URL and private string.

Payment service URL.

Two properties are defined and these two properties we will get from the properties file.

So for that we'll be using the added value annotation.

And within this we can define dollar.

Micro services dot product.

And similarly, you're at the rate value within the course dollar.

Micro services dot payment.

These to you all we have defined now this to you all we can use your.

So let me just change it.

We are calling the product service.

Right?

So rather than calling this product service, I'll be passing just the product service URL plus or the

DOT product ID.

And here as well, what I will do is I will just pass the payment service URL plus.

And I'm going to change all this.

Slash, slash we have here, right.

And we are going to use append the order slash order ID so you can see that we just changed the information

here.

All done right now.

Similarly, we have to do the changes in our test cases as well.

So if we go to the test cases in the test folder, if we go to the resources.

Okay here.

Also, we need to add, so let's copy from here.

Okay.

So we'll copy this properties and we will add in the test cases as well.

And for the eureka we need to remove because we are not using Eureka anymore.

So let's remove that.

Our properties are defined for our test cases as well.

Now, if we go here in the Java, if you go to the test service instance here, we have defined the

payment service.

So yeah, this payment service is related to the service registry that we have added, but now we need

to convert to the Kubernetes service.

So we need to change this payment service as well and this product service as well to point to the product

service as we see and the payment service as we see.

So let's change it.

So this payment service will be changed to payment service as we see, and this will be also changed

to payment service as we see.

And this product service will be changed to product service, service and product service as we see

all things are done.

Now if you go to the order service config, I don't need this bean anymore, so I'll just remove this

and if I go to my service layer, so if there is any URL used here, we need to change it here as well.

So you can see there is the URLs that we have used.

These are the product service and the payment service.

These are part of the service registry.

So this is also something that we need to change.

So let's do that.

So what I will do, I will create properties here.

That is the private string product service.

URL and private string payment service.

Earl So these two you also mentioned, I can get the values with added value and addition, and with

dollar, I can define microservice dot product.

Similarly, here I can define a value dollar with curly braces, microservices dot payment OC.

All the two values are now added here.

So this too.

I can define here that I need to call the product service here and here I need to call the payment service

URL.

Plus I don't want this, I need to order.

So all the two URLs are mentioned.

Now one more thing we need to do here is it's really important because in the order service, like when

you are calling the order service in the get order details, okay, If you go to the implementation

here, we have two properties defined, right?

So these two properties also we need to assign when we are calling the test cases, right?

So for that reason, what we have to do is we have to go to our test cases and using the reflection

we need to assign those values within the order service.

Currently within this order service test, we got these two values, but in the actual object also we

need to assign it right then only will be able to call.

So here we need to do that.

So let me just define one method to assign those values.

So I'll just define one method, public wide setup.

And this method will be called before each method, and yet with the reflections will be assigning those

values.

So with the reflection test utils dot set fields in the order service, we need to assign some properties.

So what we need to assign, we have product service URL, right?

So this is something that I need to assign and with what I need to assign with the product service URL.

Right.

Again with reflection utils dot set field in the order service, I have payment service URL.

So this I need to assign this value with the payment service URL, whatever the value is.

So currently you can see that I'm just setting the values like whatever the values that I got from my

application or the amount of the test instance that I am attaching in the order service.

Because in the order so is also I have these two fields and I need to assign this to fields because

those values are coming from the properties files.

So this is all I have defined and I have changed the value.

So I need to I change the values here.

I need to change the values here as well.

So let me just change the product service, this product service URL here, also payment service, URL

plus.

So you can see that everywhere we have changed the values.

Now everything is pointing to the Kubernetes service is not the service industry.

Everything is now with the Kubernetes service registry.






Transcript - 
Now all the configuration related to our project is done.

Now we need to build everything and we need to publish to our Docker hub.

So let's do that.

So what I will do, I will open the terminal for the Cloud Gateway and I will do an VN clean install

jib build.

Okay.

It is running for this.

Let's do for the config server as well.

Same command.

Let's do for the other service as well.

Open and terminal.

Lets do this or the service let's do for the payment service as well.

Opening terminal.

Let's do for the product service.

And there is registry we don't need.

Okay, so let's check everything is done or not.

You can see that this is done.

Cloud Gateway is done.

Config server is also done or the service is running.

Let all these things completed, so we'll wait for that to complete.

So you can see that everything is complete, all the bills are successful and all the new images are

created and published to the Docker hub.

Now, in the next video we will do the configuration related to the Kubernetes files.





Transcript - 
So now in all our Kubernetes files, everything should point to the Kubernetes service registry rather

than the server registry itself.

So we have this service registry created, right?

So this service registry, I don't need because we are using the inbuilt service registry, right.

Kubernetes.

So what I will do, I'll just delete this file.

I don't need this.

Okay, so now service registry is gone.

Now we need to make sure that we do not have the Eureka Server address as the environment variable as

well, because in every file we have added that as an environment variable as well.

So if you go to the cloud gateway, you can see that we have the normal variable, right?

That is pointing to the Eureka server address.

So this is something also we don't need so we can remove this.

So now you can see that nothing is related to the Yureka server.

If you go to the config map, we can remove this as well.

Okay.

We don't need this config map.

That was the Eureka server.

Next, we'll go to the config server.

And also we don't need this announcement variable in the config server.

Okay.

We'll remove this.

We'll go to the my SQL.

This is all okay if you go to the order service.

In the order service, we don't need the Eureka server dependency, so we'll remove that environment

variable as well.

We'll go to the payment service and in the payment service also we don't need.

So we'll remove this as well.

Okay.

And if you go to the product service, same way, we will remove that.

And radius deployment is going to the same.

Zipkin deployment is going to be the same.

So now we have removed all the dependencies of the Eureka server and everything should be pointing to

my service registry.






Transcript - 

What we will do is we will deploy each and everything and we will see everything is working fine or

not.

So let's go to the terminal here.

Let's clear it out.

Everything dashboard is open.

That's fine.

We can stop the services and clear everything we can again, stop this service also and clear everything

similarly there as well.

Okay.

All the services are stop now if you go here.

Only dashboard is running and currently no resources are available.

So if you see with Cube it'll get all.

Nothing is running.

Okay, everything is cleared.

So now let's deploy everything.

So we'll just do Cube CTL deploy.

Sorry, apply hyphen f Kubernetes folder and you can see that everything is created.

Okay.

Now, if we go to the browser and if you go to the dashboard, you can see that everything is coming

up.

We will wait for everything to come up.

So now you can see that everything seems to be green, so everything is up.

If we go to the stateful set, we have only my SQL no Yureka server.

Similarly replica sets.

We don't have anything related to the Eureka server, only our services and the My SQL, Zipkin and

Redis.

So everything looks good and everything should be now connected to the service registry.

Okay, now to check it, what we will do is we will go to the terminal, we will go to the services

here.

We'll just check.

It'll get services.

Okay, here we are getting all the services now we will just expose our cloud gateway.

And from that cloud gateway, we should be able to access all the services.

If we are able to access the Cloud Gateway and from the Cloud Gateway, if we are able to call the product

service or order service or anything, that means everything is working completely fine.

So let's expose the service.

We will just use mini cube service and we will give the Cloud Gateway service.

So let's give this and it will open the tunnel and we'll get the URL as well.

And now once we get this, we again need to go ahead and chain the port information here.

So we will.

Go here.

We will go to the doctor in the application.

We will go to the applications and we'll go to our microservices application.

And here in the general settings, we will edit and we need to change the port information.

So the port information is five, six, 945.

So we'll just change it.

Five, 694556945.

Yeah, correct.

And we'll copy this and we will save the changes.

Once this is done, we'll open a new window and we will call the login API.

So it's authenticate slash login and we should be able to get the login page so we can see we are getting

the login page.

Let's log in with the user.

So let me go to the directory people.

And let me get the email ID So with this email ID, I should be able to log in.

And I should be getting my access codes.

And you can see that I got my token as well.

Now, with this token, we should be able to access.

So let me just copy this token here.

And if I go to my postman and if I check with the post and I'm passing the body here and I'm passing

the header information, sorry, not header authorization or to token request header and I should be

giving the token.

So let me just paste the token here.

And now if I hit the send, you can see that the port information is wrong.

So let me change the port here.

So it's five, six, nine, four, five.

Five, 6945.

Okay, so let's hit on sand.

And you can see that I got the request.

The data is created.

So if I go ahead with the request for and if I do get here, I should be able to get the data as well.

You can see that I'm getting the data.

So that means my services are internally connected.

Using the Service registry of the Covenant is because we have everywhere define this service, only

know where we have deploy the service registry and everything is working fine.

Cool.

Right.

So you can see that this is all about working with the Kubernetes, using the inbuilt service registry.

We are not using anything else, and this is how you will be able to deploy each and every component

or each and every resources that you create in the Kubernetes cluster.

It is really, really easy to work with the Kubernetes.

You just need to create files, and with a lot of plugins, you'll be able to easily develop all these

files.






Transcript - 

So in this case records.

What we are going to do is we are going to do a lot of things.

So let's get the overview of like what we are going to do in this course, and then we will move ahead

with the understanding about what is the pipeline and how we can build in our project, how we can use

all the different projects and we can build our pipelines.

So what we are going to learn in this course is we are going to understand what is the pipeline.

And with that we are also going to use the cloud platform.

So the cloud platform that we are going to use here is the GCP.

So this is the Google cloud platform that we are going to use to deploy all of our applications to the

Kubernetes cluster.

So within this GCP, we are going to use the Google Kubernetes engine to deploy each and everything

in the Kubernetes cluster.

And for this pipeline, we are going to use the.

Jenkins So this is widely used in the industry.

So once you are aware about it, you will be able to work on it and you will be also able to work on

the different flavors of the Jenkins and the different pipelines as well differentiated pipeline providers.

So there are many.

So you will be able to understand the basics of each and everything and you will be able to create the

pipeline as well.

So that's the agenda of this course.

And we are also going to create a complete pipeline for all the different projects.

So we are going to see all the different steps involved in the pipeline and how we can build a pipeline

that can be quickly deployable.

Once you do a commit to a repository.

And within the pipeline, we are also going to implement the test cases and we are also going to implement

this on our checks as well.

So the sooner checks will help us to understand the code coverage of our project.

Like how many lines of code has been covered from the test cases for our project that all conditions

and check that we are going to implement in our pipelines as well.

So once all the checks are passing, then only your pipeline will be proceeding to the next step.

Otherwise it will fail.

So all those checks and everything we are going to do and we are also going to implement the deployment

process, that means from this pipeline itself, once everything, every checks, build the test cases,

the sonar checks and everything is done, it will directly deploy each and every component to the Kubernetes

cluster of the Google Cloud platform.

So all the steps that we are going to do in this particular course.

So this is just a gist of it, like what we are going to do.

But now let's understand what is the pipeline and what we are going to build in this particular tutorial.






Transcript - 

Now what is seized pipelines now seized refers to as continuous integration and continuous deployment,

which means that whenever you're building a project like, what you do is you take the project right

from the any of the git providers like GitHub, GitLab or anything Bitbucket.

So you have your project and what you do, you do your changes on top of it and those changes you commit

to your get and then you push your changes to the repository.

So whatever the repository would be, all your changes will go there.

So what it will allow us to do is like whenever you do all those commits and push at that particular

time, you have the capability to start the build process, checks and everything to understand like

whatever the push that you have done to the repository, everything is valid or not.

Everything is as per the code quality or not.

All those steps we can do so that we can be assured that whatever the changes have been done are production

ready.

So once all those things are done like build phase, you can say like you, you can do the build, you

can do the testing and after that what you can do, you can do the checks as well, like all the basic

necessity checks, and then you can do the deployment.

So deployment to the end of the environment.

So all the steps you can see that will be automated based on the push that you do.

There are many checks that you can do, like you want to do based on the push you want to do based on

the merge request and many others.

But this is just the basic.

Like when you want to do any automated process of each and everything from the development to the deployment

at that time, we'll be using the QCD pipelines.

Now, this is not the textbook definition, but ideally this is what you will be doing.

And within this process, the development team and the DevOps team will be involved to make sure that

everything is working completely fine.

So this is what we are going to do.

But let's understand in detail like what we are going to build.

So what currently we have is we have one repository.

So this is our current scenario where we have the order service, product service payment service,

cloud, gateway config server, all these repositories we have combined in one repo.

Currently we have built up poly repo and all these repository are, all these projects are within that

repository.

And we have also one folder called K Test folder which contains the Kubernetes configuration files.

Right.

So currently this is our structure.

If you want, I can show you as well.

So if you go to the browser and if you go to the GitHub and if you go to our repositories and this is

our repository, and here you can see that this is our structure currently where we have one repository

and within that repository we have all the projects available.

So this is our current point.

What we will do from here is we are going to convert this poly repo into mono repo for each and every

projects.

We already saw about the differences between the poly repo and the mono repo in the starting of this

course.

But we are going to convert each and everything to the monarch repo and with that we will be creating

the different pipelines for all our different projects because what we want is we want the independent

services.

Like if I do the changes in only order service, I just want to deploy the order service.

If I am doing any changes in the Cloud Gateway, then I just want to deploy the Cloud Gateway.

So we are separating out each and everything.

Ideally, that's how we will be building in the production application where you're working as well.

So all these services will be converted into the mono repo, each repository for the each service we

have.

And so for the other service will be creating the repository for the product.

Service will be creating another for payment cloud, gateway config server and for all we will be creating

another OC and we will place the individual Kubernetes files to that respective project as well.

So we had created the Order deployment service right in the components file so that we will move to

the order service.

We have the product deployment service that will move to the product service and so on.

And we have some common configuration file as well, like setting up the config maps, setting up the

MySQL, setting up the ready, setting up the Zipkin, all those for all those, we will be creating

another repository that is the microservice initial setup.

This initial setup is not something that we will be deploying every time.

This is just for the initial setup, so we will separate it out and that can be used once or whenever

is needed.

But the rest of the things are containing more changes because those are your functional components

and that may require more changes.

So for that we will be creating all the different repositories.

So we will be creating how much order service, product service payment service, cloud, Gateway config

server and the microservice initial setup.

So these are all repositories that we are going to create separately.

So that's the one task that we are going to do.

Now other than this, what we are going to do is we are going to use the GCP as the cloud provider.

So within this GCP, what we are going to do is we are going to create our account first, we are going

to create our account and this is going to be the free account that you can create.

Google provides $300 of credit for three months so that we are going to leverage so you can create your

own account.

You have to provide your credentials and everything and you have to provide your credit card information

as well.

So we will be creating our account once we create the GCP account.

What we have to do is we have to create one virtual.

Such an instance and this virtual machine instance will hold the Jenkins for us.

So we are going to use the Jenkins.

So we are going to install Jenkins in this virtual machine itself.

We are not going to keep anything in local, so we are going to create a Jenkins setup also in the cloud.

So we are going to install Jenkins in the virtual machine itself.

Then we are going to create the Google Kubernetes engine cluster.

So we are going to create a Kubernetes cluster with the help of Google Kubernetes engine in the GCP

cloud so that we can deploy all of our components to that Kubernetes engine.

So that is also something that we are going to create.

And we are also going to create a Docker private registry.

Currently, we know that we have pushed all of our images to the Docker public repository, right?

Docker Public Registry.

But now we are going to push all of our images to our private registry because for your organizations

you will be doing the same thing, right?

So for that reason we are going to build our own repository, own registry for the Docker images and

that we are going to use to push those images and get those images from there.

So that is something that we are going to do and for that we have to do the changes in our projects

as well.

So that is also something that we are going to do.

So these are all the high level details that I'm explaining to you.

But when we go and create all this components and all this configuration, there are minor, minor details

as well involved.

So we will go through that as well.

And alongside once we do this, we are going to also install the sonar to do all of us on our tests

so that we are going to install in our virtual machine instance itself.

And once we are done with that, we are going to create the pipelines, we are going to create the pipelines

in Jenkins for each and every project that we have.

So these are all the agenda of this course.

This is all the things that we are going to do.

I hope you are excited.

I am super excited about this all set ups.

So let's get started.







Transcript - 

So now for the starting, what we are going to do is we are going to convert this poly repo into mono

repo.

So what I'm going to create is in my GitHub, I'm going to create all the different repositories and

I'm going to move the code to that place.

So let's start with it.

What I will do is I will create a new repository here and I'm going to name it as a cloud gateway.

I will make sure that I will name the same thing that is Cloud Gateway and I will keep the same description

and I'm going to create the private repository.

Why?

I'm going to create the private repository because I'm going to show you that how you can log in with

the private repositories as well as how to use it open as well.

Ideally for your organization it would be private deposit is only, but if you're working with the open

source, it can be public as well.

So that's why I'm showing you with the private repo.

If you're using the public repo, you don't have to provide the credentials.

That's easy.

So let's go with the private repository.

And what I will do is I will add the redmi file here and then I am going to create the repository.

So this way you have to create the repositories for each and everything.

So now let's go ahead and create for the config server.

So let's go.

I'll create for the config server.

I'll just name it as a config server.

I'll give the description the same.

I'll read the private repository and read me file and that's it created.

I'll create another one new repository and I will name it as order service.

I'll give the same description private repository, and read my file and create repository.

I'll create another one.

And this will be product service.

And I'm going to create the private repository at a file and create repository.

And let's create another new repository.

And this is going to be your payment repository.

Sorry.

This is going to be payment service, payment service private.

And I read my file and create a repository.

So you can see that I created a cloud config order payment product.

All these things we have created.

I don't want service history because we are going to use the Kubernetes service registry, but I also

need one more repository that is the microservice initial setup.

So let's create one more and I'm going to create GMs initial setup.

And this is also going to be the private repository, and I read my file and the create repository.

So look, all the repositories are created.

Now what I will do is I will clone these repositories in my local and I will merge the code.

So let's do that.

What I will do, I will just use the GitHub desktop here.

So that's the easiest way you can use any tool to do that.

You can use the command line as well.

So I will go your code open in GitHub desktop.

Open GitHub desktop.

And you can see that the path I'm getting here directly and I will clone the repository.

Okay.

And this is cloned.

I will do the same thing for the config server as well.

Open with GitHub desktop.

And clone.

Similarly I will do for the.

Or the service.

Clone.

And I will go to the product service and do the same thing.

And for the payments there is also same thing.

And for the micro services initial setup, I will do the same thing as well.

So all the repositories are clone in my local so I can move my code OC.

So you can see that everything is closed now.

So what I will do, I will go to my finder and if I go to the documents and GitHub here, you can see

that all the repositories are their product service payment, service order service, Microsoft's initial

setup config server and Cloud Gateway.

So here I will move all the code.

So I will go to the documents.

I will go to the GitHub and this is my Springboard microservices project.

So I'll keep it side by side so you can see better.

So I'll go to the Cloud gateway, I'll copy this and I will go to the Cloud Gateway and paste everything

here.

I don't need target photos folder, so I'll just delete it.

So Cloud Gateway is done then.

Config server.

Then order service.

Then payment service.

Product service I do.

Again, product service is also done now, so you can see that I've copied all the files.

Now I need to move the Kubernetes files as well.

So let's go to the Kate s, and what I need to do is I need to pass all these files.

So let's start with the Cloud Gateway.

So within the Cloud Gateway, I need to create one folder.

So I'll create one folder and I will name it as Kate as OC, and within this Kate as folder, I will

copy the Cloud Gateway file here and I will rename it to deployment YAML.

Simple thing.

I will do the same thing for the config server order service payment service and the product service.

So let's do it.

So, Cloud Gateway.

I'll go to the config server.

I'll create a folder.

Kate s.

And within this, Kate has folder.

I'll copy the config server deployment and I will rename it OC and I'll go to the order service new

folder.

Kate s and I will add the order service file and I will rename it to deployment yaml.

Similar thing.

I'll go for the payment service as well.

Yet.

Also, I'll create the folder K s and within this folder I will copy the payment service file and I'll

go to the product service as well.

I'll create a folder K.

S and I will copy the product service file.

So you can see that for all the services, I have copied all the files available.

Now what I will do is I have the microservice initial setup as well.

So I'll create the folder gates.

And here I will copy all the different files which are left out and which are the required for the initial

setup.

That is the config map.

That is what I required.

And then I need the my SQL connection.

That is my SQL deployment.

Then I need the radius deployment.

And then I need the Zipkin deployment.

All these files are required for the initial setup for our microservices, so we need to deploy the

Zipkin Redis, my SQL and the config map as well.

And once that is done, I can deploy each and every components when we need or whenever there is a change

in it.

So that's the plan.

So you can see that everything is ready.

Now, all the changes that we have done, we have changed our report to the Monarch report.

So let's commit all the things.

So let's go to the GitHub desktop.

And here you can see that I am getting all the files for the microservice initial setup.

And these are the files.

I don't need this file.

Ideally, I can add the git ignore file and I can add all the configuration, but let's skip it because

we need to do a lot of things as well.

So all these files I have added here and I will add the initial commit here.

And I will commit to me and I'll push the origin.

Similarly, I'll go to the payment service and do the same thing.

I have all the files available.

I will just remove the files, which I don't need.

That is the data store rest.

Everything is there, so I'll just commit it.

Initial with initial setup commit to mean.

Then I'll do for the product service.

Same thing, initial setup and commit to me.

Then I'll go to the cloud gateway.

I'll do the same thing here as well.

And I'll go to the config server.

Same thing here as well.

And I will commit my changes and push to the repository.

So everything is done if I go to my browser again.

And if I just refresh everything.

Okay.

You can see that all my code is available for the Cloud Gateway.

Everything is available.

Same way for the config server for the order service.

It's pending.

So let's do for the other service as well.

Product service is there, payment service is there.

So let's go to the GitHub desktop and let's go to the order service and I will do the same thing here

as well.

So now everything is set up, all my mono repo are ready.

Now we can do the changes according to what we need to deploy each and everything to the Google Kubernetes

engine so that we will do later.

So all your repositories are ready now.

Now what we will do is we will work with the Google Cloud platform.






Transcript - 

To work with the Google Cloud platform that is a GCP.

We need to have our account as well.

We need to create the account.

So we need to go to the Google Cloud platform, search for it.

And this is the account you can see you can create a free account as well and you will get a $300 of

credit for three months to use so that we can leverage to create our paid instances as well.

And you need to make sure that you keep an eye on that.

So once the $300 credit is used, your own personal money is not used.

And also make sure that whatever the resources that you create, once you are done with the resources,

once you are done with studying and creating all the things, make sure you delete each and everything.

So let's go to Google Cloud platform here, click on it and it will ask you to create the account.

So get started with the account and everything.

I have already created the account, so I'll just switch to that account.

So I will go to the console directly here and this is going to be my console.

So for you it will be asking a lot of details about the product, details about the billing, information

about the credit card information, everything.

You will be also asked to give your tax ID number as well.

So if you are in India, it will be asked for your pan card.

But if you are outside India it will be asking for your tax ID number.

You need to give that and you need to upload all the documents as well.

You need to upload your tax ID number, the card that you have, and you need to upload the image of

your credit card as well by removing all the digits.

You just need to keep the last four digits and you just need to give the name on your card, rest everything

you can hide it out.

So all those things that you need to upload during this process, it's fine.

That's the standard procedure that you need to follow to create the account.

And you need to also give your credit card information as well so that you will be able to create the

billing information as well.

So once the billing information is created, then only you will be able to access your free account

and you will be able to access the free 300 credit as well.

So once you do the request, I think within the 24 hours or 48 hours, everything will be set up.

You will get all the details and your account will be set up.

Then you will be able to create all the different resources in your cloud.

So that's one day or two day process that you have to follow to create the account.

If you already have the account, that's well and good.

But this is the new process and you have to follow this new process.

So now once all these things are done, your account is set up.

What we have to do is we have to use all the different services that GCP provides because we are also

going to use the Google Kubernetes engine and we are also going to use the Docker repository as well.

Those are the two things that by default are deactivated in your account.

Whichever the services that you need, you have to enable it and then you can be able to use it.

The basic services will be available for you, but these are the services that you have to enable and

then you can use it.

So from the search you will be able to see each and everything.

So if you search for the Kubernetes API, Kubernetes Engine API, if you come here by default, this

will be disabled.

Currently you can see that the API is enabled.

So if it's disabled for you, there will be a button to enable it so that you have to enable it.

So that's the one thing that you have to do.

And then if you search for the artifact registry.

So this also will be disabled for you.

So there's also you can enable it.

So that's the true thing that you have to do to services that we have to enable rest of the things we

can directly use it.

And you can see that for the 90 days you'll be able to claim the credit for me.

You can see that this is the credit available and the days remaining are 78.

So I've just created this account a few days back.

Only you can do the same thing.






Transcript - 

Now let's create the artifact repository, which we will use to push our Docker images.

So for that reason, we need to create one artifact repository that supports Docker.

We are not going to use the Docker.

So for that reason we are going to create this one.

So let's go to the artifact registry itself.

And here you can see that there is a button called create repository.

So we are going to create a repository here and here you can see that you will get with this all details.

So here we can add the different name here.

Okay, whatever the name you want.

So I'll just mention spring microservices.

You can give any name and you can see the format, whatever the format you want.

We want to work with the Docker, so that's why we are going to create the Docker registry.

So you can see that you can create the MAVEN repository, Python repository, anything that you want

you can create.

But let's go with the Docker and make sure that the location type is region and the region you can select.

What I'm going to use is I'm going to use these US West for Las Vegas region and also make sure that

what are the different resources that you are creating those all resources are within the same region.

So with that it will be a less configuration that we need so that all the different resources can connect

with each other If they are in the different regions, we have to do other configurations as well.

So within this example, we are going to create each and everything within this region itself.

That is the US West for.

So the description, I don't have to give anything and Google managed encryption key.

All these are fine and let's create it.

It's simple.

You can see that by just a couple of clicks and a couple of information.

We are able to create our Docker registry, and you can see that our Docker registry is created.

Currently, nothing is available, but we will add everything.

So one step is done.

Now let's create the virtual machine.







Transcript - 
Now to create a virtual machine.

What we will do is we will go to your that is a navigation menu.

And from here you can see you can go to the compute engine.

So let's go to the compute engine.

And from here you can see you can create the virtual machines.

So let's click here and you can see that currently you don't have any virtual machines available.

Your virtual machines is nothing but a machine with which you can deploy or you can install anything

you are, any application.

And from the cloud itself you can use anything.

So this is the virtual machine that we are going to create to install Jenkins.

Jenkins is something that we are going to use for our CD pipelines.

So let's create the virtual machine here and don't be overwhelmed by a lot of services.

You can only use the services from the cloud, which you need.

You do not have to run each and everything at one go learn step by step.

So within this tutorial we are going to only use the services that we need currently, right?

So the things we are going to skip.

So for any virtual machines that we want to create, we can create using the compute engine.

And from here within the VM instances, we can create the instances.

So if you click on the create instance here, there will be a lot of details available.

So here you can see that you can give the instance name.

So let's give Jenkins VM because particularly we are going to use for the Jenkins.

So from your name itself we will be able to understand you can add the labels as well if you want.

With the labels, you will be able to understand more about this virtual machine.

Suppose this particular washing machine is something that you are only creating for the day environment.

Then you can add the label for the environment for other reasons.

Also for the different teams.

If you are creating, you can add all these different labels as well.

And if you want to create any resources and you want to attach those resources for a particular label

that also we can do.

But those are some advanced things you can skip it for now, let's focus on creating the virtual machines.

You can see that this particular virtual machines will cost about $0.04 hourly, and that is $28.65

monthly.

Don't worry, all this amount will be used from the free credit.

That is the $300 that you get and it is hourly basis.

So whatever the number of hours that you use, that only will be deducted.

And you can see that it will give us to virtual use and four GB of memory and ten GB of balance, persistent

disk OC And you can see that I am still using the same region that is the US West for Las Vegas OC and

the zone is same.

You can see there are different zones available per region that also you can select OC three zones available

for the same region and there are many different regions available as well.

See, these are a lot of regions, so we have used one region and the zone and we gave the name of the

virtual machine.

Then you can see that machine configuration also that you can do.

Currently we are using E two series which is giving us two virtual ships and four GB of memory.

If you select here, you can see that a lot of different options available and you can also configure

manually as well.

So you can see that we have selected E two medium if we select E too small, this is the configuration

that we will get if we select something else.

If we select E two standard two, it will give us two CPUs and eight GB of memory.

Similarly, whatever the configuration that you need, you can select the type of that particular machine

type and that will be configured for you.

We will be going ahead with the same configuration that is, that is the E two medium with two virtual

CPUs and four GB of memory.

This would be enough for us as we are not doing any heavy work and it's free here, so let's keep it

simple.

So this is the configuration that we are going to use.

Other than that, you can see that you have the boot disk as well available.

That is a tangibly and the image that we are using is the Debian Linux.

If you want to change, you can go ahead, click here and you can change the operating system as well.

Whatever the operating system that you need, you can configure accordingly if you want to use the custom

images available that also you can do, or there are a lot of convictions that you can do here if you

want to change the disk size as well.

That also we can do if you want to change the boot disk type as well, that also we can do if you want

to use the SSD that you can use, but this will be very costly as well.

So all these configurations are available here and the access code, we will keep it as the default

scopes and I will just enable the allow HTTP graphics from here.

So these are the configuration that I need I will create from your.

It will take around some time to create your instance and you can see your instance is created.

That is the Jenkins film The Zone is this and this is your external IP.

With this IP, you will be able to access anything that you are installing.

Now, if you go inside this, if you click here, Jenkins VM, you will be able to see more details

about this.

You can see all the details you can see here.

Now, if you want to connect to this particular virtual machine, there are multiple ways as well.

So if you click here, you can see that you can connect to serial console and you can connect via access

as well.

So there are the different options available if you're using put files or any SSH tool.

So with that tool, you can get the keys and you can connect to it.

What we will do is we will connect using the browser itself because that's the easiest way.

So what we will do, we will connect using the SSH open in browser window.

So once you click it, a new window will be open and you will be inside this particular virtual machine.

So you can see that it is transferring the keys to the VM and we will be able to log in so you can see

that now we are under this Jenkins VM.






Transcript - 

Now, what we will do is within this Jenkins instance, we will install Jenkins.

Now, to install Jenkins, what we need is we need to install Java as well.

So we are going to install Java and we are also going to install Jenkins.

So let's do it.

I will share all the commands with you so you can easily follow all the steps.

I will be just copying all the commands which I have listed down so we do not end up in any issues,

but I will share all the commands with you in step by step order so you can follow along as well.

So the first command that we are going to use is the pseudo app to get install open JDK 11, and you

can see that we are able to install this.

Going to take some time to install.

We'll wait for it to complete.

Yeah, you can see that your java is installed.

If I do this java here, you can see that I'm able to run the command.

So everything is working fine.

Let me clear it.

Now, let's install Jenkins.

So you can see that this is the command.

I will give you this command as well.

But it is adding the repository and the signing key and we'll update it.

Let me just clear it now.

And now let's install Jenkins and the command is sudo install Jenkins.

And you can see that Jenkins is also installing now.

Now, alongside Jenkins, what we need is we need it as well because Jenkins will use the gate to fetch

the git repositories so that also we need.

So let's install git as well.

So to install the gate the command is sudo get install git.

So this will install git in our system.

I'll just pass by here.

And you can see that it is also installed.

If I just run the command gate, you can see that it is working.

Now, let's clear it out.

Now, other than Git, what we need is we need the cube still as well, because with Jenkins we are

going to run the cube command as well, because with that command it will run all those commands and

it will deploy each and everything in our Kubernetes cluster.

So for that reason, we need to also install here.

So let's install it as well.

It's very easy.

We had installed all this in our local system as well, right?

So we are going to install here.

We are not going to install Mini Cube here because last time we installed Mini Cube as well to create

a Kubernetes cluster in our local.

But here rather than Mini Cube, we are actually going to use the Google Kubernetes engine.

So to install the cube, the command is pseudo install cube CTL.

And you can see that it will be installed very quickly.

And you can see it is also installed.

If I run the command cube, CTL, you can see that that is also working completely fine.

So let's clear it out.

And yeah, whatever the software is, we need it.

Everything is installed.







Transcript - 
Now let's go and open Jenkins What we've installed right now, if you go here in the virtual machine

that is a Jenkins VM, we will have one external IP.

So if you scroll down here and you can see that this is the external IP, if I just copy this and I

have one here, that is 191 call and if I run 8080, this is the default port where Jenkins will work.

So if I just hit enter.

So you can see that it is just trying to get into the virtual machine that we have.

Right.

But firewall is blocking it.

So what we have to do is we have to open this port 88 so external traffics can come in.

Now, within this GCP, we also have to create a gateway that is the network gateway with which external

traffic, that is the internal traffic that can come in.

So that also we have to create.

So let's go ahead and create that as well.

So to create a gateway, what we'll do is we'll just search for the network gateway.

Okay, well, just for the gateway and you can see that there is a cloud net available.

So we'll go here and we are going to create a cloud network, Cloud Net Gateway.

So get started here.

And here you can see we can give any name.

So here we will give the name as Net Gateway.

And we'll select the router here.

We'll select the default.

And the region we are going to select is the US West for Las Vegas because for this we want the traffic

right?

And the cloud router that we are going to use is the route region west for.

If you are not getting any route, just click on create new router and give the same name.

That is the route.

If you click here, it will be open here and you can give any name and region will be there, all the

things will be there.

You just need to create it.

So let's I already have.

So I'll just use that directly, rest all the things I'll keep same and I will create.

So you can see that your gateway is ready.

You can see that it is running.

It will take some time to complete and now it is completed.

Now we need to actually allow the port as well, because we were not able to log in.

Right.

So what we will do is we will go to the firewall here.

I will search for the firewall.

And here you can see we have a firewall, Right.

So let's go to the firewall and you can see that we have the different options available.

You can see it will show all the IP ranges and the ports available like which all the ports are allowed

to get the Internet connection to your VPC by default, all the securities are available, so no one

will be able to access it until and unless you give the permission.

Without the permission, no one will be able to access all your resources.

So we are just giving the permission that from the internet.

If anyone is accessing the port 8080, we should allow those traffics.

That is something that we are doing.

So for that we have to create the rules, firewall rules.

We enable the internet using the network Cloud Gateway, but now we are allowing to access the traffic

to our firewall as well.

So here we will create a new firewall rule, and here I will just name it as Jenkins Port because we

are allowing Jenkins port.

And here you can see that the network is default.

You can create different network as well.

But we are going with the default network and the.

Direction to the traffic.

We are allowing the traffic internally, so for that it should be ingress.

If you are allowing outside, it should be egress.

So it's ingress traffic allowing.

Oil instance in the network.

We are not allowing specific tags yet.

You can see that there is a specific target tags.

This is used when you have already added the labels.

When you added labels with the tags, those labels will be matched and for that only the network will

be enabled.

Currently, we do not have anything and we have only one instance available.

So that's why I am selecting all instance in the network.

OC and within the IPV four ranges we have to allow which particular IP addresses will allow you to use

your instance.

If you are in the one environment, then you can add that CIDR range.

So within that IP address only you will be able to access your network.

But currently, currently, if you want to secure that, only your IP address should be allowed, then

you can get the IP address of your machine and that only you can use.

What I want is I want to allow each and everything currently later we will remove it because it's just

for the demo purposes for our tutorial to understand how we can do everything.

So for that reason we will allow everything.

So to allow everything, we just have to give 0.0.0.00 so it will allow each and everything.

And there is no other filters.

And yeah, we have to enable the port TCP and the port is 8080.

That's it.

We are enabling this port.

Now, once we create this.

It will take some time to create.

And you can see that the port is created.

You can see here you will get all the information here.

Jenkins Port is created as the ingress applied to all and the target IP address and the port is TCP

IP and the action is allowed.

Now, if I go here and if I run this.

You can see that now.

My Jenkins is starting.

I got the Jenkins page.

So now we have the access to our Jenkins.






Transcript - 
Now let's set up Jenkins.

So within the Jenkins, you can see that it is asking that you should get the password from here.

So what we will do is we will just copy this and from here we will get the password.

So let's copy this.

Let's go to the terminal that we open.

So this is the terminal.

We are open.

So here I will just run the command cat and I'll give you the path.

And here the permission is denied.

So I'll just use pseudo cat and this command.

So here you can see I got the password, so I'll just copy this.

I'll go here and I will add the administrator password and here I will click on continue.

And yeah, you can see that you will be treated with this page and yeah, you will select install suggested

plugins.

All the suggested plugins will be installed automatically.

So let's click on it and it will take some time to install all the plugins.

You can see that it is installing all this plugins available so we do not have to install it later.

So we will wait for this to complete.

And you can see that the step is completed.

Now we need to create a user, so let's create the user here.

So I will just name it as Jenkins.

As a user password I will mention.

And you can give any name and you can give your email address as well.

So let me just give you my email address.

So that's it.

Now let's save and continue.

Password didn't match, so let's add it again.

And this is your Port Jenkins.

So we'll be using this.

This is just a default port.

That's the address only.

So let's save and finish.

And your jenkins is ready.

Start using Jenkins and you'll be treated with this Jenkins homepage.

Now, let's read about Jenkins.

So what we have to do is we have to go to the dashboard here and from here go to Jenkins.

And and from here let's go to the global tool configuration.

So here we need to configure all the different tools available so we don't need JDK.

It will be default added.

So for the gate, we need to give the gate path.

So what I will do, I will name it as get here and for the path, what we will do is we will go to the

our terminal and we will use which git command to get the path.

So this is the path.

So I will copy this and I will add here.

So your gate is configured.

Now, Gradle, we don't need and we don't need.

We need Maven.

So what I'll do, I'll add a maven here and I will name it as Maven and I will just click install automatically.

This is the version.

Cool.

And I will apply the changes.

Save changes so you can see that your global tool configuration is done.

Now the next thing you need to do is to manage the credentials as well.

We are going to use the GitHub, right?

We are going to use the GitHub to clone our repositories in this Jenkins environment.

So what are the changes are there?

We will take all those changes and we will work on it.

And as we have created the private repositories, we need to authenticate as well.

So the authentication will only work based on the tokens that we provide.

So within the GitHub, we need to create a token for our account and that token we have to use in Jenkins

so that they can connect with each other and can take the latest code changes.

So that is the configuration that we have to do.

So what we will do is we will go to our GitHub account and here from the account will go to the settings

and from the settings we will go to the developer settings and from the developer settings we will go

to the personal account, personal access tokens.

And here we are going to generate a new token.

And within this token what you need is you need the admin, repo and repo permissions.

So let me just generate new token here and I'll just mention Jenkins token because this is something

that we are going to use for Jenkins and I will make sure the expiration here this is for the seven

days, okay?

After seven days this will expire and I need the report details and I need the admin repo hook, these

two permissions that I need.

Now, once this is done, you need to change the token and don't share this token with anyone.

So I'll be generating the token and I'll copy the token, but I will not be sharing it.

But that's the same token that we have to use.

So let's generate the token from here.

Now, within this manage.

Jenkins what you have to do is let's go to the manage.

Jenkins And let's go to the manage credentials.

Let's go to the global.

And here let's add the credentials and yet with the username and password.

The scope is global.

The username you have to give your username of digit.

So my username is Shabbir TWD 53.

This is the username that I'm giving and you need to copy the password and paste.

So let me do that.

So I give the password your ID you can give, make sure that whatever the name you give, that's the

same name we have to use in our pipelines that we create.

So I'm just giving it here and I'm creating the credential.

That's it.

So your credential is also added here.

Now, what we also have to do is from Jenkins, you are deploying each and everything to the Google

Kubernetes engine.

So that means Jenkins should also have access to the Google Kubernetes engine so that permission also

we need to give.

And for that permission, what we have to do is we have to create one service account.

So with that service account and with that public and private information, Jenkins will be able to

connect to our Google Cloud platform and the Google Kubernetes engine.

So for that, let's create the IAM account and let's configure that with the Jenkins itself so that

also we have to do so.

For that, what we will do is we will go to our GCP and we will search for the IAM here, we'll go to

the IM.

I am is nothing but the identity and access management and here we have to go to the service accounts

and yeah, within the service accounts you need to create one service account.

So let's create a service account here and I will give the name as Jenkins and.

Done.

So you can see that your service account is created.

Now we need to get the key as well.

So go to this account.

Okay.

And go to the keys here and here you can add the key.

So let's add the key here.

Create new key, and this will be the JSON key.

So you will get an entire key in the JSON format so that directly we can use in our Jenkins.

So create a key and you can see that the key is created and you can see that it is also downloaded.

So now your account is created.

So this account is also created and you got the key as well.

Now we need to go to the Jenkins and use it and to configure it so later we can use it.

So let's come here.

Now, within Jenkins, what we will do is we'll go to the dashboard, manage Jenkins and manage plugins.

So here within this available plugins, what we will do is we'll search for the Google.

So this is a plugin that we are going to install so that we can configure our user to use this plugin

and we can also configure that where we have to deploy in all those steps.

So this is something that plugin we will need.

So within this Jenkins, let's install this plugin, click on this and install without restart.

So this will be installed in the Jenkins.

So you can see that it is installed here.

Everything is installed here.

Now what we have to do is we have to go to the dashboard here and let's go to the manage Jenkins and

go to the manage credentials.

And here let's go to the global and add new credentials.

And here rather than username and password, we are going to use the secret file here because we are

going to upload a secret file and let's select this file here and I'm just selecting the file and here

I'll just give the name as GCP and I'll just define GCP credentials and you can see that your credentials

is also configured here.

Now, this is the same key that we can use in our plugin as well as a Google authentication.

So will again go to the ADD credentials here and we are going to select this a Google service account

from Private Key.

And here we are going to select the same JSON key here.

So let me just select the same thing and you need to give the project name.

So the project name, if you see here, this is going to be a project name.

Okay.

Project ID.

So let's copy this and let's give it to you.

Once you do it, let's create it.

Now, you added the key as well, and also you have added the Google authentication as well for the

key that you have.

So now your authentication for your Google Kubernetes engine is completely done.

We have created the service account and the same service account we have connected with our Jenkins

as well.

Cool, right.






Transcript - 
Now the big boy, we have to create the Kubernetes cluster itself.

So we will go to the GCP here and within the GCP, we will go to the your that is the Google cloud and

year we will go to the Kubernetes engine.

And yeah, you can see that we will see each and everything related to the GCC.

And yet you can see that it is asking for us to create the Kubernetes cluster.

So here, let's go ahead and create the cluster.

And you can see that you are getting a couple of options that is standard.

And the autopilot, we will go with the autopilot because that's the recommended way by the GCC as well.

And it will also add all the configurations related to the auto scaling and everything.

So we will do this.

So let's configure it.

And within the configuration you need to give the name and year I'm going to name as microservices Cluster

one and the region is something that we have to select.

So we are going to select as us West for that is the region that we are working with currently.

So we have selected it and it is going to be the private cluster and the rest of the things we will

keep it as default.

So that's it.

Let's create the cluster so you can see that our cluster is getting started.

It's in the autopilot mode, so we will wait for a minute to complete all the processes.

Now this will take some time to complete it.

So we will go to the next step and we will add all the environment variable configurations in Jenkins.

Meanwhile, this will get completed.

So let's go to Jenkins and then we have to add all the different environment variables.

So let's go to the Jenkins and here in the dashboard we'll go to the Manage Jenkins and we will go to

the configure system and here we will search for the environment variables.

So here you need to enable the environment variables and here you need to add all the different environment

variables.

So let's add all the different environment variables that we are going to need.

We are adding all the enrollment variables here so we can directly access all those enrollment variables.

We do not have to hard anything whenever we are creating the pipelines.

Once we add this, you will get a better understanding why we are doing it.

So let's click on ADD here and you can see enrollment variable is nothing, but you need to give the

name and the value for it.

So the first enrollment variable that we are going to add is the artifact registry.

So if you go here right in the Kubernetes, and if you go to the artifact registry, that's the name

that we have given, right?

So we will give that name so we can refer to.

So this was the name.

So let's copy this and let's give it.

So that's one of the name.

Let's add another variable then we will give the name of the cluster.

So within this name of the cluster, if you go to the Kubernetes engine, Kubernetes engine, and the

cluster name that we have given is microservices cluster one, so that we will copy and give the name

here.

Let's add one more than we need is the project ID.

So I'll just define project underscore ID, and the project ID is if you click here in the Google Cloud

console, this is the project ID, so let's copy it.

Whatever the project that you are working on that you have to select.

Okay, so I'm working in this, my project, my first project, so that's why I'm copying this.

Whatever you have selected that you need to select.

So let's copy it and let's add that as well.

And then I need is one more that is the registry URL.

Visit to underscore URL.

Now this URL is something that is a defect registry.

So if you go here and if you go to this spring micro services, this is the URL that we have to give

that is a US West for Docker.

So let's copy this.

And pasted here we don't need so we will just move, remove this.

So this also we have added and one more thing we need is the zone information zone is.

US West for.

So let's copy this because we are working with the US West for region.

So that's why we have added US West for.

So these are all the variables that we will need in our pipeline.

So that's why we have added all these variables.

Now let's save it.

So your enrollment variable configuration is also done for the Jenkins.






Transcript - 
Now as our Jenkins is set up and our Google commodities engine and all the repositories and registries

and everything is set up, now, we can create the pipeline for all of our projects that we have.

So what we will do is we'll start with one config server and then we'll move ahead to the different

services.

So for creating the pipeline, what we are going to do is we are going to store all the pipeline scripts

within our repository itself.

So whenever we want to change any stuffs in our pipeline, we can do the changes and we can commit those

changes and the everything will work together.

All the changes will reflect to the pipeline and everything should be deployed according to the new

changes.

So let's see how we can do it.

So firstly, what we have to do is we have to go to a repository and within that repository we have

to create the pipeline file itself.

So let's go ahead and do that.

So what I'm going to do is I am going to open the project, so let's work with the config server first.

So let me just open here.

So here I will open the folder, I'll go to the GitHub and I'm going to open the config server here.

So let me just open this.

Okay.

And this is my project.

Now, here's what we have to do, is we have to do a couple of things.

First of all, so let me start by creating the pipeline file itself.

So whenever we have to create the pipeline file, we have to create a file.

And so for that, let me just create a file with a new file.

And as we are using Jenkins so I'm going to create a Jenkins file, so I'll just name it as a Jenkins

failure.

So here this file is created, and here within the Jenkins file, we are going to write all the steps

and these stages of the pipeline that we are going to build.

And as Jenkins uses the groovy language, we are going to read the scripts in the groovy language.

So that is something that we are going to do.

Other than that, what we have to do is we have to do the XML file changes as well.

So if I go to the XML file and if I scroll down here, you can see that we have the MAVEN plugin tool

and within that we are using the registry hub, dot, dot, dot com slash daily code for this as a repository.

But rather than that, what we have to do is now we have to use the artifact registry that we have created

within the GCP.

That is something that we have to use.

We do not have to use the Docker.

So for that what we will do is we will create one URL, we will create one variable here, and the value

of that variable will pass from the pipeline itself.

So that is something that we will do.

So let's do that change.

Rather than pointing to the Docker hub, we will point to the artifact registry.

So everything will be in the Google cloud itself.

So rather than this registry hub, dot, dot, dot com slash daily code buffer, I am just going to

parse the dollar wrapper and this wrapper URL I'm going to pass from the pipeline itself.

So this is one of the change that we are doing.

Other than that, what we have to do is we have to write our Jenkins file.

So let's go ahead and write this.

So Jenkins file is something that we will write entirely in the groovy language.

Now, this entire Jenkins file, entire pipeline will be running in one of the node.

So for that, we will define a node here.

And within this node we have to define some stages.

So let's define those stages as well.

So I can define stage here and I can define the name and I can define this method.

So suppose I'm defining this as a checkout stage, then I can define one more stage that I can say this

is a build and push image.

Okay, then I can define this, a stage called deploy.

So these are the different stages that I am defining.

And within those all stages I can define our code as well, like what we should do in each of the stages.

So let's start with the checkout.

What checkout means we need to check out our code and after that, within the build and push, we have

to build our project and we have to push the image that we have created to the Artifact registry in

the GCP and within the deploy stage, we will deploy our images that we have created using the Kubernetes

file that we have in the Cadence folder to the Google Kubernetes engine.

Okay, That's the plan.

Now, let's start with the checkout.

So what I will do is I will copy one by one line over here and I will explain you what we are doing

here, because with that, we won't be making any mistakes and it will be easier to follow as well.

And all these steps are pretty straightforward.

You will everything from the Jenkins documentation itself, like how to define all the stages and all

this pipeline code as well.

So there is nothing fancy here.

This is just a different language that we are using that's groovy.

So that's why we might feel that it's something different, but it's really easy.

So within the checkout stage we have to check out the code from the repository.

So for that we are defining the checkout stage.

So let me just define everything here.

Let me just give you all the enters so we can see better.

So here within the stage we have defined that we are doing we want to do the checkout.

All these things, all these methods are part of the different plugins that are installed.

So you can see that within the dollar class we define that we are using the Git scheme.

You already know that within the Jenkins set up in the starting that we were doing, we would, we added

the gate right at that point.

This will be used here and then you can see that we have defined branches like which particular branches

that we want to use.

So we want to use the main branch.

Is there any extension that we want to use?

No.

And yet we have defined use remote configs.

Are there any configuration that we want to use?

Yes, we want to use the credentials ID as git, because with this credentials we will be able to log

into the github account and using this git id only we have to store the credentials.

If you go to the jenkins and if you go to manage Jenkins and manage credentials here, you can see that

we had added the git credentials and we have given the ideas git.

So this is something that we are using here that we passed git and you are, we pass the URL of our

config server.

So if you go to the config server here, this is the URL if you copy from your https, this is.

All.

We have to go, so let's copy and paste it over here.

So that is something that I had done.

So this we have added now your checkout citizen.

Now we need to work with the build and push images.

Now, within this build and push images, we need some variables as well because we had already stored

some of the environment variables that I want to refer and I want to also refer the main one as well.

So for that I will define some variables so that I do not have to do string concatenation so I can directly

use those variables when I want.

So for that reason I can define the variables and I will define the variables at the node level so I

can use that within this stage.

If I define within the stage, then I can use only within that stage method.

So here I will define at the node level.

So the first thing I'm defining is the wrapper URL.

So here you can see that we define the wrapper URL, and the wrapper URL we created is using the registry

URL, slash, project ID, slash artifact registry.

This is this is going to be your wrapper URL.

Now, all these three things, right?

That is the registry URL, project ID and artifact registry that we have defined in our environment

variables.

So if you go here to the Jenkins and if you go to the Manage Jenkins and configure system and if you

search for the environment variables, right, you can see that we have defined the artifact registry.

We define the project ID and we define the registry URL.

So with the combination of that, we had created the wrapper URL and that report URL.

We have to use to push our images.

So that variable we have defined so we can directly use the variable in the build and push stage.

Then we are defining the maven home.

So here you can see that we defined MAVEN home and where we have to take the Maven home we had, we

are using the tool.

So that's why we have defined tool and the name of the tool is Maven and the type is MAVEN.

So this is a standard syntax for using the Jenkins so that we have used to refer to the tool that we

have used.

So this will give me the Maven home path.

And with this Maven home path, I can run my MAVEN commands and with this Maven home I can define my

MAVEN command as well.

So here I define MAVEN and then command, which will refer to this variable that is the MAVEN home I

created slash been slash Maven and I have given this space over here.

So after that and then whatever the command I want, I can call all those commands that is Maven, clean

Maven installed in all those stuffs.

So this three variable we needed, so we have defined it, You can create any variables here and whatever

the different pipelines that you are creating and if you want any variables and you want to use it that

way, we can create here.

It's really simple.

So we define this three variables and we define the checkout stage as well that I want to check out

this particular URL with this credentials and using this git system and the branch we want to use is

mean.

It's pretty straightforward.

You can directly copy this and change your URL and it should work.

That's how easy it is.

Then let's go ahead and use the build and push mid stage.

We are going to use the GCC plugin that we are install and the different GCP commands here because we

are going to connect to the GCP and then we are going to run some commands.

So within this stage we have to use the commands and credentials as well.

So let's add those steps.

So here I'm defining that this entire stage should be within the credentials.

So this is something that I already find like with credentials file as credential ID, GCP and the variable

as GCC and this course key.

So what I'm doing here is I'm using this with credentials method of the Jenkins to get the GCP credentials

here.

You can see that GCP is something that we have defined, right?

So if I go here and if I go to the dashboard and if I go here to the manage Jenkins and manage credentials,

GCP is something that I have defined.

Right?

And this GCP is something coming from the file itself that we have downloaded that is a JSON file,

right?

So that's why we will define that it is coming from the file with the credentials ideas, GCP and that

credential I am storing in the GC underscore key variable.

This is something that I would define simple.

So what are the credentials?

Were there as part of GCP?

Key that I'm storing in in GC underscore key is a variable that GC and the key variable I can use here.

Now what we have to do is we had created one service account, right?

If you remember that's using that service account only we got the key right.

So that service account we need to activate and with that activated service account we can run some

commands here.

So that is something where we have to do every time because of the security reason this is done.

So we will activate that account here.

So as we are going to run the Shell script command, we are going to use the search function here.

So here you can see what we are doing is we are doing the search function and these are the Google cloud

commands.

So I'm just using G Cloud and I want to authenticate and activate my service accounts with the key file

as GC underscore key.

So whatever the secret file that we had.

Right, with that credentials, I am activating my service account here.

Simple stuff.

Then after that I am using this command that is the g cloud auth configured.

Tucker.

So whatever the doctor registry that we have created, right?

So for that dog registry, we have to authenticate as well.

Where we have to authenticate.

We have to authenticate ad us West for dot org.

Dave So this is the registry that we have created.

So this is, this is something if you go and check here, right, if you go to the Kubernetes, Google

Cloud here, and if you go to the artifact registry, and this is the artifact registry, if you go

inside this, you can see that this is the one US West for Docker dot org do.

So we are giving this we are authenticating here.

So that is the configure Docker command.

Now we have to actually run the MAVEN command and push everything.

So let's do that.

So you can see that I am running the MAVEN Clean Command.

So MAVEN Command, I'm using OC.

This is something that we have defined here that point to MAVEN home slash bin slash MAVEN space that

command we are using, then we are using clean install and after that we are using the Git build plugin

to build the images.

And after that we have defined the repo URL.

So this is the hyphen, the wrapper URL.

So this particular variable will be defined with this particular variable we are defining here, right

in the XML file.

This is something that we have defined.

So this value we are passing here.

So this value should be registry URL, slash, project ID, slash artifact registry.

Right.

We have just define our stage.

That is the build and push image where we build and push the image to the repository that we have defined

internal.

So this page will do everything for us regarding the building and pushing the images.

Then comes with the deployment stage.

So whatever the build image has been created, we have to deploy that.

So within this deployment step, if you go to the Kubernetes file, so if you come here in the case

folder and in the deployment YAML file, if you go here, you can see that we are directly using the

daily code buffer image, but by default this will point to the Docker hub.

But ideally we should be pointing to the Google Artifact registry.

So for that what I'm doing is I am just passing the image earlier rather than the daily code buffer.

What I'm doing is I am passing the image URL now from the deployment command.

I will convert this image URL to the actual repository URL.

So by default, if tomorrow you are changing the repository URL in another region or something, then

with this variable you can replace the code.

You do not have to worry about changing this file itself.

So that's why in the communities file itself, we have defined that.

I want to convert this.

I am passing this image URL and from outside I will change this image URL to the actual repository URL

and after that it will point to the config server.

So once that is done, if I go back to my Jenkins file in this deploy stage, I will do that first step.

So what I will do is I will use the set command here that said hyphen I and search for the image URL

and change it with the wrapper URL which we have.

So whatever the wrapper URL we have defined, change it with that.

So what it will do is it will search for the image URL in the code as slash deployment yaml file.

So in the case test deployment yaml file, it will search for the image URL and this it will change

to the actual registry URL.

So now our file will be correct.

Now after that we have to run this file.

So let's see how we can do now for deploying everything to the Google Kubernetes engine.

We are going to use the Google Kubernetes engine plugin that we have used, and that plugin gives us

this particular class that is the Google Kubernetes engine builder, Kubernetes engine builder.

With that class, I'm going to deploy.

So what I am passing is I'm passing the Project ID as environment, Project ID, cluster name as any

cluster location as environment zone.

You can see all these are the environment variables that we have defined right now.

You can connect like why we define the environment variables to refer to this particular locations.

So we define all this particular things.

And after that, in the manifest pattern, we provided the Kubernetes manifest file that is the CD slash

deployment YAML.

After that, in the credentials ID we pass, like any project ID with that credentials we are going

to use here, if you come here in the Jenkins, you can see that this is the one that we defined as

the ID.

So this ID we are passing and after that verify deployments equals to true.

So it will verify that whatever the Kubernetes resources that we created are up and running, then only

it will mark.

This particular pipeline is complete.

If you mark it as false, it won't verify.

It will just create the resources and that's it done.

So that's why we mark this as true.

So we will get to know that whatever the resources are created are up and running or not.

So that's it.

This is the complete Jenkins file configuration that we had to do nothing else.

Now what we will do is we'll commit all these changes.

So let me commit all these changes.

I will go to the.

Order service.

Sorry config server here.

So we did the changes in our format xml file, Kubernetes deployment file, Jenkins file.

That's it.

And we will define that Jenkins configuration.

Pipeline and I will commit all of our changes so you can see that your branch is created now.

Everything is now done.

Now we need to create a pipeline.

So let's go ahead and create a pipeline.







Transcript - 
Now to create a pipeline we have to go to.

JENKINS So let me just go to the dashboard here.

And from here you can see that I can create a new item.

So let's click on your new item and you can see that you can create an item from here.

So here I will just name it as a config server.

You can name whatever you want.

This, this is going to be the name of your pipeline and from here you have to select the pipeline.

And once you select this to click on okay and your pipeline is created.

And yeah, we have to add the configuration, you can add the description as well.

Now what we will do is we will add the GitHub project here.

So this is going to be GitHub project and here you have to define the URL.

So the URL is if I go your config server, so this is the one, I'll copy this and I will give here.

And if you want to change the display name, you can do it.

This is all fine.

Now after that you can see that your build triggers like how you want to trigger the builds.

How I want to trigger the build is like whenever there is some commits, whenever I'm pushing something

at that time, this build should start automatically.

Okay.

That is some, that is something that I want.

So for that what I have to do is this Jenkins and my GitHub board should connect and both should constantly

communicate with each other.

Like, okay, is there any commit or not?

Then accordingly I need to start my build.

So for that we have to create the web hooks.

So let's go ahead and create the web hooks.

So here I will just enable it.

Like I want GitHub hook triggers for Git as I'm pulling so that I've enabled now to create the web hooks.

What I have to do is I have to go to the repository that is the config server repository in that I have

to go to the settings.

And if you scroll down here in the web hooks, this is something that you have to enable.

So go to the web hooks and add web hook.

You need to give your payload URL and the secret information.

So what I will do is, first of all, I will go to the Jenkins here and now.

What are the changes we have done that we will see.

Okay.

So you can see that this pipeline is created.

If you go to the dashboard, this pipeline is created.

Okay.

But this will not work because we have to do the changes as well.

Now, within this workbook, you can see that we need the payload URL and the content type.

So let's define the payload URL.

So the payload URL would be this.

I'll just copy this and I will come here and I will paste it here.

So that is going to be your URL of your Jenkins and then you to define it hub, hyphen, web, hook,

slash.

So this is going to be your URL.

This is what you have to define.

After that, you have to define content type as application JSON.

And you can see that you have to provide one secret.

And this secret will be the credential details of your Jenkins because we are going to connect Jenkins

here, right?

So for that we will need the secret information.

So what I will do is I will come here in the Jenkins, I will select here, I'll click on Jenkins here

and I will come here and configure.

And if I scroll down here, you can see that I can generate the API tokens, this API token I have to

use.

So I will just add new token here and I will give the name as GitHub.

Generate the token, don't share it with anyone, so generate the token and the token is generated.

Copy this token and that token you have to come here in the ADD web hooks and you have to define this

secret here.

This is not copied.

Sorry.

So your secret is added here.

Make sure that you store this secret.

So what I will do is I will copy the secret and I will keep it at one place handy so we can add the

same secret in all the other repositories as well.

Once this is done activated so you can see that your hook is set up now.

Now, whenever you will commit anything, this hook will be triggered and the build will be started.

Now, if you come here, if you if you again go to the dashboard and if you go to this config server

and here you have to go to the configure because we need to complete our rest of the changes as well.

So let's go to the configure and here you can see that we have defined that GitHub project URL and we

have also enabled that we are going to trigger based on the GitHub hooks and after that you need to

check for the scripts as well, like which particular scripts that we want to run, pipeline scripts.

So Pipeline script, we will define that pipeline script from Sxm and after once we select this, we

have to select that we are going to use the get here.

And once we define this, we need to enter the GitHub URL as well.

So let's go here and let's go to the config server and this URL we have to provide that is the config

server.

Once we define this GitHub server URL and in the credentials we have to select our GitHub credentials,

that is should be WD 50 slash our credentials.

Once we select it, it will be able to connect.

You can see that it was able to connect and after that if you scroll down here, you have to provide

the branch which branch we have to select.

So if you come down here in the config server, you can see that this is our main branch.

So we have to define that, that this is going to be the main branch.

And after that, if you scroll down here, you can see that we have to give the script path as well.

And if you see that our script path, that is going to be my Jenkins file directly in the same directory.

So that's why we have defined that.

Or it will default come as Jenkins file.

If you created any different file, you can give the different file name here.

Currently we both have the same file, so that's why we have kept it same.

And click on Save Changes.

Okay.

And your pipeline is created now.

And now we can run this pipeline.

So before that, what we will do is we'll go to the GCP.

And if you go and come here to the.

Kubernetes engine.

You can see that your Kubernetes engine is created, your cluster is created.

And if you go to the workloads currently, there is nothing.

Currently, if you go to the service also, there is nothing.

Everything is clean.

Now, if I run this OC config server, you can run from here.

So build will get started and the checkout, the deploy build and all the steps will work.

So click on it.

You can see that build a schedule and build started here.

So let's go here and you can see that all the steps will come here.

You can see that all the steps checkout step is done currently build and push stages going on and if

you want to check the logs, you can go inside this and you can see console output.

So here you will be able to see all the logs.

What is happening currently, you can see that the process is going on.

It is downloading all the different dependencies that we need and the build is going on and it will

run all the test cases and everything currently.

And yet we will be able to track at what stage our pipeline is and it failed.

Let's see why it failed.

So we'll go to the console output.

If you scroll down, build failure because so you can see that till this stage everything happened correctly.

So let's check what happened here.

You can see that we are getting that main class configured as maven.

Jar plugin is not a valid java class.

Okay.

And fail to execute goal.

So the issue is we do not have the proper permission, right?

Because whatever the service account that we have created doesn't have the permission, so we have to

give the permission to that account.

So that's why we are in the I am right now.

And you can see that we should have the ED permission for that user that only we will be able to access

all those particular resources on the services.

So for that, what I will do is I will give the editor permission so that we will be able to access

it.

So we'll come to the IAM and within this view, by roles we have to add the user to add the role here.

So I'll grant the access here and you can see that this is the account that we are getting that is Jenkins

here.

We have to select this account.

This is the principal and for this principle I have to add the editor role.

So from this principal for this principal, I'll go here, I'll go to the basic and I will add the editor

role and I will save here.

So you can see that the editor role is given to our account that we have created.

Now what we will do is once this is done, we can run our script again, our build again.

So we'll go to the config server and now we can create a build.

So here you can see that build.

Now you can start a new build so you can see that the build will get started.

You can see the build is started and now it's running as well.

So let's see what's happening.

We'll wait for it to complete so you can see that build and push also completed.

And now we are in the deploy stage.

So now it is deploying everything.

And now once this is done, we can go to our GCP.

And if you go to the artifact registry here, the artifact should be created for us.

So if you come here, if you go to the spring microservices within this, you can see that the config

server is created and that's the Docker image.

And now if you go to the Kubernetes engine and let's see, you can see that deployment is still happening

because it is going to be validated also, right?

So if you come here, you can see that within this, if you go to the workloads, you can see that the

workload is created.

That is the spring server app, but it is not running.

We will see that.

And if you go to the service as well, you can see that the service is also created and if you select

the workloads, the issue is that insufficient CPU and insufficient memory, this is fine.

It will come up because we have also created the autopilot auto scaling.

So based on the CPU and memory required, it will start automatically after some time.

But as we are using the free tier, this is going to happen.

So you can see that we created one pipeline and with that pipeline we were able to deploy each and everything

to our GCP using the pipeline.






Transcript - 
Now let's do for the Maven initial setup as well that also we will set up in the Jenkins pipeline.

So what I will do is I will go to the Visual Studio code here.

And what I'm going to do is I am going to copy this file here and I'll go to the file, I'll open the

folder, and that is going to be the microservices initial setup.

Let me just open this folder and we have all the Kubernetes files available here and here I will create

a new file, and that is going to be Jenkins file.

And here I'm going to piece the entire code.

Super simple, right?

My code is done now.

Yet within the test folder, you can see that we have the config maps available and we have the MySQL

deployment available and we have the Redis deployment available that is available and we have the Zipkin

available.

You can see that everything is available Now.

What I will do is I will go to the Jenkins file and here's what I have to do is I have to change the

URL.

So let me just go here and if I go to the repository, that is the microservices initial setup, this

is something that I need.

So I just copy this and I'll come here and I will change the URL.

Like this is the URL that I need to get.

We just define our URL that this URL we need for our microservice initial setup and the branch is going

to be the mean.

This all commands.

I don't think so.

I need.

So let me because we are not going to build anything here.

Right.

So I will remove this step as well from here.

So what I will do, check out and deploy.

And within this deployment as well, I don't need this as well.

So I'll just remove this command as well because I do not have anything to change as a image URL like

how we did in the config server.

Because in the config server we were using the custom images, yet we do not have any custom images.

So I'll just remove that as well.

And within this Kubernetes engine builder, rather than deployment of the HTML file, I want each and

every file within this folder.

So I will define this way and I am going to remove all this as well.

And we are not going to use the repo URL as well.

So I am going to remove this as well.

So you can see that your Jenkins file is clean, whatever we needed that only we can trust everything

we have removed.

Now what I will do is I will commit this file and we'll create a new pipeline.

So let me just commit this.

So I'll go to the microservice initial setup, and this is the Jenkins file that we have changed and

I'm going to commit it.

So this is done.

Now what I will do is I will go to the Jenkins here and I'll go to the dashboard and I'll create a new

item here and new item.

And this new item, this is going to be microservice initial setup.

And this is also going to be the pipeline.

So I'll select Pipeline and click on.

Okay.

And this is your pipeline.

Here I am going to use the GitHub project.

So I've just defined GitHub project and I'm going to copy this URL.

So this is my GitHub project and here we have to use the GitHub hook here.

So we enable it and we are going to get the pipeline script from the ECM and that is going to be the

gate.

And.

We are going to give the rapper URL and the credentials is should be d w d 53 slash the d one credentials

in the configuration.

Once that is done, the branch is main and the file is Jenkins file.

You can see that all the configurations are done, but we have two.

Let me just create this.

So this is done.

Now we need to add the hook as well.

So if you come to the config server here, right.

So if you go to the settings, we had created one hook.

So if you go to the web hook, this is the one, right?

So let me just click on this.

I just copy this URL because the same URL we have to use here.

So in the initial setup, we will come to the settings, we'll go to the web hooks and we will add the

web hook and we are going to give the payload URL as web hook, the same URL.

This is going to be the application, JSON and the secret.

I will give.

So let me just copy paste it.

This is the secret and activate the web hook where book is also activated and the pipeline is also created.

So let's run this pipeline.

So I'm just running this pipeline now and you can see that the pipeline is running and the deploy is

in progress and it failed.

Great.

Let's see why it failed.

So if you go to the console output and if you scroll down, you can see that you can see that it failed

at the creation of the volumes.

It violates one or more policies and blah, blah, blah.

So you can see that it failed for the volume creation.

Why it failed in the volume creation is because we have given the host path and everything manually.

But GCP provides by default storage classes and we can use those storage classes to get the volumes

out of it.

We had briefly understood that storage class is the dynamically created volumes and we can use those

volumes based on the persistent volume claim that we have.

So if you go to the GCP here and in the Kubernetes engine, if you go to the storage here, you can

see that we created this MySQL PVC, which filled OC.

But if you go to the storage classes here.

You can see that these are the default storage classes that we have.

We can use all these storage classes to get the volumes.

So you can see that different types of storage classes are there.

So what we will do is we'll use the standard storage class here and with that storage class we can create

the persistent volume claims.

We do not have to create the persistent volume itself because we already have the storage classes available.

So for that reason, what we will do is we will come to our Visual Studio code and we will go to our

MySQL deployment and this is the position volume that we have created, right?

So we don't need it ideally.

So we'll just remove this because we are going to use the storage class name and the storage class name

is standard that we are going to use standard so that I can define that.

I want to use the standard storage class simple and redact accesses, read right ones I need and the

rest of the things is similar.

That's it.

Now what I will do is I will come here position volume claims, and as this claims was already created,

I can delete this.

I don't need.

I will recreate it.

Okay, so let me just delete it.

Now let's commit this file.

So we'll come here and we will commit this update.

Now, the moment we commit this, the auto trigger should happen and the build should start automatically

because we have added the books as well.

So I will commit this file, I will push this.

I will go to the.

Jenkins here.

Set up and you can see that the build started.

You can see that pending second bill.

So you can see it automatically started based on the hooks that we have added.

Och cool.

Right?

So whenever you will do the commit, everything will be started.

So you can see that the deployment also done successfully within one second and everything should be

available.

So for that, if you go to the GCP and if I refresh your OC, you can see that my SQL PVC is created

based on the storage class that is standard.

And if you go here you can see that the volume is created for us and if you see here you will get the

YAML file as well if you want.

And if you go to the workloads here, you can see that Zipkin is created, Redis is created.

My SQL.

That is an issue, but it will start config server is started and if you go to the services you can

see we have all the services also created.

So you can see that everything is now coming together with our different pipelines and everything is

there in the cover and this cluster of the GCP.

Now these issues we are getting because we are in the free tier and we do not have that much powerful

instances as well.

But if you have the configurations with you, everything should work smoothly.

And within this free tier we only have eight instances available.

So more than that we won't be deploying as well.

So what we will do is whatever is needed that will keep the rest of the things, we will remove it.

So you can see that your Microsoft is initial setup is also done, your config server is also done.

Similarly, we will do the changes for the rest of the service as well.

So let's go ahead and do that.





Transcript - 

So now let's do for the Cloud Gateway.

So I'll come here, file open folder, and I will open the cloud gateway here and within the Cloud Gateway,

what we need to do is we need to go to the XML file and we need to change this URL.

So let's do that.

So I'm just changing this wrapper earlier.

So the wrapper URL is change and after that.

And with that we need to change the Kubernetes file as well.

So within this deployment YAML file, we need to change this as well.

So rather than daily code before I will mention the image URL.

And now we need to create the Jenkins file as well.

So let me just create a new file and I'll define Jenkins file.

And here what we will do is we already understood the code, so I just copy paste the code.

So I'm just copy pasting the code here where we have the node.

And within that we have the wrapper URL MAVEN home and the MAVEN command, everything set up here and

we have checked out the main branch of the Cloud Gateway and after that we are using the build and push

images.

You can see that it's the same code that we used in the config server.

Everything is same, just the URL of the gate is different.

This is the cloud gateway URL.

Everything is going to be the same thing.

So I will just commit.

This changes here, I will go here, I will go to the Cloud Gateway and I will go in and I'm going to

commit the Jenkins file cabinet is yaml file and the Palmer xml file.

Right.

So the things are from the target directory so I don't need it.

And I'll define Jenkins config.

So Cloud Gateway is done.

I need to create the pipeline.

So I'll go back and I will go to the Jenkins.

I'll go to the dashboard and I will create a new item here.

And this new item is going to be the Cloud Gateway.

This is going to the pipeline.

And click on OC, and this is going to the GitHub project.

I have to add the Cloud Gateway URL.

Similarly, this is going to be the good hook trigger for guidance and polling and pipeline script we

will get from the scheme.

And that is going to be the gate.

And we need to parse the repository URL and we need to select the credentials.

And the branch is going to be the main.

And the file is Jenkins file.

So you can see that this pipeline is also created.

So you can see that how easy it is to create the pipelines in Jenkins for our CSD process.

Now, let's go ahead and do for the order service, product service and the payment service as well.





Transcript - 

So let's go to the visual strip code.

Let me open our folder.

That is the order service and we will do the same changes.

We'll go to the XML file.

And here we need to change the artifact registry name.

So this is something that we need.

So I'll just change here.

So this is done.

Now I need to go to the Kubernetes folder and deployment yaml file, and here I need to give the image

name image URL.

So this also done.

Now I need to have the Jenkins file.

So let me just create a new file.

Jenkins file.

Let me rename it as a G Capital and let me piece the code here and here and change the URL.

So let me just change the URL as well.

So this is going to be the order service.

So let me just copy this order service URL and I will change this URL order service.

And after that I think everything should be same.

Yeah, there is no changes here as well.

So this is also done.

So our order is also done.

So let me just push all the changes for the order service.

So I'll just select Jenkins file deployment YAML and XML file and Jenkins pipeline changes.

So I'll commit everything here and we need to create the pipeline as well.

So we'll go to the Jenkins and.

I will.

Create a new item that is going to be the order service is going to be the pipeline.

Okay.

And this is going to be the GitHub project.

I need to give the URL and then I need to give the GitHub hook trigger for Git ACM pulling script from

the ACM that is going to be the get and the repository URL.

I have to go for the other service.

I need to select the credentials and the branch name is going to be the main rest.

Everything is fine.

And now I need to go to the order service and I need to add the web hooks.

So I'll go to the settings and add the box as well.

Add the web hook and the URL is going to be the same one.

Okay, I'll add the verb hook earlier.

And this is going to be the application JSON.

And let me add the secret as well and add web hook so you can see that your order is also done.

Now let's go and do the product service as well.

So now here, let me just open the product service.

So let's open the folder for the product service and here as well, we'll do the configuration for the

pond XML file where we'll replace our repository URL.

So this image URL we are going to change pointing to the wrapper URL.

And if you go to the status file here, we need to point the image hyphen url image underscore URL.

These two changes.

Alongside that we have to create the Jenkins file.

So let me just create a new file, Jenkins file, and let's add the code for the this as well.

And let me change the URL so I'll go here.

And for the product service, let me just add the URL.

So this is going to be the product service.

Okay.

So rest all the things should be same.

No changes your.

So your configuration is done.

Let me commit all the changes.

So I will just switch to the product service and I will come with Jenkins file, Kubernetes deployment

file and the XML file.

And I will mention this is the Jenkins pipeline changes commit and now we have to create the pipeline.

So let's go and create the pipeline will go to the Jenkins dashboard new item.

And this is going to be the product service.

This is going to be the pipeline and create.

So you can see that it's just a repetitive steps only.

It's really easy.

So again, GitHub project, we will copy this URL.

This also get hub hook and the pipeline scripts from the FCM and that is going to be the gate and the

wrapper URL.

We have to go.

Credentials we have to select and Branch is going to be the main branch and Jenkins file.

So similarly we have to add the books as well.

So let me just copy the book URL, so I'll copy this URL and go to the product service, go to settings,

go to web hooks, Add web hook.

This is the URL.

It is going to be the JSON.

And let's add the secret and add web hook.

So product service is also created now for the payment service.

So now for the payment service, let's open the payment service project.

So payment service project by open, we will change the XML file to point to the repo.

So we'll just change this information here that we are going to use the repo URL and then we are going

to go to the Gators folder deployment YAML file.

And rather than this we are going to use the image underscore URL and we have to create the Jenkins

file here as well.

So let's go ahead and create the Jenkins file and add the code as well.

For this I'm just copy pasting the code and let me refer to the URL correctly.

So I'll go to the browser and I'll go to the payment service and this is the URL I need.

So let me just change the URL here.

Payment service and the rest of the things should be saved.

No changes in that.

Okay.

So you can see that we are just changing the URL test.

Everything is same for our pipelines.

Now let's commit this.

So I will go to the GitHub desktop and I'll go to the payment service and I'll select Jenkins file,

Kubernetes YAML file and the format XML file.

And this is going to be the Jenkins pipeline.

Changes.

That's it.

I'll come with this changes.

And now we have to create the pipeline.

So let's go to the Jenkins here and go to the dashboard.

Go to the new item.

And this is going to be the payment service.

And we'll click on.

Okay.

We have to select the pipeline and.

Okay.

And here I will go to the payment service.

I'll copy this URL and I will enable the GitHub project.

I'll add the GitHub URL.

Similarly, I'll enable the GitHub hooks and I'm going to select the script from the scheme.

So I need to select it.

I need to select the credentials and I need to pass the URL and the branch name should be main and the

file name should be Jenkins file.

Done.

Now I need to enable the web book as well, so let me just copy the book URL.

And I'll go to the payment service, go to the settings, go to web hooks.

Add web hook and parse the URL and content type should be application JSON.

And let's add the secret as well.

Add a book so you can see that for all the services we have created the Jenkins file for the repository

and we have created the pipelines and we have also added the web hooks.

So all the setup for our pipeline is ready.

We can run any of the pipeline and it should get deployed.






Transcript - 
Now let's deploy the other components with the belt and let's see.

Everything is working fine or not.

If you go to the GCP here and if a refresh, you can see that everything is running OC, my config server

is running, my MySQL is running Redis and Zipkin is running, all the components are running and we

have to now deploy the rest of the components.

But now we only have the limited number of instances available.

We can have only maximum up to eight.

So what I will do is currently I do not need Zipkin and Redis, so that is something that I will delete

so we can get some room to deploy other components.

So that's why I'm just removing all this two things.

So let me just go ahead and delete this.

So rest of the things that we can deploy, ideally, if you have the paid tier, you will be able to

do each and everything, but we are just understanding the gist of each and everything, like how everything

should work together.

And Redis app and Zipkin is something that is just for the logging purposes and the rate limiting purposes.

So that's why we can remove it for now.

It won't hamper our application.

It doesn't have the direct function impact, so I will just delete this to resources.

Now, what I will do is I will go to this Jenkins, and I'll go to the dashboard and I will run the

Cloud Gateway.

So Cloud Gateway should get deployed.

Okay.

So you can see that build is also started, so we'll wait for it to complete.

If you go here, you can see that as well, like what is happening.

So we currently build and push images happening.

And now we are in the deep state.

But it is trying to validate and you can see that that is validation is complete and it's deployed as

well if you go here and if refresh.

You can see that Cloud Gateway is also up and running.

Now, let's go ahead and do for the order service or the product service.

Let's go with the product service.

So you can see that you'll be able to see as well the status and everything here.

So let's go to the product service and deploy that as well.

So let's start the build.

And you can see that build is started now.

So if you go here inside it, we'll be able to see the statuses as well.

Now what you can do is you can explore Jenkins as well, like we have a lot of things available.

So all these different menus and everything is available so you can explore each and everything and

you will get to learn a lot of things as well.

So try to explore Jenkins as well So you can see the build is completed and the deployment stage is

running now and it's taking time because it is trying to validate as well.

And if we have limited resources, sometimes it may go down and it will come up as well.

So that's all fine.

So you can see that production is currently down, but it will come up eventually and the issue would

be similar.

We do not have the sufficient CPU in memory, so that is fine.

It will auto scale and we will get the instances ready in some time.

So we will wait for it until it gets up.

And if you want to see the services and everything should be running and you can see that we have the

load balancer services, right?

So for that load balancer service, you can see that we have the endpoint ready and that that is something

that we can actually use.

The rest of the things you can see we do not have the external APIs for that and now you can see that

everything is up and running and our pipeline should also be completed.

Our pipeline is also completed.

Now, I'm not deploying the order service in the payment service because eventually if we deploy that

should also work the similar way.

But as we have the limited resources, let's try to test all these components because ideally what we

are going to do is we are going to expose our Cloud Gateway service and we are going to do the configuration

for the OCT as well.

And if we are able to create a product and read the product that is as similar to as all the services

should be working.

Right?

So let's test one services and the rest of the services should also fall in place.

So if I go to the services, this is the one service that we have right expose, End Point.

So this is something that we can use.

So let's open this and you can see that we got the error because we do not have the octet configured

to use this URL.

So we have to configure it.

So let's go to the home page.

Let's log in with our credentials and sign in with my Google account here.

This is my account.

And if I go to the applications, applications and microservices application and if I scroll down here

in the general settings, we have this URL is available, right?

That is the sign in redirect URL and the sign out redirect URL.

This is something that we have to change.

So if I click on edit here, we have to give this IP address.

So what I will do is I will come here and I will give the IP address here and if I scroll down I will

give the IP address as well.

So our configuration is done, octet configuration, rest, all the things should be fall in place,

no other changes.

So let's see if the changes.

Let me copy this URL and let me open in incognito mode.

And after slash.

Ideally our login url is authenticate slash login.

So from this we'll be able to get our credentials so you can see that we are redirected back to this

login page.

Let me use my user ID so if I go to the Okta, if I go to the directory people, this is the user that

we are using, right?

So let me just copy this and paste here and let me give the password.

And we should be able to get our token as well.

So you can see that we got the token information as well.

Right.

So with this token, we will be storing the data and getting the data.

Go back here and let me open the postman with the postman.

We are going to do the request.

So the IP address is this.

So let me just copy this and give the IP address here as this is the directly HTTP port that is port

80.

We do not have to give the port as well, so we will just do slash product, we will do the post request

and within the authorization auth 2.0 request headers we have to add the token.

So let's add the token information.

So with the access token here, I will copy the token.

This is still here.

So this token I need to give here.

And with the token, we have the body information as well that we are passing my book Price and Quantity.

So we should be able to do this request.

Now we're getting the method not allowed because of the circuit breaker that we have in place, because

it's taking more than 10 seconds to process the request.

And we have the limit.

We have the default limit in the circuit breaker that if you're not getting a request within 10 seconds,

it will fail.

You can see that our services should be working completely fine.

We are able to hit that and from that only we are getting the method not allowed request from our services

only.

But though it's taking too much time, the circuit breaker is kicking in.

Our idea with this pipeline tutorial is that we are able to create the QCD pipeline and should be able

to deploy each and everything and all our applications should be running that we have achieved here

and our services are also working.

You can see we're getting 200 requests.

Okay, we got our token and everything.

Everything is running.

But the only thing is we have a lot of latency.

So that circuit breaker is kicking in at every 10 seconds.

That's the only thing available here.

And if you come here in the workloads and everything, you can see that everything is running actually.

So everything is deployed.

And if you want to check logs of each and everything, you can go to the particular deployment and you

can check the logs as well here.

So you will be able to see what is happening within your application.

If you want the YAML file of the deployment, if you click on YAML file, you will get your entire YAML

file.

You can see the difference events like what happened and revision history and everything, like multiple

versions.

We have deployed that also we can see.

So all those information you can see will be available here.

So rest of the things will be in homework to deploy all the other components as well, because currently

we have deployed one product service only the rest of the things that is the payment service and the

other service you can deploy each and everything.

Make sure if it's not working for you as within the free tier, you can remove the other application

that is the Zipkin and that is that we don't need.

Actually, that's not a functional part for us so that we can remove and test all the things is needed

at config server and the cloud gateway is actually we need and the one of the services that we can deploy

to check everything is working fine or not.

So at this particular point we can say that we were successfully be able to understand this GCD pipeline.

We were able to create a CD pipeline in Jenkins, everything in the Google Cloud platform and from there

we were able to use the Google Kubernetes engine to deploy each and everything to the cluster so that

all things that we did, we did all the configuration regarding the cloud and Jenkins and everything.

So this is a lot of things that we have done.

Make sure you go through it once again so you get all the understanding about the cloud changes that

we did and the Jenkins changes also we did.





Transcript - 

Now.

Currently, till now, what we did is we just created a pipeline which was involving the checkout process

and the push process of the images and the deployment process.

Currently, there is no validations or anything.

So now let's introduce the sonar checks so that it will check the code coverage for the actual application

that we have built.

And based on that code coverage, it will do the validations of our pipelines.

So what it will do is if the code coverage is greater than the defined percentage at that time, our

build will complete.

Otherwise, if it's less than the defined sonar checks, it will fail the deployment.

It failed.

It will fail the build with the appropriate reasons.

So that's the check that we are going to do.

And for that we are going to use the sonar.

Q Sonar Q will allow us to do all these checks in our pipelines and for our code.

And for that, we have to do some configurations in our project as well, in our virtual machines as

well, and in our pipeline as well.

So for all those, we are going to do all those changes.

So let's see that.

So first of all, what we have to do is we have to make sure that our test cases are running properly.

All the test cases should work.

And for all those test cases, we have to add the plugins as well.

That is the cuckoo plugin.

So with that plugin we will be able to generate the output file and that output file can be used by

sonar cube to get all the results.

So that is something that we need.

So for that, let's go to the order service and do those changes.

So I'll go to the order service here.

So let me just open the order service.

And within this order service, let's make sure that we are able to run the test cases first and everything

is working.

So if I go to the CRC test and what we have is the order service this test, So let's run this if this

is running or not.

Now you can see that our service is completely running fine.

So this is okay.

Now let's check for the controller layer as well, if everything is running.

So let's run this.

And you can see that there are some failures test in place order.

We are getting final, but actually we should get to 100.

So let's try to solve this.

So for that, let's go here.

And what we have to do is we have to go to the test service because that's the ideally reason.

And so if you go to the resources in the application YAML file, we won't be getting the services,

we will be getting each and everything in the local host.

Right.

So let's change this.

And you can see I am changing this in the test folder.

Okay.

Test resources, not the application, not the main resources.

So here I will change to local host column 88 is the port here.

Also I will change to local host.

Column 88 is the port because that's the same port we have defined here.

So let's try to run this if let's see if it's working or not, because until and unless it is working

all the test cases, we should.

We are not we won't we won't be able to work with this on our.

And you can see that everything is green, so everything is working fine.

So now we are okay with the test cases.

So now what we have to do is we have to change.

One thing I will show you.

It's really important that we do currently, if you see in our virtual machine we are working with Port

88 or Jenkins is working on Port 88.

So that port 88 is already used in our virtual machine in in our cloud.

And if you come here, all of our test cases are also working on Port 8080, which is not correct because

the port 88 is already used.

So we won't be able to run all this test cases in Port 88 integration test.

So for that we have to change the port.

So what I'll do, I'll change the port to 8081.

So now my all the things should be working on port 8081.

Other than that I also have to make sure that my service instance should also be now with the 8081.

Right.

So I'll just change to 8081 year as well.

And within the order service as well.

My wire should be running on 8081.

So now everything is running on board 80, 81 now.

So that's why it won't conflict with the port 8080 that is by the Jenkins.

So that's the one change we need.

Other than that, we have to integrate the Jenkins plugin as well.

So let's go ahead and implement the Jenkins plugin.

I will go to the permit XML file and within this plugin section we need to add the Jenkins plugin after

the Jib maven plugin.

So once this is done we need to add the plugins.

So here I'm just pasting the plugin section here and I will explain you.

It's very easy so you can see that in the plugin section.

I'm using the commune plugin here and this is from the on go.

I have defined the version and that version is coming from Django version, so I need to define the

version information in the properties as well that we will do.

Now we have to add the exclusion here as well.

Why we need to do the exclusion means, whatever the files are, your static files like for which you

are not writing your test cases.

Suppose your utility class is your constant files, your files for that.

Ideally, we do not write our test cases.

Those are just our helper files just to store the data and to pass along the data and to do our actual

business logic.

So those things we can exclude in the code coverage.

So that will not be included in the code coverage.

So we'll get a clear code coverage and soon will not bother about this files.

So that's why we have added the exclusion files.

Ideally, you can see that we have added the entities and the exceptions.

That is something that we do not need as a part of our code coverage.

We actually need our controller service layer repositories, which are actually important.

So for that reason, other than that, we have added as a exclusion here and in the execution we define

that initialize and prepare the agent and at the execution it will generate the report for us.

So that's the thing we have defined.

Now let's define the properties as well.

Now within the property section, I'm defining the few properties here.

You can see that we define the Jekyll version and the Sonar Java coverage plugin that we have defined

as Jayco.

So for the sonar coverage, we are going to use the Jekyll plugin and for the dynamic analysis, they

use the same report that we are going to generate as the technical execution file, and this is going

to be in the target directory that we have defined and the language we have defined as Java.

So we are our project is Java, So we are going to use the Java project that we have defined.

So that's all we have to define for the sonar coverage and the Google plugin for our project.

Now we have to commit all this changes.

Now let's go with the other configurations.






Transcript - 

Now, as we are going to use the sonar, we have to install the sonar cube as well in our virtual machines.

So let's install the sonar cube.

So for that, what we have to do is we have to go to the compute engine and we have to log into our

compute engine.

So this is our Jenkins VM.

Within this only, we are going to install the sonar cube and we will point that sonar cube to our jenkins

so they both can connect to each other and we can get the sonar reports.

So let's go to the Jenkins VM and let's connect open the connection via SSH in the browser.

So let it open here And now you can see that we are connected.

Now we have to install the sonar.

So to install the sonar, we have to first install the postgres as well.

And there are a lot of steps involved in it.

So for that I have created a detailed documentation with multiple references and I will share this document

in the course itself so you can refer.

So this is the entire documentation where we I have kept all the commands in the sequential format.

So and the description as well.

So you can just follow all the commands and you will be able to install Postgres and Sonar Cube as well

in your machine.

So let's start.

So first we need to add the PostgreSQL repository.

So this is the command, so we'll just copy this command and we will go to our terminal and we will

run this command and the repository is added.

Then we are going to install the W kit.

So with that W get we can get the actual Postgres.

So let's install the W gate as well.

Let's clear it out.

Now we need to add the PostgreSQL signing key, so let's copy this.

And run this command.

Key is added.

Now we need to install PostgreSQL, so we'll run this command.

And this is going to install Postgres.

So we'll wait for it to complete the installation.

And you can see that the installation is completed.

Now, the next step is to enable the database server to start automatically on reboot.

So in the system controller, we need to add the PostgreSQL.

So we will add it.

And that is also enabled.

Let me just clear all this and then we need to start the database server.

So we will start the database service here.

And then we need to change the SQL password.

So let's do that.

Okay.

And new password, I'm going to add.

You can use your own password.

Okay, so password is updated successfully.

Then you can switch the user to the post address.

So I'm going to do the same thing.

And the password?

I'm going to use the same password.

Now we need to create the user for this owner.

So let's do that.

We are creating a create user sonar.

And the user is created.

And then we have to log into the Postgres SQL.

Okay, So now you can see that we are in the PostgreSQL and then now you can see that we have to alter

the user.

And here you can see that you can give any password.

I've just defined my strong password so you can give any password.

So what I'm going to do is I'm going to just define.

Okay, so I'm just going to define this password.

Postgres admin.

You can define anything and I'm going to run this command that is the alter user sonar with encrypted

password as this.

So now the password is going to be the encrypted.

What are the earlier you have said, it's not going to be that it's going to be this new one and that

is going to be the encrypted one.

And then you need to create the database sonar for your sonar.

So let's do that.

Database is also created now.

And then we need to add the privileges.

So let's add the privileges.

And the privilege is also done.

And now we can create the Postgres.

So let's run this command.

So Postgres works is done.

Now let's exit from here as well, from this user.

So now we are with our route user.

Let me clear it out.

So that configuration is now done.

Now we need to install the zip utility so we can download the sonar zip and we can unzip it.

So for that reason we are just installing this sudo able to get install zip.

With one year as a year flight.

So that is now installed.

Let me clear it.

Now, you can see that I have to go to the official download page here.

So let me just go to the official sonar queue page and I can go here.

And what I'm going to use is I'm going to use the long term support version of this one, and I'm going

to copy the link address.

And that is something that I'm going to paste here.

And this entire link I can cut paste and I can give here.

So this is your command.

So let me just copy this and I can run this.

Okay.

So this zip is going to be downloaded here, so you can see that one section of zip is downloaded and

now I can unzip it.

So what I'll do, I will just copy this version and I'm going to add this version here.

Okay.

Same thing I'll do here as well.

So I'm going to unzip it.

So I'm going to run this command.

So soon I will be unzipped here.

And we will move this to the slash OPD slash sonar cube directory.

Okay.

Let me just clear this.

Now we need to add on a group end user.

So let me turn this command that is pseudo group add sonar.

Pseudo group add sonar.

Then we need to create a sonar user and set slash opt sonar cube as the home directory for it.

So that is something that we are doing here.

Sudo user add hyphen dd slash opd slash owner cube hyphen g.

Sonar sonar oc sonar is the user and then sonar is the group.

And then we are going to change the access ownership as well.

So you can see that everything is very easy.

You just have to follow all the commands.

Now we need to add some configurations here.

So let's change the properties file here.

So it will open in Nano editor.

And what we have to change is we have to find the username and password and there we have to give the

correct username and password.

So if we scroll down here, you can see that here is the sonar JDBC username and password.

So uncomment this line and give the username and password.

So the user is sonar and the password is something that you gave your.

Okay, this one.

So let's copy this and give the same thing here.

So this step we have done now, after this step below two lines, we have to add this URL as well.

So let's copy this URL, JDBC URL and let's add it.

So this year also we've added now, other than that, this is done.

So what we will do is we will save and close this file.

So we are going to do control X.

Save?

Yes.

So this file is saved now.

Now we have to edit this on our scripts as well.

So let's run this command and we will run this.

So here's what I have to do is I have to find this run as user and I need to give this one.

So what I will do is rather than finding I will add here in the first or second line itself.

Okay.

And I'm going to save an exit the file.

So I'll just hit control X.

Save?

Yes.

Okay.

Now we have to do the system D service setup as well.

So I will run this command that will start this on our queue where this system would as well.

Sudo nano slash slash system d slash system, sonar services.

And with this we have to add this code here OC and all this configuration.

So I'm just going to copy this and I'm going to paste it here and I will do control X to exit the file.

Yes.

Save it.

That's it.

Your configuration is done.

Now we have to enable this on our cube service to run at the system startup as well.

So we are going to change the system controller to enable it.

So pseudo system controller enabled sonar.

And then we need to start it.

So this is to start this sonar server.

And this is started.

Let's check it as well.

The status.

So this is going to check the status that is system controller status, sonar.

And you can see that it is active and everything is running completely fine.

Press Q to get out of it.

Now we need to modify the kernel system limits as well.

So this system configuration that we will add here.

So let me just clear it and let's add the configuration for that as well.

So I'll just scroll down completely and add all this following configuration OC.

And I will just save it with control.

X, See you.

Yes, that's it.

Now, once it's done, we have to reboot our system.

So let's reboot.

Okay.

So once you reboot this, the connection will be done.

So we'll just close this out and we'll go to our TCP and we'll go to the VM instances.

You can see that it is restarting.

We'll wait for it to come up.

So your Postgres and SQL is installed now.





Transcript - 


Let's start this one out and do the configuration Now.

Sonar Q by default, run on Port 9000 and by default we won't be able to run the 9000 port.

We need to give the permission as we gave for the add port for the Jenkins.

So for that, what we will do is we'll go to the firewall and we will enable that port.

So what we will do is you can see that we have enabled the Jenkins port.

Similarly, we will create a new rule that is create firewall rule and we'll name as sonar port.

And there's going to be the ingress port and the order instance in the network and the range should

be 0.0.0.00.

So everyone should be able to access it.

Okay.

And the port is going to be the TCP and the port is 9000.

Once it's done, we'll create the rule.

And you can see that the rule is created.

And now if I go to the virtual machine back, that is going to be my compute engine.

Jenkins, VM, and this is my IP address.

So let me just copy it and.

Let me close this.

And you can see that I've added a step as well that by default it will run on Port 9000 and the default

username and password would be admin.

So let's go to this URL and we will just define port 9000 OC.

And we should be able to open this one for us.

You can see that we are able to open sonar and by default the username and password is admin admin.

So we'll just define admin admin.

Let's change the password.

I'm just giving the new password here and I'll update it and you can see that now you are within the

sonar.

So you can see that your sonar is also set up now.

Now we need to do the configuration for our Jenkins and sonar to connect.

So let's do that.

So for that, what we have to do is we have to go to the Jenkins first and go to the dashboard.

Let me just refresh it.

I'll go to the dashboard and I'll go to the manage.

Jenkins and.

Manage plug ins because we are going to now install the sonar Q plugin will go to the available plugins

and will search for sonar.

And this is the plugin that we are going to use that is the sonar scanner.

So we will just click on it and install without restart.

And you can see that it is installed now.

Now Jenkins should know the password of the sonar cube as well.

Right?

So for that, we have to add the credentials for that.

So what we'll do is we'll go to the dashboard and we will go to the manage Jenkins and manage credentials

and we will go to the global and add new credentials.

And here we need to add the credentials.

So for that, getting the credentials, what we have to do is we have to create the token rather than

passing the password.

So what I will do is I will go to the sonar cube, I will click on the user my account.

And within the year I'll go to the security.

And here I will create the token.

So here I will just define the name that is the Jenkins token, and I will generate the token here and

this token I can use in our Jenkins.

So let's go to the Jenkins.

And here within the new credentials, we have to select this as a secret text.

And this secret text that we have to copy from here and we have to go to Jenkins and we have to give

this as a secret here.

And the ID is something that we have to give as owner.

So that's it.

Now we are going to have the credentials that can now connect to the sonar cube.

Now, once the credentials are done, we need to configure the plugins as well.

So we will go to the Manage Jenkins and.

Will go to the configure system, will search for the sonar.

And yes, we need to add this one, our cube.

So here, let's add the sonar cube.

And here you need to give the name.

So I'll just define this name as a sonar and the URL you have to give.

So you are out of the sonar cube is here.

So if you go to the sonar cube home, this is the URL that you are to give till 9000 port.

So let's give that and you need to give it to token.

So this is a sonar token that we have created so that we will give you once that is done, we'll click

on Save.

So you can see that now your sonar plug in your sonar token and everything is configured in the jenkins

now so that we can now use in our pipelines.

One more thing we need to do in the sonar configuration is that we have to create the Web book as well,

because your Jenkins is going to do the request, the sonar to do the analysis, and it will wait for

the sonar to give back the response.

So we need to have the Web book so it can get the data back.

So let's create the hook as well.

So we'll go to the projects and here's within the administration.

And then the sonar cube will go to the administrations.

And within this configuration, we have to add the web hooks.

So we'll go to the Web box here, and we have to add a Web book.

So we'll create one.

And here we'll give the name as Jenkins, because this is we are going to create for the Jenkins and

the URL.

So the URL for it should be will go to the Jenkins dashboard.

This is the URL for the Jenkins.

And this is something that we have to go slash.

We have to give the hook name as well.

So it should be sonar, cube, hyphen, web hook.

We're not going to pass a cigarette.

So let's create the web book now.

And you can see that the Web book is also created now all the configurations related to the sonar and

Jenkins and the connection between them is done.

Now, we can modify our pipeline to integrate the sonar changes.





Transcript -
we
Now  have implemented the test cases in our order service.

So let's leverage that.

And within the order service we will implement this sonar.

And similarly, you can implement for the other services as well.

So let's go ahead and do that.

So we'll go to our Visual Studio code.

And within this order service we have over Jenkins file.

Right?

So within this Jenkins file, you can see that we are doing the checkout stage and then we are doing

the build and push image and the deploy stage.

Now we will introduce one stage that will do the sonar checks for us.

So let's do that.

So after this checkout stage, what I want to do is I want to just build the stage, which will build

our project.

So what I will do is I will create one stage and that is going to be my just build stage where I will

just run the command and then click install.

That's it.

So this is going to be my build stage where I'm just doing and then click install command.

After this stage, I will do my sonar checks, so let's do that.

So now after this, I'm adding one stage that is the sonar cube analysis.

Within the sonar analysis, what I'm doing is I'm using the plugin that is the width sonar cube environment

sonar.

This is the application that we have.

This is the in the environment.

This is the credentials and all the configuration that we have added as the sonar name that I'm using.

And within that, I am running the command that is the MV sonar call and sonar.

This will start the sonar cube analysis for us, and this will create the sonar project for us and it

will get all the analysis done.

Once that is done, once this stage is done, I'm adding one more state that is the quality gate.

And with this quality stage and what I'm doing is wait for the quality gate.

So what we have added as the web.

All right, So with that, we will get the analysis status from the sonar.

So what happens?

Like did the quality get passed or failed?

So here we have defined that abort pipeline.

True.

So it will check for that.

And the pipeline will abort if the quality get fails.

And after that, we are just pushing the images.

So what I will do is rather than build and push, I will just define push images and within that we

will do all the same things with credentials and everything, but rather we will not do clean install

because that is something that we have already done in the build phase.

So we'll just do JIT build here and after that deployment is the same thing.

So this is just the configuration change for our sonar integration.

Now we will commit all the changes and start our build to check everything.

So let's go to the GitHub desktop, let's go to the order service.

So this is the Jenkins file.

We have, the XML file we have, then we have over other service test supplier application HTML.

After that, everything is in the target directory so that we don't need.

So these are the changes that we need.

So let's commit as soon as changes, commit to main and push to origin.

And now we will go to the Jenkins here and this is our order service.

So let's go to the order service and let's start the build.

So we'll start the build now and see.

You can see check out done build is going on.

So this build stage will run all the test cases as well.

So it will take some more time as well, because in the other project we have not added that much test

cases, but in the other service we have added a lot of test cases.

So you can see that build is completed now.

So an archive analysis is also started.

Now this will create the project as well.

So if you go to the sonar cube home here, we will get the project.

So you can see that sonar cube analysis is done.

Now we are in the quality check and you can see that the quality gate check is also done.

And now we are in the push image stage.

And you can see push is also done.

Now we are deploying everything.

So you can see all the stages that we have refined and everything is working as we have done, as we

have configured in our pipeline script.

You can add multiple series and you can add multiple checks as well and accordingly everything will

work.

It's really easy.

You will get all the commands and all the steps from the documentation is as well.

You do not have to memorize anything.

Everything is available, you just have to reuse it according to our requirement.

So deployment will also get completed and it will be waiting for the verification.

And if we come to the sonar cube and if we refresh the page, you can see that the project is created

for us.

And here you can see that order is created and it is passed.

If this was failed, then we would have got the failure here in the Jenkins and the push image would

have stopped.

Currently, you can see that we have very low coverage as well, but the current configuration is low.

We can configure like how many percentage that we need and everything.

So if you go to the order service here, you will get all the information about the sonar checks.

Currently, you can see that this many percentage that we have and this many lines that we have covered,

it will also show all the details.

Like for this particular class, we have four uncovered lines, 0% coverage and everything we have few

of the coverage is also available and it will show all the details as well.

Like how many lines were covered, what what lines were not covered and all those details.

So from here, we will get all the details about what things that we can improve on.

We can also get the details about the security hotspots, all the different issues.

Currently, we have one blocker issue and five major issue.

This is also we can go through and check.

You can see all this are available.

So this way we can improve our code as well.

So this is how we can implement these sonar checks as well in our pipelines.

And here you can see that the deployment is also completed and everything is running.

If you go to over TCP and if you go to our Kubernetes engine and if you go to the workloads.

You can see that the order service is also up and running.

Everything is running.

So you can see that the entire cluster and all the pipeline and everything will come together.

And this way your all the things are automated now you just need to do your code changes, your function

changes and you commit your changes and it will trigger the build pipeline and it will deploy each and

everything to your Kubernetes cluster with all the sonar checks and everything in place.

So you can see that we cover each and everything in the microservices, from the concepts to the development

to creating the Docker images to the Kubernetes cluster, to this pipeline to automate each and everything.

So go ahead and implement the same thing in your different projects as well.

Add the test cases in the different projects, and accordingly you can implement the different pipelines

to incorporate this changes.

You can also try to add the new test cases to increase the code coverage as well.

So currently we have less code coverage, right?

So that also you can improve and you can exclude the UN relevant file as well.

This is all about the sonar cube implementation with Jenkins for your entire Kubernetes cluster for

for each and everything.



Transcript -
So in this section of this course, what we are going to do is we are going to upgrade our entire microservices

application to Springboot three.

Also alongside that, it will also upgrade our entire application to spring Framework six.

And alongside that, we are also going to upgrade our application to Java 17 as well.

So currently our application runs on Spring Boot 2.7.

Okay.

Java 11 and alongside it, it is also using the spring Framework five.

Okay, so these are the current versions of the libraries and frameworks that we have used in our microservices.

So now you might have this situation as well, like how you can upgrade your applications to Spring

Boot three.

So we are going to do the same thing in this section of this course where we will upgrade our application

to Spring Boot three.

So we will upgrade all our libraries and all other dependencies.

So those are compatible with the Spring Boot three and Java 17 as well.

So first, let's go ahead.

What is the plan?

And accordingly, we will implement the changes.

So what we are going to do is, first of all, for each and every applications that we have, right

may be config server, cloud Gateway, it should be service registry or order service payment service

and the product services, right?

So all these applications that we have, we will upgrade all those applications to spring boot 3.1.

Okay.

So this is the current latest version available.

So we are going to upgrade to spring Boot 3.1 alongside it, we are also going to upgrade to Java 17.

Okay.

Now to upgrade to Java 17, what you have to do is you have to install Java 17 as well within your system

and alongside Spring Boot 3.1 upgrade, what it will bring is it will also bring spring framework six

upgrade as well.

Okay.

So you can see that all the three things that we are going to upgrade here.

So with this upgrade, our application would be with the latest and greatest technologies that is Spring

Boot 3.1 and we will be able to use all the features of Spring Boot 3.1 and Java 17 as well.

So this is the plan and this is what we are going to do in this section.

So let's start with the implementation of this change.







Transcript -
Now, as we discussed, we are going to upgrade each and every libraries and frameworks here.

So what we are going to do is if we go to the application, okay, this is our application, we have

all these applications available and if you go to GitHub as well, I will add the link as well.

So you can go through the application as well.

Okay.

So if you go to the GitHub here, okay.

And this is our project that is spring boot microservices.

Okay, here are all the applications are available.

Okay.

This is what we have been creating throughout this course as well.

So what we have done is we have taken the main branch over here and we have opened this in IntelliJ

idea and all this application is with Spring Boot 2.7 and we are going to upgrade it.

Okay.

So first of all, what we are going to do is we are going to start with the service registry.

Okay?

So if you go to the service registry and if you go to the Pom.xml file, okay, here, you would be

seeing that the spring boot version is 2.7.3.

Alongside the Java version is Java 11 and the cloud version is 2021 .0.3.

So these are all the things that we need to upgrade and alongside it, if there is any other versions

available that also needs to be upgraded, all the versions you can see all the dependencies added here.

Those dependencies will be automatically upgraded the moment we upgrade our spring boot version.

Okay, because Spring Boot is a parent of all those dependencies.

Okay.

If we have defined any particular version then we have to check that particular version is compatible

with that spring boot 3.1 or not.

And accordingly we need to upgrade.

Okay, so this is all that we have to do and this is what you would be doing in your applications as

well.

Now, alongside this, alongside each and everything we have been using in our system and we have been

also using Zipkin in our systems.

Okay.

So those two things also would be available.

The same thing you can use that is the Redis version and Zipkin version.

What I will do is I will start each and everything within the Docker itself, the same steps we have

followed earlier within the course as well.

Okay, so here I'm just starting my docker within my local machine and I'll start the two containers

that is the Redis container and the Zipkin container.

Okay, so this is the Zipkin container that I'm starting and this is the Redis container that I'm starting

so my application can connect to Zipkin and Redis both.

You can go to the terminal and you can run those commands which we have shared earlier to start your

application.

Okay, Those are the same thing.

We are not doing anything different here for Redis and Zipkin.

We will just upgrade our applications here.

Okay.

So with all the Jibber-jabber done, with all the basic stuffs done, let's start upgrading the application

now.

So now as you have seen that we have been using spring 2.7.3, what we will do is to get the latest

information, like what particular versions and everything we have to use to make it our life simple.

We will be using the spring initializer.

So what I'll do, I'll go to the chrome here and I will do Start.spring.io.

Okay, so this will help us to get the exact version that we want to use and if there is any dependencies

change, right?

If there is any artifacts change, then we can directly get it from here.

Okay.

So what I'll do is I will select the Maven project here because we have been using the Maven project

and here you can see that spring boot versions.

I will be using spring 3.1.1 version here.

You can use the any versions available over here.

Okay.

But we'll be using the 3.1.1 year because that is the stable version available.

And what we will do is we will have the packaging as Jar and Java version as 17.

Okay.

With all this details selected, what we will do is we'll go to our IntelliJ idea and with service registry

you can see that we have the Netflix Eureka server as a dependency.

Okay?

Once you add this dependency cloud starter and everything will come automatically for that.

Okay.

So I will add the.

Eureka Server here.

Okay.

You can see that there is the spring cloud discovery.

So once I add this and if I go to explore here, okay, the file would come here and you can see that

the versions and everything is different, right?

So what I'll do is I'll just copy this spring version here and I'll add the version here.

So I'll just replace 2.7.3 to spring.

3.1.1.

Okay.

Now also we have to change the Java version.

Okay.

So we will change the Java version as well.

So you can see the Java version is available here and spring cloud version is also changed, right?

So these two things we will change.

So you can see that we changed the Java version and we changed the spring cloud version as well alongside

our spring boot version.

Now you need to make sure that you have installed Java 17 to run this application.

If you have not installed Java 17, you can search for Java 17 and you can download Java as well from

there.

Okay.

It is very easy to install Java 17 the same way you would have installed the earlier versions of Java,

the same way you can install as well.

There is nothing else we have to do for that.

And if you are still facing any issues regarding installation of Java 17, do let me know in the comment

section.

I will help you out to install that as well.

So these two versions I have added already a spring boot version and the Java version and the cloud

version.

If we scroll down, there is no particular versions available.

All the things will be available directly from the Spring Boot 3.1 version.

Okay, so we are good with this.

Now we can just reload our Maven project here and all the latest libraries will be downloaded and we

would be good for this service registry.

So you can see that it is downloading all the dependencies.

So once it is downloaded we will start our service registry.

So here you can see that everything is downloaded.

So what I'll do is I'll go to the services, okay?

And I will start the service registry application.

And here you can see that the service registry is started.

If I go here, if I open it, you can see that my service registry is completely working fine.

So you can see one of the application is been upgraded to Java 17 and Springboot 3.1 version.

Okay.

Now let's go ahead with the other service.






Transcript -

Now let's go ahead and upgrade the product service available.

Okay.

So if we go here to the product service.

Okay.

If we go to the Pom.xml file here, you can see that this is also working on spring 2.7.3.

So this is also something that we need to upgrade.

So what we are going to do here is we are going to add the similar dependencies, okay?

What we have used here within our spring initializer.

So this will help us to get all the new dependencies and we can add all those dependencies which are

missing and we can replace as well.

Okay.

So what I will do is I don't need Eureka Server for my product service.

I need Eureka Client.

Right?

So what I will do is I will just remove this, I will go to Add and I will do add web because this is

a spring web application.

Okay, so I have just added spring web alongside what I need.

I need Eureka Client.

Okay.

Eureka Discovery Client.

So these two dependencies I have added alongside this, my application is working with as well.

So I'll add this spring data JPA dependency and alongside my application is working on my SQL.

Right?

So I'll add the MySQL driver as well.

Okay.

Because there might be possibility that my SQL driver will be upgraded based on the newer versions of

Spring Boot as well.

Okay.

So we should use those drivers as well.

Okay.

Alongside that, if we can scroll down.

Okay.

There is a config dependency as well and Zipkin includes dependencies as well.

There is a spring boot starter dependency and octa dependencies as well.

So let's add those dependencies also.

So what I'll do is I'll go and add the security dependency that is spring security alongside.

I will add the octa dependency.

Okay.

Then there was a Zipkin Dependency, right?

Zipkin Observability.

So I will add this as well.

And alongside there was a dependency for.

Config client.

Right.

So I will add the dependency for config client as well.

Okay.

So you can see that most of the dependencies that we have added.

So what I will do is I will explore this and I will add the related changes.

So first of all, what we have to do is we have to change the version, okay, of spring boot.

So I'll just copy this and I will scroll to the top and I will change the spring boot version to 3.1.1.

After that I will change the Java version and cloud version as well.

So I will change this to version that is Java 17 and the cloud version.

Okay, so these versions are also upgraded.

Now, if I scroll down here, okay, what you can do is you can also compare these two files so you

will get more understanding what has been changed and what needs to be upgraded.

Okay.

So what I was going through this is and I found out that the octa dependency has been changed.

Right?

23.0.4 So I can upgrade this as well.

So what I'll do, I'll just search for the octa.

I'll just scroll down.

Okay.

And I'll upgrade this version as well.

Okay.

And Zipkin and Sleuth.

So in the earlier version you would be adding Spring Cloud sleuth Zipkin and Spring Cloud starter Sleuth.

But from now on, if you scroll up here.

Okay.

Sorry.

Let me scroll down here.

Right.

So you can see that to add the Zipkin, we need to add the micrometer dependency because this was added

as a part of Zipkin and Zipkin.

Reporter brings these two things has been added.

So we need to take this two dependency that is the micrometer tracing brave and Zipkin.

Reporter Brave.

Okay, this two we need to add and we need to remove this two dependency that is Zipkin and sleuth,

and we need to add this.

Okay, So Zipkin is also been added.

Now then we have also upgraded Okta.

Then we need to go back here and we can check that MySQL is also changed.

You can see that.

Okay.

If I scroll up here, you can see that earlier it was used my sequel, Connector Java, and now my SQL

Connector has been used and the group is also been changed.

So I'll just copy this dependency and I'll replace the dependency here.

Cool, right?

So you can see that.

Most of the things are upgraded.

Let me just go through it once.

And yes, all the things are upgraded here.

So we just upgraded the application to Spring Boot 3.1 Java 17.

The cloud version is upgraded.

The Octa is also upgraded.

The MySQL connector is also upgraded.

And alongside that, Zipkin install dependencies are also changed.

Those are also upgraded here.

Okay.

Once all these are changed, what we have to do is we have to reload our Maven project.

Okay.

So I'll just reload the changes so it will download all the latest changes and all the libraries will

be downloaded.

So we'll wait for that to complete.

And now what we have to do is we have to start our application and there might be chances that it may

fail also.

But what we will do is we will go ahead and we will solve all those changes, all those issues as well.

Okay.

So let me just go through the services and we did the changes in product service.

So let me just go to the product service and start the application.

And here you can see that we have got the compilation error as well.

Okay.

So here you can see that it is not able to resolve the entity.

Okay.

So that's a good point that we can come to the next section where you can see that it is not able to

access the entity because from the Java 17 onwards, all the Java related packages has been moved from

Java to Jakarta.

Okay.

So this is the change that happened in Java 17.

So we need to do that change as well.

Okay.

So what you can do is ideally if you.

Do control space here.

Okay.

You can see that you are getting that entity is from the Jakarta persistence.

Now it is not from Java persistence.

Okay, so the packaging has been changed because from Java X it's been everything moved to Jakarta.

So what I will do is I will just change this import from Java X to Jakarta.

Okay.

So now you can see that all the.

Compilation error has been gone now.

Okay, so now I will try to run my application again.

Okay.

And now we have got one more error.

So let's see, what is the error And and the error is could not locate locate the property source.

The resource is optional.

Okay.

Okay.

This is the error because our config server is not running.

Okay.

We will do the change for config server as well.

But is there any other error?

There is a connection.

Refused.

Okay.

Okay.

So I can see there is one error available that is for the config server.

Right.

So what we will do is let me just start the config server and I will restart my product service.

Again, consider config server is still with spring 2.7.

Okay.

We have not upgraded it but it needs config server.

So I'm just starting the application.

Okay.

So let me just start the config server because config server is something that provides the configuration

for the product service to start it.

Okay.

So now config server is started.

So now I should be able to start my product service.

So let me just restart my product service again.

Okay.

And it is throwing again error.

So let's check the error.

So if we scroll up and the error is been creation exception and the issue is error creating bean with

name entity manager.

Okay.

So that means we are not able to create the entity manager factory.

That means we are not able to connect to our database or we are not able to connect from our GP2 database.

Right?

Is not able to connect here.

And if we scroll here and here we can see that we have used the dialect as MySQL five seven in Node

DB dialect.

Okay.

And this dialect is not available.

That's the issue, right?

Unable to resolve, name this one.

So now what we have to do is as we have upgraded our MySQL connector as well, right?

So that means this dialect might also been changed, right?

So what we have to do is we have to search for this error and we have to get the newer dialect right.

So what I will do is I will just change this dialect to a newer version of dialect, and we should be

good here.

So currently you can see that the dialect is 57 MySQL, five seven Innodb and currently we are whatever

the version we are using and whatever the MySQL server I'm using that is the MySQL eight.

Alongside.

What I have to do is I have to use the same dialect so I will upgrade to my SQL eight dialect.

Okay.

So what I will do is I will just come here and I will search MySQL eight dialect.

Okay?

And I can use this.

Okay.

So I'll just copy this dialect.

And what I will do is I'll go to product service.

I will go to resources and application Yaml file and here you can see that this is the same dialect

that I've used, which I'm getting the exception right.

That means I do not have that particular class available within my jar file, so I need to replace this.

So the latest version of dialect available is this one that is my SQL eight dialect.

So once I do this change and if I restart my application.

You can see that my application is started, right?

So my application is completely working.

If I go to Chrome and if I go to Eureka and if I refresh my page, you can see that product service

is available, right?

And config server is also available.

So now you can see that we upgraded our spring boot to two of the projects, that is the service registry

and the product service.

Now let's go to the next one.




Transcript -
So now let's upgrade our payment service as well.

So within the payment service we will upgrade the Spring Boot three and Java 17 as well.

So I'll go to the payment service and I'll open the Pom.xml file.

Okay.

So mostly this payment service is also the other micro services similar to product service.

So we are using this similar type of dependencies as well.

So whatever I have added here as a part of spring initializer, that should be good to upgrade my payment

service as well.

So from here I will just copy this spring boot version.

Okay.

And I'll upgrade this spring boot version and then I will upgrade the Java version as well.

So let me just copy Java version and the spring cloud version from here.

Okay.

And then what I have to do is I have to remove the Zipkin.

Okay.

So let me just remove Zipkin.

Then Webb is fine.

Cloud starter is fine.

Cloud starter config is fine.

Eureka Client is fine sleuth.

Because we know that Zipkin and Sleuth has been changed, right?

So I'll just remove it.

And what I will do is I will just scroll down here and I will select this micrometer dependency and

the Zipkin Reporter dependency.

And I will add here, okay then my SQL connector as well.

So I'll just scroll down here and my SQL connector dependency has been changed, so I'll just copy this

and I will change this as well.

Okay.

And then I know that.

In the previous application.

Also, we upgraded the.

Octa, right.

So this also we are going to change octa dependency.

Octa is 3.0.4.

Okay, so we have upgraded all the dependencies.

Let me just reload our Maven project so all the changes are reflected in our application.

So here you can see that everything is reloaded now.

Now what I have to do is I have to go to the payment service.

Okay, I'll go to the product service first.

I'll go to the.

SIRC main resources and application Yaml and I'll copy this dialect because the same dialect I have

to use in my payment service.

So I'll go to the payment service, SRC main resources and application yaml file and I will upgrade

the.

Dialect.

Okay, Now I need to start my application.

So I'll go to the services.

I'll go to the payment service and let me start the application.

Okay.

And now we got the same error here because Java is being moved to Jakarta, so I'll just change to Jakarta.

Okay.

And I will start my application again.

So I'll start the payment service again.

And we got one more error.

Okay, So let us see the error as well.

What it is saying is the end matcher method symbol is not available.

So that means end matcher method is being removed from the latest version of spring security six.

Okay.

So what it is saying is variable authorized request of type expression URL.

Expression registry.

Okay, So let me just copy this.

Exception.

And let me search what it says.

Okay.

Can cannot resolve method and matches.

Yes, this is what I need.

And if you scroll down, what is the solution?

It has been provided.

Okay.

Yeah.

This is the solution and it has been approved as well.

And here we can see that the end matcher method has been deprecated.

Right?

So if you are using Spring Boot three project, we have to use the request matches.

Okay.

So what we will do is we will change the end matches to request matches.

Okay, that's it.

You can see that the problem has been solved now.

Okay?

So you can see that it's very easy to find the solutions.

Only some of the methods has been changed.

Okay, so now the spring security has been upgraded now.

So we change the method to make it uniform.

This was changed, right?

So let's go to the services.

We'll go to the payment service and we will start the.

Payment service.

And the payment services also started.

So you can see that it is completely working.

If we go to the browser, if you go to the Eureka service and now you can see that payment service is

also available.

Okay, Now let's go ahead and upgrade the order service.



Transcript -
So now let's upgrade the order service.

So we'll go to the order service and we'll go to the XML file.

And this is also similar services to payment service and product service.

So it is also using the similar types of dependencies.

So we can use the same dependencies from here within the spring initializer and we can upgrade everything.

So we'll upgrade this spring boot version first.

Okay.

So you can see that how simple it is, right?

So we just upgraded the spring boot version.

Then we have to upgrade the Java 17 version and spring cloud version from here.

Okay.

And then if you scroll down here, we have to change the MySQL.

So MySQL connector is com dot MySQL and it is MySQL.

Connector.

J Okay.

And then if you scroll down spring boot test, cloud config is there.

Open Fein is the same thing.

It will be part it will be derived from the spring boot 3.1 version.

Then we have the Zipkin and Sleuth, right?

So this is what we need to remove Zipkin and sleuth and we need to add micrometer and Zipkin.

Reporter Dependencies.

Okay.

And then circuit breaker resiliency is the same.

Auth to client is the same and we just need to upgrade the version of this.

This is 3.0.4.

Okay.

And one more question is also same.

We can keep it same.

Okay.

So here you can see that the dependencies are upgraded.

I can reload the Maven project.

Okay.

And what I will do is I will go to the payment service.

Let me just copy the dialect from here rather than going.

Okay.

So I'll just copy this dialect information and I need to go to the order service source main.

Resources and application yaml file and I'll upgrade the.

Dialect.

Okay, let me start my order service as well.

And we have the issue.

So what we will do is we will change from Java to Jakarta.

Okay.

We'll go to the order service and restart our order service application.

And here you can see that other service is also started.

If we go to the browser and go to Eureka Server and if you refresh, you can see that other service

is also coming up.

Okay.

So that is also completely working fine.





Transcript -
Now let's upgrade our config server as well.

So we will go to the config server.

We'll go to the POM XML file and let's upgrade the.

Version of this as well.

So we will go to the spring Initializr.

We will upgrade the spring version.

Okay, then we have to upgrade the Java 17 and.

Spring cloud version as well.

So let me just go through that and upgrade that as well.

And then if you scroll down.

There is no other versions available and there is no other dependencies available which has a specific

version needs to be added, right?

So all this version will come from the spring boot starter parent as well.

So we will just reload the Maven project and once the libraries are downloaded, what we have to do

is we have to restart the config server.

So let me just restart the config server as well.

So your config server is restarted.

Right?

So if I go back and go to the Eureka server and if I refresh I should be getting the config server as

well.

Okay.

So you can see that all the services are being upgraded to Springboot 3.1 and Java 17 as well.

There is one service remaining that is the cloud gateway.

So let's implement the upgrade in that as well.





Transcript -

So now let's go to the cloud gateway.

Okay.

And within this cloud gateway as well, we will upgrade everything.

So we'll go to the Pom.xml file and we have to upgrade the versions.

So let's go to the spring initializer and the version that we need is spring 3.1.1.

Okay.

And then I need to upgrade the Java version as well.

So let me just go here and let me upgrade the Java versions.

Okay.

And then if you scroll down here, I need to remove the sleuth dependency Web flux is part of reactive

springboard and it will come within the spring 3.1 as well.

So we do not have to change anything here if we scroll down.

Config is the same part.

Gateway is the same cloud part.

Right?

Eureka client.

Also same.

And.

Sleuth, right?

So this is something we need to remove.

So let me just remove that.

And what I will do, I will go to the.

Browser.

And let me just copy the micrometer dependency and Zipkin.

Reporter Brave dependency.

So this is to we need for the Zipkin sleuth.

Okay, then rest of the dependencies are all okay.

Octa we need to upgrade to 3.0.4.

Right.

That's the same thing.

Let me check here.

If you scroll up, it's 3.0.4, right?

And if you scroll up again, let me just check all the versions.

3.1 Java 17 2020 203.

And.

Rest of the things are.

Fine.

Okay, so let me just reload the Maven project as well for my cloud gateway.

So here the maven load changes are done.

Let's go to the services and let's start the cloud gateway as well.

And we got some error.

Okay, so the issue is it's not line number.

Okay.

Enabled Eureka client.

Okay.

So enable Eureka client is not available.

It's not able to resolve the dependencies for that.

Okay.

So there's the two things.

Ideally within your Springwood application when you are working with the default changes, right, with

the auto configuration, you can just remove this dependency as well that it is not needed or this enabled

Eureka client is being changed with enable discovery client.

Okay.

Either you can add this or if you don't add also.

That's fine because we are using the default configuration.

So let me just optimize the import so it will just remove this.

Okay.

So we have just removed the import.

That class was not available.

So let me go to the services again and let me start the cloud gateway again.

So you can see that Cloud Gateway is also started.

If I go to the browser and if I go to Eureka, if I refresh API, Gateway is available.

Okay.

So you can see that all the services are being upgraded to Springboot 3.1 version.

Java 17 version.

Spring Cloud 2022 .0.3 version and the remaining dependencies are also upgraded.

Now let's test the application as well.





Transcript -

Now to test the application, we have different APIs to test alongside that.

We have also been using the radish in our system.

Right?

So if I if you have seen that within the cloud gateway, right, we have been using the radish dependency

as well, right?

And alongside radish there is also Zipkin installed.

Right.

So if I open localhost 941.

Okay.

You can see that Zipkin is working, right?

So Zipkin is also been installed using Docker here and all our application has been connected to our

localhost 941 Zipkin as well.

So any queries or anything that we do, all those queries will be visible here.

So we will be able to trace each and everything as well.

So your zipkin, your radish, everything is also connected to your applications.

Okay.

So everything we have upgraded.

So for testing, what we can do is if we go to our IntelliJ idea, we have three services available.

That is the product service payment service and order service.

Okay.

And all the services have spring security also.

So any user who has customer or admin role can call those APIs differently based on the permissions

that we have.

Okay.

Only admin can add the new products to the system and only the customer and admin.

Both can see the products like what products are available, right?

And only customers can place the orders from the application, right?

So these are all the different things available if I just show you that as well briefly.

Right.

So if I show you controller, product controller, right.

So you can see that this is a product controller where we have a method to add the product and this

product is can only be accessed by admin.

We have a product to get, we have a controller to get the product.

This can be accessed by admin and customer both.

Right?

Similarly, we have APIs to place the order as well.

Right.

There is an order and place order API and this can only be accessed by customer.

There is a get order details API which can be accessed by admin and customer.

Both.

Okay.

So these are all the things that we have already implemented in our previous sections.

So here we are going to check if everything is working fine after we upgraded our applications to Springboot

3.1 and Java 17.

If everything is working fine then we can say our upgrade is successful.

So what we will do is we will go ahead and we will add a product, we'll get the product details and

we will be able to place an order as well.

Okay.

And all these things you can see we have added the authorization, so everything is working via Okta

as well.

So if I go to Okta right here within the Okta, we have this application, the same application that

we have added here.

And within this application we have people available.

Okay, so this is the user that I'll be using.

So this user has both the permissions available as a customer and admin.

Both Okay.

So we'll be using this.

So first of all, what we have to do is if we go to the postman here and we have the API, now we have

the.

API gateway, right?

So all the API, all the requests we will do via the API gateway only an API gateway is running on port

9090.

Okay.

So if you go to the services, you can see Cloud Gateway is running on 9090 and everything.

We will run via that only.

Okay.

So what we will do is we will get a product so I know that I already have one product available, so

let me just try to get that product.

Okay?

And here you can see that whenever I'm trying to get a product, I'm getting the exception that is 401

unauthorized.

Okay?

So I need to authorize my request as well.

So for that, what I will do is I have to get the auth token.

So let me just open this and let me just go to incognito mode and I will go to localhost 9090.

Okay.

And to get the authorization token, what we have done is we have implemented the API, that is if we

go here within the cloud gateway.

SRC main Java controller and this is the authentication controller we have been using and the request

is authenticate.

Slash login.

Okay, this is the API authenticate slash login.

This is the API that we have to use to get the authorization token from the Okta.

So let's go here and let's hit this API and you should be redirected to login.

And what we will do is we will go here and we will use this email ID to login and we will use the password

and sign in.

So once it is signed in, you can see that we are getting the access token, refresh, token and everything.

So what I'll do, I'll take this access token.

Okay.

I'll take this access token.

I'll go to the postman, I'll go to headers, and within the headers we have to add the authorization

header.

And within the authorization header we have to add the bearer token.

So I'll just select everything.

I'll add bearer bearer space, entire token.

Okay.

This is what we have to add.

Once we add this and we hit the API again, we should be authenticated, authorized, and then we should

get the data.

So you can see that we authenticated everything and we got the data.

Currently it's showing Productseries is down because of the circuit breaker.

So let me just hit it again and you can see that I'm getting the data.

Okay.

Now let me just add the data as well.

So I'll add a data with post request.

I'll go to the body.

I'll just change the API and what I'll do I'll add lad.

Mac Pro.

The price of Mac Pro

is 6,000 USD and quantity is 100.

Okay, so let me add this product.

Okay.

It has been created now.

So if I go to get again and if I do product slash two because that's the ID, right?

So and if I hit send, I should get the product that is Mac Pro.

So now here you can see that from the cloud gateway, I'm able to access my product service and I'm

able to store the data as well.

Okay.

All the data is been stored in MySQL, so if I show you MySQL, we should be able to see the data as

well.

So let me just connect here and within the product DB tables we have the table that is the product and

if you see the data, you can see we have the iPhone 13 and Mac Pro, which is what we recently added.

Okay, now let's do the order as well.

So if I go to this order API, what I will do is I'll have localhost 1980 slash order with a post request

and within the body I need to have the product ID, what is the amount?

So I will be selling at 6500.

I want to quantity and the payment mode is cash.

Okay.

This is what you will get from the order service if you go to the order service and order controller.

Okay.

This is the API.

That is the.

Slash orders slash place order.

So let me just add place order as well.

Okay.

And this is the post request.

And these are all the fields that you need.

Product ID, total amount, quantity and product.

Payment mode.

So if I go to this order request.

Right, this is what we have.

Product ID, Total amount quantity and payment mode.

Payment mode is cash, PayPal, debit card or credit card.

Okay.

So we have selected the cash here.

So this is the data that we are going to pass.

And if we hit on send, okay, it is unauthorized.

So I have to change the headers.

So let me go and here change the headers.

So I have to go Here.

Let me select the header.

Okay.

And.

Bearer token.

Okay.

And let me hit and send.

Okay, we are getting 405.

This is because of circuit breaker issue.

Okay.

Because circuit breaker might be closed.

Okay.

So you can see that we got the request as well.

So now if I we got the 54 ID, right.

So if I go and check with the 54 ID with the get request.

You can see that we got the data right.

That order ID is 54.

It was placed, the amount was 6500.

We ordered Mac Pro product ID was two.

Okay.

And this is the payment details for that as well.

For payment ID was 54 and the payment mode was cash and it was successful as well.

Okay.

So here you can see that we are able to call all the different APIs from order service payment service

and the product service as well if we go to the browser.

Okay.

And if you go to Zipkin and if you run the query, you should be able to see all the queries as well,

right?

You can see all the queries are available now currently.

Okay.

You are not seeing all the queries.

Okay.

Only some of the queries are only getting added here.

Okay.

For this to get all the queries to be traced here within the zipkin, we have to do the configuration

change as well.

Okay.

That would be homework for you to get the configuration like what configuration that we need to do so

that we should get all the traces as well within the.

Zipkin Okay.

It's a very simple change.

Okay.

I will add in the description, but there is the homework for you to find out and what change we have

to do to get all the traces within the Zipkin Okay.

I will tell you within the hint is that we have to do the change in application dot yaml file to make

sure that all the tracing has been added here.

Okay.

So you can see that by all these changes we have upgraded our application to Springboot 3.1 and Java

17 with Spring framework six and spring security six.

So you can see that how simple it was to upgrade each and everything.

And similarly you can do the same things within your application as well.

So this was all about this section of this course about upgrading your application to Spring Boot 3.1

and Java 17.






